=== Project Directory Structure ===
.gitignore
.terraform.lock.hcl
README.md
ap_south_1/
    ap_south_1_main.tf
    ap_south_1_outputs.tf
    ap_south_1_variables.tf
    ec2/
        ec2_bastion.tf
        ec2_variables.tf
    efs/
        efs_main.tf
        efs_variables.tf
    eip/
        eip_main.tf
        eip_outputs.tf
        eip_variables.tf
    eks/
        eks_main.tf
        eks_output.tf
        eks_variables.tf
        test.yaml
    elb/
        elb_main.tf
        elb_variables.tf
    endpoint/
        endpoint_main.tf
        endpoint_variables.tf
    iam/
        iam_outputs.tf
        iam_role_eksCluster.tf
        iam_role_eksNodegroup.tf
        iam_role_karpenterNode.tf_
        iam_role_ssm.tf
        iam_user_velero.tf
        iam_variables.tf
    igw/
        igw_main.tf
        igw_outputs.tf
        igw_variables.tf
    nat/
        nat_main.tf
        nat_outputs.tf
        nat_variables.tf
    rds/
        rds_aurora_mysql.tf_
        rds_variabels.tf
    redis/
        redis_main.tf
        redis_variables.tf
    route_table/
        rt_db_route.tf
        rt_outputs.tf
        rt_private_route.tf
        rt_public_route.tf
        rt_variables.tf
    s3/
        s3_bucket_main.tf_
        s3_bucket_velero.tf
        s3_outputs.tf
        s3_variables.tf
    security_groups/
        alb_sg.tf
        bastion_sg.tf
        locals.tf
        redis_sg.tf
        sg_outputs.tf
        sg_variables.tf
    subnets/
        db_subnets.tf
        private_subnets.tf
        public_subnets.tf
        redis_subnets.tf
        subnets_outputs.tf
        subnets_variables.tf
aws.tf
backend.tf
gather_files.py
main.tf
output.txt
test_output.txt
variables.tf


=== File Contents ===


--- [FILE]: ./main.tf ---
### 리전별 리소스 관리를 위해 모듈 생성 ###
####################################

module "ap_south_1" {
  source       = "./ap_south_1"
  project_name = var.project_name
  region_code  = var.region_code

}


--- [FILE]: ./gather_files.py ---
#!/usr/bin/env python3
import os
import sys

# 무시할 디렉토리/파일 이름 목록 (간단 버전)
IGNORE_PATTERNS = [
    ".terraform",  # Terraform 모듈 디렉토리
    ".git",        # Git 디렉토리
    ".terrafofm.lock.hcl",  # Terraform lock 파일
    "__pycache__", # 파이썬 캐시 폴더
    ".DS_Store"
    # 필요시 추가 ...
]

def should_ignore(name: str) -> bool:
    """
    간단히 name이 IGNORE_PATTERNS 중 하나와 정확히 일치하면 True 반환.
    더 정교하게 하려면 fnmatch 등을 사용할 수 있음.
    """
    return name in IGNORE_PATTERNS

def generate_tree_structure(root_dir, indent=""):
    """
    root_dir 하위의 디렉토리 구조를 재귀적으로 순회하며
    트리 형태의 문자열 리스트를 만들어 반환합니다.
    무시 패턴에 해당하면 스킵합니다.
    """
    entries = sorted(os.listdir(root_dir))
    lines = []

    for i, entry in enumerate(entries):
        if should_ignore(entry):
            continue  # 무시 목록에 있는 항목은 스킵

        path = os.path.join(root_dir, entry)
        if os.path.isdir(path):
            lines.append(f"{indent}{entry}/")
            lines += generate_tree_structure(path, indent + "    ")
        else:
            lines.append(f"{indent}{entry}")

    return lines

def gather_all_files(root_dir, output_file):
    """
    1) 트리 구조를 출력
    2) 모든 파일을 순회하며, 파일명과 파일 내용을 output_file에 기록
    무시 목록에 해당하면 스킵.
    """
    with open(output_file, "w", encoding="utf-8") as out:
        # 1. 트리 구조 기록
        out.write("=== Project Directory Structure ===\n")
        structure_lines = generate_tree_structure(root_dir)
        for line in structure_lines:
            out.write(line + "\n")

        out.write("\n\n=== File Contents ===\n")

        # 2. 모든 파일 내용을 순회
        for dirpath, dirnames, filenames in os.walk(root_dir):
            # dirnames, filenames 리스트도 필터링 (무시 목록에 있는 것 제거)
            # 파이썬 os.walk()는 in-place 수정으로도 동작 가능
            dirnames[:] = [d for d in dirnames if not should_ignore(d)]
            filenames[:] = [f for f in filenames if not should_ignore(f)]

            for filename in filenames:
                file_path = os.path.join(dirpath, filename)

                out.write(f"\n\n--- [FILE]: {file_path} ---\n")

                try:
                    with open(file_path, "r", encoding="utf-8", errors="replace") as f:
                        out.write(f.read())
                except Exception as e:
                    out.write(f"\n[ERROR] Cannot read file: {e}\n")

    print(f"Done! Output saved to {output_file}")

def main():
    if len(sys.argv) < 3:
        print("Usage: python gather_files.py <root_dir> <output_file>")
        sys.exit(1)

    root_dir = sys.argv[1]
    output_file = sys.argv[2]

    if not os.path.isdir(root_dir):
        print(f"Error: '{root_dir}' is not a directory or does not exist.")
        sys.exit(1)

    gather_all_files(root_dir, output_file)

if __name__ == "__main__":
    main()

--- [FILE]: ./README.md ---
# youngjin_terraform

- 이 저장소는 Terraform을 사용하여 AWS 인프라를 코드로 관리하기 위한 예제 프로젝트입니다.
- module을 삭제한 상태이기에 실제 작동시 모듈 설치가 동반됩니다.

# 목차
 1. 프로젝트 개요 
 2. 구조 및 구성 요소
 3. 사용 방법
 4. 주요 파일/디렉토리 설명
 5. 구동 예시
 6. 주의 사항

# 프로젝트 개요 및 기능

• 목적:
 • AWS 상에서 EKS 클러스터를 자동으로 구성하고, Bastion EC2, RDS, Redis, S3 버킷 등을 원하는 대로 생성할 수 있습니다.
 • 모든 리소스는 모듈화되어 있어, 필요한 부분만 활성화/비활성화하여 확장 가능합니다.

• 특징:
 • Terraform 모듈을 통해 VPC, 서브넷, NAT, 보안 그룹, IAM 등 AWS 리소스를 체계적으로 관리
 • EKS Managed Node Group를 이용한 Kubernetes 노드 자동 관리
 • **Add-ons (vpc-cni, kube-proxy, coredns)**를 Terraform에서 함께 설정
 • Velero 백업용 IAM User & S3 버킷(옵션)
 • 보안 그룹 및 IAM Role 등 기본 정책 설정
 - Karpenter 설치

# 구조 및 구성 요소

## 특이사항

- 테스트 용도 목적으로 비용이 저렴한 뭄바이(ap-south-1)을 기준으로 리소스를 생성합니다.

## 주요 파일 및 디렉토리

- **main.tf**: AWS 리소스를 정의하는 메인 구성 파일.
- **variables.tf**: 프로젝트에서 사용되는 변수들을 선언하는 파일.
- **backend.tf**: Terraform 상태 파일의 백엔드를 설정하는 파일.
- **aws.tf**: AWS 프로바이더를 설정하는 파일.
- **ap_south_1/**: 뭄바이 리전에 대한 환경별 구성 디렉토리.

## 시작하기

### 사전 요구 사항
- Terraform이 설치되어 있어야 합니다. 
- [Terraform 설치 가이드](https://developer.hashicorp.com/terraform/tutorials/aws-get-started/install-cli)를 참고하세요.
- AWS 계정 및 적절한 IAM 권한이 필요합니다.
- AWS IAM User 및 Access key가 필요합니다.

### 초기 설정

1. 저장소를 클론합니다

   ```bash
   git clone https://github.com/PYJ-zero/youngjin_terraform.git
   cd youngjin_terraform

2. IAM User 생성
   - Terraform 사용을 위해 IAM User 및 Access Key를 생성합니다.

3. Terraform Cloud 연동
   - <https://developer.hashicorp.com/terraform/cli/cloud/settings>
   - Backend는 Terraform Cloud를 사용하며 워크스페이스 및 프로젝트를 생성합니다.
   - Variables에 AWS Access Key 및 Secret Key를 등록하여 사용합니다.

4. 코드 수정
   - backend.tf : terraform cloud의 org 및 workspace의 이름을 넣어줍니다.
   - variables.tf :  project_name을 변경해 줍니다.

5. 테라폼 초기화

   ```bash
   terraform init


--- [FILE]: ./aws.tf ---
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "5.89.0"
    }
    tls = {
      source  = "hashicorp/tls"
      version = "4.0.6"
    }
    time = {
      source  = "hashicorp/time"
      version = "0.13.0"
    }
    null = {
      source  = "hashicorp/null"
      version = "3.2.3"
    }
    cloudinit = {
      source  = "hashicorp/cloudinit"
      version = "2.3.6"
    }
    kubectl = {
      source  = "gavinbunney/kubectl"
      version = ">= 1.7.0"
    }    
  
    }
}

### 환경에 따라 자격 증명 변경 필요###
############################################
# provider "aws" {
#     region = "ap-south-1"
#   assume_role {  
#     # role_arn     = "{your_role_arn}"
#   }
# }

provider "aws" {
  region = "ap-south-1"
}


--- [FILE]: ./.gitignore ---
.terraform.lock.hcl
.terraform/
backend.tf
gather_files.py
.DS_Store
test_output.txt

--- [FILE]: ./variables.tf ---
### 전체 코드에 사용될 변수 선언 ###

variable "project_name" {
  description = "Project Name"
  type        = string
  default     = "tf-dev-pyj"
}

variable "region_code" {
  description = "Region code"
  type        = string
  default     = "ap-south-1"
}


--- [FILE]: ./test_output.txt ---
=== Project Directory Structure ===
.gitignore
.terraform.lock.hcl
README.md
ap_south_1/
    ap_south_1_main.tf
    ap_south_1_outputs.tf
    ap_south_1_variables.tf
    ec2/
        ec2_bastion.tf
        ec2_variables.tf
    efs/
        efs_main.tf
        efs_variables.tf
    eip/
        eip_main.tf
        eip_outputs.tf
        eip_variables.tf
    eks/
        eks_main.tf
        eks_output.tf
        eks_variables.tf
    elb/
        elb_main.tf
        elb_variables.tf
    endpoint/
        endpoint_main.tf
        endpoint_variables.tf
    iam/
        iam_outputs.tf
        iam_role_eksCluster.tf
        iam_role_eksNodegroup.tf
        iam_role_karpenterNode.tf_
        iam_role_ssm.tf
        iam_user_velero.tf
        iam_variables.tf
    igw/
        igw_main.tf
        igw_outputs.tf
        igw_variables.tf
    nat/
        nat_main.tf
        nat_outputs.tf
        nat_variables.tf
    rds/
        rds_aurora_mysql.tf_
        rds_variabels.tf
    redis/
        redis_main.tf
        redis_variables.tf
    route_table/
        rt_db_route.tf
        rt_outputs.tf
        rt_private_route.tf
        rt_public_route.tf
        rt_variables.tf
    s3/
        s3_bucket_main.tf_
        s3_bucket_velero.tf
        s3_outputs.tf
        s3_variables.tf
    security_groups/
        alb_sg.tf
        bastion_sg.tf
        locals.tf
        redis_sg.tf
        sg_outputs.tf
        sg_variables.tf
    subnets/
        db_subnets.tf
        private_subnets.tf
        public_subnets.tf
        redis_subnets.tf
        subnets_outputs.tf
        subnets_variables.tf
aws.tf
backend.tf
gather_files.py
main.tf
test_output.txt
variables.tf


=== File Contents ===


--- [FILE]: ./main.tf ---
### 리전별 리소스 관리를 위해 모듈 생성 ###
####################################

module "ap_south_1" {
  source       = "./ap_south_1"
  project_name = var.project_name
  region_code  = var.region_code
}


--- [FILE]: ./gather_files.py ---
#!/usr/bin/env python3
import os
import sys

# 무시할 디렉토리/파일 이름 목록 (간단 버전)
IGNORE_PATTERNS = [
    ".terraform",  # Terraform 모듈 디렉토리
    ".git",        # Git 디렉토리
    ".terrafofm.lock.hcl",  # Terraform lock 파일
    "__pycache__", # 파이썬 캐시 폴더
    ".DS_Store"
    # 필요시 추가 ...
]

def should_ignore(name: str) -> bool:
    """
    간단히 name이 IGNORE_PATTERNS 중 하나와 정확히 일치하면 True 반환.
    더 정교하게 하려면 fnmatch 등을 사용할 수 있음.
    """
    return name in IGNORE_PATTERNS

def generate_tree_structure(root_dir, indent=""):
    """
    root_dir 하위의 디렉토리 구조를 재귀적으로 순회하며
    트리 형태의 문자열 리스트를 만들어 반환합니다.
    무시 패턴에 해당하면 스킵합니다.
    """
    entries = sorted(os.listdir(root_dir))
    lines = []

    for i, entry in enumerate(entries):
        if should_ignore(entry):
            continue  # 무시 목록에 있는 항목은 스킵

        path = os.path.join(root_dir, entry)
        if os.path.isdir(path):
            lines.append(f"{indent}{entry}/")
            lines += generate_tree_structure(path, indent + "    ")
        else:
            lines.append(f"{indent}{entry}")

    return lines

def gather_all_files(root_dir, output_file):
    """
    1) 트리 구조를 출력
    2) 모든 파일을 순회하며, 파일명과 파일 내용을 output_file에 기록
    무시 목록에 해당하면 스킵.
    """
    with open(output_file, "w", encoding="utf-8") as out:
        # 1. 트리 구조 기록
        out.write("=== Project Directory Structure ===\n")
        structure_lines = generate_tree_structure(root_dir)
        for line in structure_lines:
            out.write(line + "\n")

        out.write("\n\n=== File Contents ===\n")

        # 2. 모든 파일 내용을 순회
        for dirpath, dirnames, filenames in os.walk(root_dir):
            # dirnames, filenames 리스트도 필터링 (무시 목록에 있는 것 제거)
            # 파이썬 os.walk()는 in-place 수정으로도 동작 가능
            dirnames[:] = [d for d in dirnames if not should_ignore(d)]
            filenames[:] = [f for f in filenames if not should_ignore(f)]

            for filename in filenames:
                file_path = os.path.join(dirpath, filename)

                out.write(f"\n\n--- [FILE]: {file_path} ---\n")

                try:
                    with open(file_path, "r", encoding="utf-8", errors="replace") as f:
                        out.write(f.read())
                except Exception as e:
                    out.write(f"\n[ERROR] Cannot read file: {e}\n")

    print(f"Done! Output saved to {output_file}")

def main():
    if len(sys.argv) < 3:
        print("Usage: python gather_files.py <root_dir> <output_file>")
        sys.exit(1)

    root_dir = sys.argv[1]
    output_file = sys.argv[2]

    if not os.path.isdir(root_dir):
        print(f"Error: '{root_dir}' is not a directory or does not exist.")
        sys.exit(1)

    gather_all_files(root_dir, output_file)

if __name__ == "__main__":
    main()

--- [FILE]: ./README.md ---
# youngjin_terraform

- 이 저장소는 Terraform을 사용하여 AWS 인프라를 코드로 관리하기 위한 예제 프로젝트입니다.
- module을 삭제한 상태이기에 실제 작동시 모듈 설치가 동반됩니다.

# 목차
 1. 프로젝트 개요 
 2. 구조 및 구성 요소
 3. 사용 방법
 4. 주요 파일/디렉토리 설명
 5. 구동 예시
 6. 주의 사항

# 프로젝트 개요 및 기능

• 목적:
 • AWS 상에서 EKS 클러스터를 자동으로 구성하고, Bastion EC2, RDS, Redis, S3 버킷 등을 원하는 대로 생성할 수 있습니다.
 • 모든 리소스는 모듈화되어 있어, 필요한 부분만 활성화/비활성화하여 확장 가능합니다.

• 특징:
 • Terraform 모듈을 통해 VPC, 서브넷, NAT, 보안 그룹, IAM 등 AWS 리소스를 체계적으로 관리
 • EKS Managed Node Group를 이용한 Kubernetes 노드 자동 관리
 • **Add-ons (vpc-cni, kube-proxy, coredns)**를 Terraform에서 함께 설정
 • Velero 백업용 IAM User & S3 버킷(옵션)
 • 보안 그룹 및 IAM Role 등 기본 정책 설정

# 구조 및 구성 요소

## 특이사항

- 테스트 용도 목적으로 비용이 저렴한 뭄바이(ap-south-1)을 기준으로 리소스를 생성합니다.

## 주요 파일 및 디렉토리

- **main.tf**: AWS 리소스를 정의하는 메인 구성 파일.
- **variables.tf**: 프로젝트에서 사용되는 변수들을 선언하는 파일.
- **backend.tf**: Terraform 상태 파일의 백엔드를 설정하는 파일.
- **aws.tf**: AWS 프로바이더를 설정하는 파일.
- **ap_south_1/**: 뭄바이 리전에 대한 환경별 구성 디렉토리.

## 시작하기

### 사전 요구 사항
- Terraform이 설치되어 있어야 합니다. 
- [Terraform 설치 가이드](https://developer.hashicorp.com/terraform/tutorials/aws-get-started/install-cli)를 참고하세요.
- AWS 계정 및 적절한 IAM 권한이 필요합니다.
- AWS IAM User 및 Access key가 필요합니다.

### 초기 설정

1. 저장소를 클론합니다:

   ```bash
   git clone https://github.com/PYJ-zero/youngjin_terraform.git
   cd youngjin_terraform

2. IAM User 생성
   - Terraform 사용을 위해 IAM User 및 Access Key를 생성합니다.

3. Terraform Cloud 연동
   - <https://developer.hashicorp.com/terraform/cli/cloud/settings>
   - Backend는 Terraform Cloud를 사용하며 워크스페이스 및 프로젝트를 생성합니다.
   - Variables에 AWS Access Key 및 Secret Key를 등록하여 사용합니다.

4. 코드 수정
   - backend.tf : terraform cloud의 org 및 workspace의 이름을 넣어줍니다.
   - variables.tf :  project_name을 변경해 줍니다.

5. 테라폼 초기화

   ```bash
   terraform init


--- [FILE]: ./aws.tf ---
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "5.89.0"
    }
    tls = {
      source  = "hashicorp/tls"
      version = "4.0.6"
    }
    time = {
      source  = "hashicorp/time"
      version = "0.13.0"
    }
    null = {
      source  = "hashicorp/null"
      version = "3.2.3"
    }
    cloudinit = {
      source  = "hashicorp/cloudinit"
      version = "2.3.6"
    }
  
    }
}

### 환경에 따라 자격 증명 변경 필요###
############################################
# provider "aws" {
#     region = "ap-south-1"
#   assume_role {  
#     # role_arn     = "{your_role_arn}"
#   }
# }

provider "aws" {
  region = "ap-south-1"
}


--- [FILE]: ./.gitignore ---
.terraform.lock.hcl
.terraform/
backend.tf
gather_files.py
.DS_Store
test_output.txt

--- [FILE]: ./variables.tf ---
### 전체 코드에 사용될 변수 선언 ###

variable "project_name" {
  description = "Project Name"
  type        = string
  default     = "tf-dev-pyj"
}

variable "region_code" {
  description = "Region code"
  type        = string
  default     = "ap-south-1"
}


--- [FILE]: ./test_output.txt ---
=== Project Directory Structure ===
.gitignore
.terraform.lock.hcl
README.md
ap_south_1/
    ap_south_1_main.tf
    ap_south_1_outputs.tf
    ap_south_1_variables.tf
    ec2/
        ec2_bastion.tf
        ec2_variables.tf
    efs/
        efs_main.tf
        efs_variables.tf
    eip/
        eip_main.tf
        eip_outputs.tf
        eip_variables.tf
    eks/
        eks_main.tf
        eks_output.tf
        eks_variables.tf
    elb/
        elb_main.tf
        elb_variables.tf
    endpoint/
        endpoint_main.tf
        endpoint_variables.tf
    iam/
        iam_outputs.tf
        iam_role_eksCluster.tf
        iam_role_eksNodegroup.tf
        iam_role_karpenterNode.tf_
        iam_role_ssm.tf
        iam_user_velero.tf
        iam_variables.tf
    igw/
        igw_main.tf
        igw_outputs.tf
        igw_variables.tf
    nat/
        nat_main.tf
        nat_outputs.tf
        nat_variables.tf
    rds/
        rds_aurora_mysql.tf_
        rds_variabels.tf
    redis/
        redis_main.tf
        redis_variables.tf
    route_table/
        rt_db_route.tf
        rt_outputs.tf
        rt_private_route.tf
        rt_public_route.tf
        rt_variables.tf
    s3/
        s3_bucket_main.tf_
        s3_bucket_velero.tf
        s3_outputs.tf
        s3_variables.tf
    security_groups/
        alb_sg.tf
        bastion_sg.tf
        locals.tf
        redis_sg.tf
        sg_outputs.tf
        sg_variables.tf
    subnets/
        db_subnets.tf
        private_subnets.tf
        public_subnets.tf
        redis_subnets.tf
        subnets_outputs.tf
        subnets_variables.tf
aws.tf
backend.tf
gather_files.py
main.tf
test_output.txt
variables.tf


=== File Contents ===


--- [FILE]: ./main.tf ---
### 리전별 리소스 관리를 위해 모듈 생성 ###
####################################

module "ap_south_1" {
  source       = "./ap_south_1"
  project_name = var.project_name
  region_code  = var.region_code
}


--- [FILE]: ./gather_files.py ---
#!/usr/bin/env python3
import os
import sys

# 무시할 디렉토리/파일 이름 목록 (간단 버전)
IGNORE_PATTERNS = [
    ".terraform",  # Terraform 모듈 디렉토리
    ".git",        # Git 디렉토리
    ".terrafofm.lock.hcl",  # Terraform lock 파일
    "__pycache__", # 파이썬 캐시 폴더
    ".DS_Store"
    # 필요시 추가 ...
]

def should_ignore(name: str) -> bool:
    """
    간단히 name이 IGNORE_PATTERNS 중 하나와 정확히 일치하면 True 반환.
    더 정교하게 하려면 fnmatch 등을 사용할 수 있음.
    """
    return name in IGNORE_PATTERNS

def generate_tree_structure(root_dir, indent=""):
    """
    root_dir 하위의 디렉토리 구조를 재귀적으로 순회하며
    트리 형태의 문자열 리스트를 만들어 반환합니다.
    무시 패턴에 해당하면 스킵합니다.
    """
    entries = sorted(os.listdir(root_dir))
    lines = []

    for i, entry in enumerate(entries):
        if should_ignore(entry):
            continue  # 무시 목록에 있는 항목은 스킵

        path = os.path.join(root_dir, entry)
        if os.path.isdir(path):
            lines.append(f"{indent}{entry}/")
            lines += generate_tree_structure(path, indent + "    ")
        else:
            lines.append(f"{indent}{entry}")

    return lines

def gather_all_files(root_dir, output_file):
    """
    1) 트리 구조를 출력
    2) 모든 파일을 순회하며, 파일명과 파일 내용을 output_file에 기록
    무시 목록에 해당하면 스킵.
    """
    with open(output_file, "w", encoding="utf-8") as out:
        # 1. 트리 구조 기록
        out.write("=== Project Directory Structure ===\n")
        structure_lines = generate_tree_structure(root_dir)
        for line in structure_lines:
            out.write(line + "\n")

        out.write("\n\n=== File Contents ===\n")

        # 2. 모든 파일 내용을 순회
        for dirpath, dirnames, filenames in os.walk(root_dir):
            # dirnames, filenames 리스트도 필터링 (무시 목록에 있는 것 제거)
            # 파이썬 os.walk()는 in-place 수정으로도 동작 가능
            dirnames[:] = [d for d in dirnames if not should_ignore(d)]
            filenames[:] = [f for f in filenames if not should_ignore(f)]

            for filename in filenames:
                file_path = os.path.join(dirpath, filename)

                out.write(f"\n\n--- [FILE]: {file_path} ---\n")

                try:
                    with open(file_path, "r", encoding="utf-8", errors="replace") as f:
                        out.write(f.read())
                except Exception as e:
                    out.write(f"\n[ERROR] Cannot read file: {e}\n")

    print(f"Done! Output saved to {output_file}")

def main():
    if len(sys.argv) < 3:
        print("Usage: python gather_files.py <root_dir> <output_file>")
        sys.exit(1)

    root_dir = sys.argv[1]
    output_file = sys.argv[2]

    if not os.path.isdir(root_dir):
        print(f"Error: '{root_dir}' is not a directory or does not exist.")
        sys.exit(1)

    gather_all_files(root_dir, output_file)

if __name__ == "__main__":
    main()

--- [FILE]: ./README.md ---
# youngjin_terraform

- 이 저장소는 Terraform을 사용하여 AWS 인프라를 코드로 관리하기 위한 예제 프로젝트입니다.
- module을 삭제한 상태이기에 실제 작동시 모듈 설치가 동반됩니다.

# 목차
 1. 프로젝트 개요 
 2. 구조 및 구성 요소
 3. 사용 방법
 4. 주요 파일/디렉토리 설명
 5. 구동 예시
 6. 주의 사항

# 프로젝트 개요 및 기능

• 목적:
 • AWS 상에서 EKS 클러스터를 자동으로 구성하고, Bastion EC2, RDS, Redis, S3 버킷 등을 원하는 대로 생성할 수 있습니다.
 • 모든 리소스는 모듈화되어 있어, 필요한 부분만 활성화/비활성화하여 확장 가능합니다.

• 특징:
 • Terraform 모듈을 통해 VPC, 서브넷, NAT, 보안 그룹, IAM 등 AWS 리소스를 체계적으로 관리
 • EKS Managed Node Group를 이용한 Kubernetes 노드 자동 관리
 • **Add-ons (vpc-cni, kube-proxy, coredns)**를 Terraform에서 함께 설정
 • Velero 백업용 IAM User & S3 버킷(옵션)
 • 보안 그룹 및 IAM Role 등 기본 정책 설정

# 구조 및 구성 요소

## 특이사항

- 테스트 용도 목적으로 비용이 저렴한 뭄바이(ap-south-1)을 기준으로 리소스를 생성합니다.

## 주요 파일 및 디렉토리

- **main.tf**: AWS 리소스를 정의하는 메인 구성 파일.
- **variables.tf**: 프로젝트에서 사용되는 변수들을 선언하는 파일.
- **backend.tf**: Terraform 상태 파일의 백엔드를 설정하는 파일.
- **aws.tf**: AWS 프로바이더를 설정하는 파일.
- **ap_south_1/**: 뭄바이 리전에 대한 환경별 구성 디렉토리.

## 시작하기

### 사전 요구 사항
- Terraform이 설치되어 있어야 합니다. 
- [Terraform 설치 가이드](https://developer.hashicorp.com/terraform/tutorials/aws-get-started/install-cli)를 참고하세요.
- AWS 계정 및 적절한 IAM 권한이 필요합니다.
- AWS IAM User 및 Access key가 필요합니다.

### 초기 설정

1. 저장소를 클론합니다:

   ```bash
   git clone https://github.com/PYJ-zero/youngjin_terraform.git
   cd youngjin_terraform

2. IAM User 생성
   - Terraform 사용을 위해 IAM User 및 Access Key를 생성합니다.

3. Terraform Cloud 연동
   - <https://developer.hashicorp.com/terraform/cli/cloud/settings>
   - Backend는 Terraform Cloud를 사용하며 워크스페이스 및 프로젝트를 생성합니다.
   - Variables에 AWS Access Key 및 Secret Key를 등록하여 사용합니다.

4. 코드 수정
   - backend.tf : terraform cloud의 org 및 workspace의 이름을 넣어줍니다.
   - variables.tf :  project_name을 변경해 줍니다.

5. 테라폼 초기화

   ```bash
   terraform init


--- [FILE]: ./aws.tf ---
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "5.89.0"
    }
    tls = {
      source  = "hashicorp/tls"
      version = "4.0.6"
    }
    time = {
      source  = "hashicorp/time"
      version = "0.13.0"
    }
    null = {
      source  = "hashicorp/null"
      version = "3.2.3"
    }
    cloudinit = {
      source  = "hashicorp/cloudinit"
      version = "2.3.6"
    }
  
    }
}

### 환경에 따라 자격 증명 변경 필요###
############################################
# provider "aws" {
#     region = "ap-south-1"
#   assume_role {  
#     # role_arn     = "{your_role_arn}"
#   }
# }

provider "aws" {
  region = "ap-south-1"
}


--- [FILE]: ./backend.tf ---
### 백엔드 지정, terraform cloud 사용시 필요없음###
###########################################################

# terraform {
#   backend "s3" {
#     # bucket = "{your_bucket_name}"
#     key    = "testtf"
#     region = "ap-south-1"
#   }
# }
terraform {
  cloud {

    organization = "youngjinOrg"

    workspaces {
      name = "youngjin-ws-local"
    }
  }
}


--- [FILE]: ./.terraform.lock.hcl ---
# This file is maintained automatically by "terraform init".
# Manual edits may be lost in future updates.

provider "registry.terraform.io/hashicorp/aws" {
  version     = "5.89.0"
  constraints = ">= 4.33.0, >= 4.66.0, >= 5.83.0, 5.89.0"
  hashes = [
    "h1:rFvk42jJEKiSUhK1cbERfNgYm4mD+8tq0ZcxCwpXSJs=",
    "zh:0e55784d6effc33b9098ffab7fb77a242e0223a59cdcf964caa0be94d14684af",
    "zh:23c64f3eaeffcafb007c89db3dfca94c8adf06b120af55abddaca55a6c6c924c",
    "zh:338f620133cb607ce980f1725a0a78f61cbd42f4c601808ec1ee01a6c16c9811",
    "zh:6ab0499172f17484d7b39924cf06782789df1473d31ebae0c7f3294f6e7a1227",
    "zh:6dcde3e29e538cdf80971cbdce3b285056fd0e31dd64b02d2dcdf4c02f21d0a9",
    "zh:75c9b594d77c9125bfb1aaf3fbd77a49e392841d53029b5726eb71d64de1233e",
    "zh:7b334c23091e7b4c142e378416586292197c40a31a5bdb3b29c4f9afddd286f0",
    "zh:991bbba72e5eb6eb351f466d68080992f5b0495f862a6723f386d1b4c965aa7d",
    "zh:9b12af85486a96aedd8d7984b0ff811a4b42e3d88dad1a3fb4c0b580d04fa425",
    "zh:9bd2f12eef4a5dceafc211ab3b9a63f0e3e224007a60c1bbb842f76e0377033d",
    "zh:b1ac1eb3b3e1a79fa5e5ad3364615f23b9ee0b093ceeb809fd386a4d40e7abb4",
    "zh:cea91f43151b30c428c441b97c3b98bf1e5fb72ef72f6971308e3895e23437f4",
    "zh:d3f000a1696a43d8186a516aace7d476d1fd76443627980504133477e19c8ecb",
    "zh:d6f526fbbb3e51b3acc3b9640a158f7acc4a089632fca8ec6db430b450673f25",
    "zh:e0c542950f96c93e761d50602e449fef8447f1389a6d5242a0a7dc9b06826d0b",
  ]
}

provider "registry.terraform.io/hashicorp/cloudinit" {
  version     = "2.3.6"
  constraints = ">= 2.0.0, 2.3.6"
  hashes = [
    "h1:afnqn3XPnO40laFt+SVHPPKsg1j3HXT0VAO0xBVvmrY=",
    "zh:1321b5ddede56be3f9b35bf75d7cda79adcb357fad62eb8677b6595e0baaa6cd",
    "zh:265d66e61b9cd16ca1182ebf094cc0a08fb3687e8193a1dbac6899b16c237151",
    "zh:3875c3a20e082ac55d5ff24bcaf7133ebc90c7f999fd0fb37cf0f0003474c94c",
    "zh:68ce41ccd07757c451682703840cae1ec270ed5275cd491bbf8279782dfcbb73",
    "zh:78d5eefdd9e494defcb3c68d282b8f96630502cac21d1ea161f53cfe9bb483b3",
    "zh:8dca3bb3f85ff8ac4d1b3f93975dcb751ed788396c56ebf0c3737ce1a4c60492",
    "zh:9339bdaa99939291cedf543861353c8e7171ec5231c0dfacaa9bdb3338978dab",
    "zh:a8510c2256e9a78697910bb5542aeca457c81225ea88130335f6d14a36a36c74",
    "zh:af7ed71b8fceb60a5e3b7fa663be171e0bd41bb0af30e0e1f06a004c7b584e4a",
    "zh:bc9de0f921b69d07f5fc1ea65f8af71d8d1a7053aafb500788b30bfce64b8fbe",
    "zh:bccd0a49f161a91660d7d30dd6b389e6820f29752ccf351f10a3297c96973823",
    "zh:c69321caca20009abead617f888a67aca990276cb7388b738b19157b88749190",
  ]
}

provider "registry.terraform.io/hashicorp/helm" {
  version = "2.17.0"
  hashes = [
    "h1:kQMkcPVvHOguOqnxoEU2sm1ND9vCHiT8TvZ2x6v/Rsw=",
    "zh:06fb4e9932f0afc1904d2279e6e99353c2ddac0d765305ce90519af410706bd4",
    "zh:104eccfc781fc868da3c7fec4385ad14ed183eb985c96331a1a937ac79c2d1a7",
    "zh:129345c82359837bb3f0070ce4891ec232697052f7d5ccf61d43d818912cf5f3",
    "zh:3956187ec239f4045975b35e8c30741f701aa494c386aaa04ebabffe7749f81c",
    "zh:66a9686d92a6b3ec43de3ca3fde60ef3d89fb76259ed3313ca4eb9bb8c13b7dd",
    "zh:88644260090aa621e7e8083585c468c8dd5e09a3c01a432fb05da5c4623af940",
    "zh:a248f650d174a883b32c5b94f9e725f4057e623b00f171936dcdcc840fad0b3e",
    "zh:aa498c1f1ab93be5c8fbf6d48af51dc6ef0f10b2ea88d67bcb9f02d1d80d3930",
    "zh:bf01e0f2ec2468c53596e027d376532a2d30feb72b0b5b810334d043109ae32f",
    "zh:c46fa84cc8388e5ca87eb575a534ebcf68819c5a5724142998b487cb11246654",
    "zh:d0c0f15ffc115c0965cbfe5c81f18c2e114113e7a1e6829f6bfd879ce5744fbb",
    "zh:f569b65999264a9416862bca5cd2a6177d94ccb0424f3a4ef424428912b9cb3c",
  ]
}

provider "registry.terraform.io/hashicorp/null" {
  version     = "3.2.3"
  constraints = ">= 3.0.0, 3.2.3"
  hashes = [
    "h1:I0Um8UkrMUb81Fxq/dxbr3HLP2cecTH2WMJiwKSrwQY=",
    "zh:22d062e5278d872fe7aed834f5577ba0a5afe34a3bdac2b81f828d8d3e6706d2",
    "zh:23dead00493ad863729495dc212fd6c29b8293e707b055ce5ba21ee453ce552d",
    "zh:28299accf21763ca1ca144d8f660688d7c2ad0b105b7202554ca60b02a3856d3",
    "zh:55c9e8a9ac25a7652df8c51a8a9a422bd67d784061b1de2dc9fe6c3cb4e77f2f",
    "zh:756586535d11698a216291c06b9ed8a5cc6a4ec43eee1ee09ecd5c6a9e297ac1",
    "zh:78d5eefdd9e494defcb3c68d282b8f96630502cac21d1ea161f53cfe9bb483b3",
    "zh:9d5eea62fdb587eeb96a8c4d782459f4e6b73baeece4d04b4a40e44faaee9301",
    "zh:a6355f596a3fb8fc85c2fb054ab14e722991533f87f928e7169a486462c74670",
    "zh:b5a65a789cff4ada58a5baffc76cb9767dc26ec6b45c00d2ec8b1b027f6db4ed",
    "zh:db5ab669cf11d0e9f81dc380a6fdfcac437aea3d69109c7aef1a5426639d2d65",
    "zh:de655d251c470197bcbb5ac45d289595295acb8f829f6c781d4a75c8c8b7c7dd",
    "zh:f5c68199f2e6076bce92a12230434782bf768103a427e9bb9abee99b116af7b5",
  ]
}

provider "registry.terraform.io/hashicorp/time" {
  version     = "0.13.0"
  constraints = ">= 0.9.0, 0.13.0"
  hashes = [
    "h1:iwR4JouIoeVPDabb8XCqsiaZlZ28IcB3tDD9MuPeSXE=",
    "zh:3776dd78ef3053562ccb2f8916d5d3f21a28f05e78859f0f1e4510525f891ecb",
    "zh:541ca0b56f808c15d208b9396f149563b133223c4b66cdefbcfe2d8f1c23497e",
    "zh:67ed315f3572eb20ce6778423b14fbb6faba3090f454bc20ec4146489b4738c0",
    "zh:69dc375845bcfc451426480119f2941ee28b9ef01273d228bb66918180863b3a",
    "zh:78d5eefdd9e494defcb3c68d282b8f96630502cac21d1ea161f53cfe9bb483b3",
    "zh:93c24b7c87b5db9721f60782ac784152599aa78b30fdea2fc9c594d46d92767c",
    "zh:95441cf14312041ae0b34640ff33975c09540125b01f9131358fca50e7be239d",
    "zh:a294103aeed868c58987e131357a3ec259316c937c909e8a726b862d5a227b82",
    "zh:adf6ded3f2e2f318e8aebf1040bc2791b448d006af7d12f7ddc3e8d40b22047a",
    "zh:b2d9c16b7acd20d3813060c4d3647dc5f40598ebbdf59f642d53d189e4e3870a",
    "zh:bc76a5161e9bcf74cadd76b3d4a51de508aa0c62e7f7ae536a87cd7595d81ebf",
    "zh:ce6df2c1052c60b4432cb5c0ead471d7cdb4b285b807c265328a358631fc3610",
  ]
}

provider "registry.terraform.io/hashicorp/tls" {
  version     = "4.0.6"
  constraints = ">= 3.0.0, 4.0.6"
  hashes = [
    "h1:n3M50qfWfRSpQV9Pwcvuse03pEizqrmYEryxKky4so4=",
    "zh:10de0d8af02f2e578101688fd334da3849f56ea91b0d9bd5b1f7a243417fdda8",
    "zh:37fc01f8b2bc9d5b055dc3e78bfd1beb7c42cfb776a4c81106e19c8911366297",
    "zh:4578ca03d1dd0b7f572d96bd03f744be24c726bfd282173d54b100fd221608bb",
    "zh:6c475491d1250050765a91a493ef330adc24689e8837a0f07da5a0e1269e11c1",
    "zh:81bde94d53cdababa5b376bbc6947668be4c45ab655de7aa2e8e4736dfd52509",
    "zh:abdce260840b7b050c4e401d4f75c7a199fafe58a8b213947a258f75ac18b3e8",
    "zh:b754cebfc5184873840f16a642a7c9ef78c34dc246a8ae29e056c79939963c7a",
    "zh:c928b66086078f9917aef0eec15982f2e337914c5c4dbc31dd4741403db7eb18",
    "zh:cded27bee5f24de6f2ee0cfd1df46a7f88e84aaffc2ecbf3ff7094160f193d50",
    "zh:d65eb3867e8f69aaf1b8bb53bd637c99c6b649ba3db16ded50fa9a01076d1a27",
    "zh:ecb0c8b528c7a619fa71852bb3fb5c151d47576c5aab2bf3af4db52588722eeb",
    "zh:f569b65999264a9416862bca5cd2a6177d94ccb0424f3a4ef424428912b9cb3c",
  ]
}


--- [FILE]: ./ap_south_1/ap_south_1_variables.tf ---
variable "project_name" {
  description = "Project Name"
  type        = string
}

variable "region_code" {
  description = "Region code"
  type        = string
}


--- [FILE]: ./ap_south_1/ap_south_1_main.tf ---
### 기초가 되는 AWS 리소스 제외하고 Terraform에서 제공하는 module 활용###
#########################################################################

resource "aws_vpc" "vpc" {
  cidr_block           = "150.0.0.0/16"
  enable_dns_hostnames = true
  enable_dns_support   = true
  tags = {
    Name = "${var.project_name}-EKS-CICD-TEST-VPC"
  }
}

module "subnets" {
  source       = "./subnets"
  project_name = var.project_name
  vpc_id       = aws_vpc.vpc.id
  region_code  = var.region_code
  subnets      = module.subnets.subnet_list
  eks_output   = module.eks.eks
}

module "security_group" {
  source       = "./security_groups"
  project_name = var.project_name
  vpc_id       = aws_vpc.vpc.id
}

module "igw" {
  source       = "./igw"
  depends_on   = [module.subnets]
  vpc_id       = aws_vpc.vpc.id
  project_name = var.project_name
}

module "eip" {
  source       = "./eip"
  project_name = var.project_name
  depends_on   = [module.subnets]
  vpc_id       = aws_vpc.vpc.id
  nat_subnets  = module.subnets.subnet_list.private_nat_subnets
}

module "nat" {
  source       = "./nat"
  project_name = var.project_name
  depends_on   = [module.eip]
  nat_subnets  = module.subnets.subnet_list.private_nat_subnets
  nat_eip      = module.eip.nat_eip
  vpc_id       = aws_vpc.vpc.id
}

module "route_table" {
  source       = "./route_table"
  project_name = var.project_name
  depends_on   = [module.subnets]
  vpc_id       = aws_vpc.vpc.id
  igw_id       = module.igw.igw_id
  nat          = module.nat.nat
  subnets      = module.subnets.subnet_list
}

module "s3"{
  source = "./s3"
  project_name  = var.project_name
  s3_list       = module.s3.s3_list
}

module "eks" {
  source          = "./eks"
  project_name    = var.project_name
  subnets         = module.subnets.subnet_list
  vpc_id          = aws_vpc.vpc.id
  iam_roles       = module.iam.iam_roles
  security_groups = module.security_group.security_groups
}

# module "rds" {
#   source         = "./rds"
#   project_name   = var.project_name
#   subnets        = module.subnets.subnet_list
#   vpc_id         = aws_vpc.vpc.id
#   db_subnet      = module.subnets.db_subnet
# }

# module "efs" {
#   source         = "./efs"
#   project_name   = var.project_name
#   subnets        = module.subnets.subnet_list
#   region_code    = var.region_code
#   vpc_id         = aws_vpc.vpc.id
#   security_groups = module.security_group.security_groups
# } 

# module "redis" {
#   source         = "./redis"
#   project_name   = var.project_name
#   db_subnet      = module.subnets.subnet_list
#   vpc_id         = aws_vpc.vpc.id
#   region_code    = var.region_code
#   security_groups   = module.security_group.security_groups
# }

# module "endpoint" {
#   source         = "./endpoint"
#   project_name   = var.project_name
#   vpc_id         = aws_vpc.vpc.id
#   route_table    = module.route_table.private_rtb
# }

module "ec2" {
  source          = "./ec2"
  project_name    = var.project_name
  subnets         = module.subnets.subnet_list
  region_code     = var.region_code
  vpc_id          = aws_vpc.vpc.id
  eks_cluster     = module.eks.eks
  security_groups = module.security_group.security_groups
  iam_ssm_profile = module.iam.iam_roles.ssm_instance_profile
  iam_users       = module.iam.iam_users
  s3_list         = module.s3.s3_list
}

module "iam" {
  source       = "./iam"
  project_name = var.project_name
  s3_list      = module.s3.s3_list
}

# module "elb" {
#   source         = "./elb"
#   project_name   = var.project_name
#   subnets        = module.subnets.subnet_list
#   vpc_id         = aws_vpc.vpc.id
#   security_groups = module.security_group.security_groups
# }


--- [FILE]: ./ap_south_1/ap_south_1_outputs.tf ---
# VPC
output "vpc_id" {
  description = "The ID of the VPC"
  value       = aws_vpc.vpc.id
}


--- [FILE]: ./ap_south_1/s3/s3_outputs.tf ---
output "s3_list" {
  value = {
    # youngjin_s3 = aws_s3_bucket.youngjin_s3
    velero_bucket = aws_s3_bucket.velero_bucket
  }
}


--- [FILE]: ./ap_south_1/s3/s3_bucket_velero.tf ---
resource "aws_s3_bucket" "velero_bucket" {
  bucket = "${var.project_name}-velero-s3"
}


--- [FILE]: ./ap_south_1/s3/s3_bucket_main.tf_ ---
resource "aws_s3_bucket" "youngjin_s3" {
  bucket = "${var.project_name}-s3"
}


--- [FILE]: ./ap_south_1/s3/s3_variables.tf ---
variable "project_name" {
  description = "Project Name"
  type        = string
}

variable "s3_list" {
  description = "s3 list"
  type        = any
}


--- [FILE]: ./ap_south_1/igw/igw_outputs.tf ---
output "igw_id" {
  value = aws_internet_gateway.igw.id
}


--- [FILE]: ./ap_south_1/igw/igw_variables.tf ---
variable "vpc_id" {
  description = "인터넷 게이트웨이에 연결할 VPC ID"
  type        = string
}

variable "project_name" {
  description = "Project Name"
  type        = string
}


--- [FILE]: ./ap_south_1/igw/igw_main.tf ---
//  인터넷 게이트웨이 생성
resource "aws_internet_gateway" "igw" {
  vpc_id = var.vpc_id
  tags = {
    Name = "${var.project_name}-igw"
  }
}


--- [FILE]: ./ap_south_1/redis/redis_main.tf ---
module "this" {
  source    = "cloudposse/label/null"
  namespace = "prd"
  stage     = "youngjin"
  name      = "redis-cluster-01"
}

module "redis" {
  source = "cloudposse/elasticache-redis/aws"

  availability_zones         = ["${var.region_code}a", "${var.region_code}c"]
  vpc_id                     = var.vpc_id
  allowed_security_group_ids = [var.security_groups.redis_sg.id]
  subnets                    = [for subnet in var.db_subnet.private_pri_subnets : subnet.id]
  cluster_size               = "2"
  instance_type              = "cache.m6g.large"
  apply_immediately          = true
  automatic_failover_enabled = true
  engine_version             = "6.2"
  family                     = "redis6.x"

  context = module.this.context
}


--- [FILE]: ./ap_south_1/redis/redis_variables.tf ---
variable "vpc_id" {
  description = "VPC ID"
  type        = string
}

variable "project_name" {
  description = "Project Name"
  type        = string
}

variable "db_subnet" {
  description = "DB 서브넷 그룹"
  type        = any
}

variable "region_code" {
  description = "Region code"
  type        = string
}

variable "security_groups" {
  type = any
}


--- [FILE]: ./ap_south_1/nat/nat_variables.tf ---
variable "nat_subnets" {
  description = "NAT가 적용될 서브넷 목록"
  type        = any
}

variable "nat_eip" {
  description = "NAT가 적용될 eip 목록"
  type        = any
}

variable "vpc_id" {
  description = "VPC ID"
  type        = string
}
variable "project_name" {
  description = "Project Name"
  type        = string
}


--- [FILE]: ./ap_south_1/nat/nat_outputs.tf ---
output "nat" {
  value = aws_nat_gateway.nat
}


--- [FILE]: ./ap_south_1/nat/nat_main.tf ---
resource "aws_nat_gateway" "nat" {
  count = length(var.nat_eip)

  allocation_id = element(var.nat_eip.*.id, count.index)
  subnet_id     = element(var.nat_subnets.*.id, count.index)

  tags = {
    Name = "${var.project_name}-nat"
  }
}


--- [FILE]: ./ap_south_1/route_table/rt_db_route.tf ---
resource "aws_route_table" "db_rtb" {
  vpc_id = var.vpc_id
  tags = {
    Name = "${var.project_name}-rtb-db"
  }
}

resource "aws_route_table_association" "db_subnet" {
  count          = length(var.subnets.db_subnets)
  subnet_id      = element(concat(var.subnets.db_subnets.*.id), count.index)
  route_table_id = aws_route_table.db_rtb.id
}


--- [FILE]: ./ap_south_1/route_table/rt_public_route.tf ---
resource "aws_route_table" "public_rtb" {
  vpc_id = var.vpc_id
  tags = {
    Name = "${var.project_name}-rtb-pub"
  }
}

resource "aws_route" "public_rtb_igw_route" {
  route_table_id         = aws_route_table.public_rtb.id
  destination_cidr_block = "0.0.0.0/0"
  gateway_id             = var.igw_id
}

resource "aws_route_table_association" "public_subnet" {
  count          = length(var.subnets.public_pub_subnets)
  subnet_id      = element(var.subnets.public_pub_subnets.*.id, count.index)
  route_table_id = aws_route_table.public_rtb.id
}


--- [FILE]: ./ap_south_1/route_table/rt_private_route.tf ---
resource "aws_route_table" "private_rtb" {
  vpc_id = var.vpc_id
  tags = {
    Name = "${var.project_name}-rtb-pri"
  }
}

resource "aws_route" "private_rtb_nat_route" {
  route_table_id         = aws_route_table.private_rtb.id
  destination_cidr_block = "0.0.0.0/0"
  nat_gateway_id         = element(var.nat.*.id, 0)
}

resource "aws_route_table_association" "private_subnet" {
  count          = length(var.subnets.private_pri_subnets)
  subnet_id      = element(concat(var.subnets.private_pri_subnets.*.id), count.index)
  route_table_id = aws_route_table.private_rtb.id
}


--- [FILE]: ./ap_south_1/route_table/rt_variables.tf ---
variable "vpc_id" {
  description = "라우팅 테이블에 연결할 VPC ID"
  type        = string
}

variable "igw_id" {
  description = "생성된 Internet Gateway ID"
  type        = any
}

variable "nat" {
  description = "생성된 NAT 게이트웨이 목록"
  type        = list(any)
}

variable "subnets" {
  description = "생성된 서브넷 목록"
  type        = any
}

variable "project_name" {
  description = "Project Name"
  type        = string
}


--- [FILE]: ./ap_south_1/route_table/rt_outputs.tf ---
output "rtb_list" {
  value = {
    public_rtb      = [aws_route_table.public_rtb]
    private_pri_rtb = [aws_route_table.private_rtb]
    db_rtb          = [aws_route_table.db_rtb]
  }
}

output "public_rtb" {
  value = aws_route_table.public_rtb
}

output "private_rtb" {
  value = aws_route_table.private_rtb
}

output "db_rtb" {
  value = aws_route_table.db_rtb
}


--- [FILE]: ./ap_south_1/elb/elb_variables.tf ---
variable "vpc_id" {
  description = "VPC ID"
  type        = string
}
variable "project_name" {
  description = "Project Name"
  type        = string
}

variable "subnets" {
  description = "생성된 서브넷 목록"
  type        = any
}

variable "security_groups" {
  type = any
}


--- [FILE]: ./ap_south_1/elb/elb_main.tf ---
# ALB 생성
module "alb" {
  source  = "terraform-aws-modules/alb/aws"
  version = "~> 8.0" # 사용 중인 모듈 버전에 맞게 수정

  name               = "${var.project_name}-alb"
  load_balancer_type = "application"
  internal           = false
  subnets            = [for subnet in var.subnets.public_pub_subnets.id : subnet.id] # 퍼블릭 서브넷 ID 목록
  security_groups    = [var.security_groups.alb_sg.id]                               # ALB에 적용할 보안 그룹 ID

}

# # ALB 리스너 생성 (HTTP:80)
# module "alb_listener" {
#   source  = "terraform-aws-modules/alb/aws//modules/listener"
#   version = "~> 8.0"

#   load_balancer_arn = module.alb.this_lb_arn
#   port              = 80
#   protocol          = "HTTP"

#   default_action = {
#     type = "fixed-response"
#     fixed_response = {
#       content_type = "text/plain"
#       message_body = "OK"
#       status_code  = "200"
#     }
#   }
# }

# # (옵션) ALB 타겟 그룹 생성 예시
# module "alb_target_group" {
#   source  = "terraform-aws-modules/alb/aws//modules/target-group"
#   version = "~> 8.0"

#   name     = "${var.project_name}-tg"
#   port     = 80
#   protocol = "HTTP"
#   vpc_id   = var.vpc_id

#   health_check = {
#     healthy_threshold   = 3
#     unhealthy_threshold = 3
#     timeout             = 5
#     interval            = 30
#     path                = "/"
#     matcher             = "200-299"
#   }

#   tags = {
#     Environment = var.environment
#     Project     = var.project_name
#   }
# }

# # (옵션) ALB 리스너 규칙을 통해 타겟 그룹으로 트래픽 전달
# module "alb_listener_rule" {
#   source  = "terraform-aws-modules/alb/aws//modules/listener-rule"
#   version = "~> 8.0"

#   listener_arn = module.alb_listener.this_listener_arn
#   priority     = 100

#   actions = [
#     {
#       type             = "forward"
#       target_group_arn = module.alb_target_group.this_target_group_arn
#     }
#   ]

#   conditions = [
#     {
#       field  = "path-pattern"
#       values = ["/app/*"]
#     }
#   ]
# }


--- [FILE]: ./ap_south_1/security_groups/locals.tf ---
locals {
  bastion_sg_name    = "${var.project_name}-bastion-sg"
  bastion_sg_desc    = local.bastion_sg_name
  redis_sg_name      = "${var.project_name}-redis-sg"
  redis_sg_desc      = local.redis_sg_name
  alb_sg_name        = "${var.project_name}-alb-sg"
  alb_sg_desc        = local.redis_sg_name
  description_suffix = "by terraform"
}


--- [FILE]: ./ap_south_1/security_groups/sg_variables.tf ---
variable "project_name" {
  description = "Project Name"
  type        = string
}

variable "vpc_id" {
  description = "The ID of the VPC"
  type        = string
  default     = ""
}


--- [FILE]: ./ap_south_1/security_groups/sg_outputs.tf ---
output "security_groups" {
  value = {
    redis_sg   = aws_security_group.redis_sg
    bastion_sg = aws_security_group.bastion_sg
    alb_sg     = aws_security_group.alb_sg
  }
}


--- [FILE]: ./ap_south_1/security_groups/redis_sg.tf ---
// 보안 그룹 생성
resource "aws_security_group" "redis_sg" {
  vpc_id      = var.vpc_id
  name        = local.redis_sg_name
  description = local.redis_sg_desc

  tags = {
    Name   = local.redis_sg_name
    t_addr = "${path.module}/redis_sg.tf"
  }
  lifecycle {
    ignore_changes = [
      tags["CreateDate"],
    ]
  }
}

### 인바운드
############

resource "aws_security_group_rule" "redis_sg_rule_ingress_0" {
  security_group_id = aws_security_group.redis_sg.id
  type              = "ingress"
  protocol          = "tcp"
  from_port         = 6379
  to_port           = 6379
  cidr_blocks       = ["150.0.0.0/16"]
  description       = "redis sg ${local.description_suffix}"
  lifecycle {
    ignore_changes = [
      # description,
    ]
  }
}

resource "aws_security_group_rule" "redis_sg_rule_egress_0" {
  security_group_id = aws_security_group.redis_sg.id
  type              = "egress"
  protocol          = "tcp"
  from_port         = 3306
  to_port           = 3306
  cidr_blocks       = ["150.0.0.0/16"]
  description       = "redis sg to rds ${local.description_suffix}"
  lifecycle {
    ignore_changes = [
      # description,
    ]
  }
}


--- [FILE]: ./ap_south_1/security_groups/alb_sg.tf ---
// 보안 그룹 생성
resource "aws_security_group" "alb_sg" {
  vpc_id      = var.vpc_id
  name        = local.alb_sg_name
  description = local.alb_sg_desc

  tags = {
    Name   = local.alb_sg_name
    t_addr = "${path.module}/alb.sg.tf"
  }
  lifecycle {
    ignore_changes = [
      tags["CreateDate"],
    ]
  }
}

### 인바운드
############
resource "aws_security_group_rule" "alb_sg_rule_ingress_0" {
  security_group_id = aws_security_group.alb_sg.id
  type              = "ingress"
  protocol          = "tcp"
  from_port         = 443
  to_port           = 443
  cidr_blocks       = ["0.0.0.0/0"]
  description       = "sg for alb ${local.description_suffix}"
  lifecycle {
    ignore_changes = [
      # description,
    ]
  }
}
resource "aws_security_group_rule" "alb_sg_rule_ingress_1" {
  security_group_id = aws_security_group.alb_sg.id
  type              = "ingress"
  protocol          = "tcp"
  from_port         = 80
  to_port           = 80
  cidr_blocks       = ["0.0.0.0/0"]
  description       = "sg for alb ${local.description_suffix}"
  lifecycle {
    ignore_changes = [
      # description,
    ]
  }
}
### 아웃바운드
############
resource "aws_security_group_rule" "alb_sg_rule_egress_0" {
  security_group_id = aws_security_group.alb_sg.id
  type              = "egress"
  protocol          = "tcp"
  from_port         = 443
  to_port           = 443
  cidr_blocks       = ["150.0.0.0/8"]
  description       = "sg for alb ${local.description_suffix}"
  lifecycle {
    ignore_changes = [
      # description,
    ]
  }
}

resource "aws_security_group_rule" "alb_sg_rule_egress_1" {
  security_group_id = aws_security_group.alb_sg.id
  type              = "egress"
  protocol          = "tcp"
  from_port         = 80
  to_port           = 80
  cidr_blocks       = ["150.0.0.0/8"]
  description       = "sg for alb ${local.description_suffix}"
  lifecycle {
    ignore_changes = [
      # description,
    ]
  }
}


--- [FILE]: ./ap_south_1/security_groups/bastion_sg.tf ---
// 보안 그룹 생성
resource "aws_security_group" "bastion_sg" {
  vpc_id      = var.vpc_id
  name        = local.bastion_sg_name
  description = local.bastion_sg_desc

  tags = {
    Name   = local.bastion_sg_name
    t_addr = "${path.module}/bastion.sg.tf"
  }
  lifecycle {
    ignore_changes = [
      tags["CreateDate"],
    ]
  }
}

### 아웃바운드
############
resource "aws_security_group_rule" "bastion_sg_rule_egress_0" {
  security_group_id = aws_security_group.bastion_sg.id
  type              = "egress"
  protocol          = "tcp"
  from_port         = 443
  to_port           = 443
  cidr_blocks       = ["0.0.0.0/0"]
  description       = "sg for bastion ${local.description_suffix}"
  lifecycle {
    ignore_changes = [
      # description,
    ]
  }
}


--- [FILE]: ./ap_south_1/iam/iam_role_eksNodegroup.tf ---
resource "aws_iam_role" "eks_node_role" {
  name               = "${var.project_name}-eks-node-role"
  assume_role_policy = data.aws_iam_policy_document.eks_node_trust.json
}

data "aws_iam_policy_document" "eks_node_trust" {
  statement {
    actions = ["sts:AssumeRole"]
    principals {
      type        = "Service"
      identifiers = ["ec2.amazonaws.com"]
    }
  }
}

# 노드 관련 정책 연결
resource "aws_iam_role_policy_attachment" "eks_workernode_policy" {
  role       = aws_iam_role.eks_node_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy"
}

resource "aws_iam_role_policy_attachment" "eks_cni_policy" {
  role       = aws_iam_role.eks_node_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy"
}

resource "aws_iam_role_policy_attachment" "ecr_read_policy" {
  role       = aws_iam_role.eks_node_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"
}


--- [FILE]: ./ap_south_1/iam/iam_variables.tf ---
variable "project_name" {
  description = "Project Name"
  type        = string
}

variable "s3_list" {
  description = "List of S3 Buckets"
  type        = any
}


--- [FILE]: ./ap_south_1/iam/iam_role_ssm.tf ---
# (1) IAM 역할 생성
resource "aws_iam_role" "ssm_role" {
  name = "${var.project_name}-role-ssm"
  assume_role_policy = jsonencode({
    Version = "2012-10-17",
    Statement = [
      {
        Effect = "Allow",
        Principal = {
          Service = "ec2.amazonaws.com"
        },
        Action = "sts:AssumeRole"
      }
    ]
  })
}

# (2) AmazonSSMManagedInstanceCore 정책을 역할에 연결
resource "aws_iam_role_policy_attachment" "bastion_attach_admin_policy" {
  role       = aws_iam_role.ssm_role.name
  policy_arn = "arn:aws:iam::aws:policy/AdministratorAccess"
}

# (3) 인스턴스 프로파일 생성
resource "aws_iam_instance_profile" "ssm_instance_profile" {
  name = "${var.project_name}-ssm-instance-profile"
  role = aws_iam_role.ssm_role.name
}


--- [FILE]: ./ap_south_1/iam/iam_role_karpenterNode.tf_ ---
#############################
#Karpenter IAM Role 생성
#############################
resource "aws_iam_role" "karpenter_node_role" {
  name = "${var.project_name}-karpenter-node-role"
  assume_role_policy = jsonencode({
    "Version" : "2012-10-17",
    "Statement" : [
      {
        "Effect" : "Allow",
        "Principal" : {
          "Service" : "ec2.amazonaws.com"
        },
        "Action" : "sts:AssumeRole"
      }
    ]
  })
}
#############################
#Role에 Policy Attach
#############################
resource "aws_iam_role_policy_attachment" "karpenter_worker_node_policy" {
  role       = aws_iam_role.karpenter_node_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy"
}

resource "aws_iam_role_policy_attachment" "karpenter_cni_policy" {
  role       = aws_iam_role.karpenter_node_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy"
}

resource "aws_iam_role_policy_attachment" "karpenter_ecr_policy" {
  role       = aws_iam_role.karpenter_node_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"
}

resource "aws_iam_role_policy_attachment" "karpenter_ssm_policy" {
  role       = aws_iam_role.karpenter_node_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore"
}

#############################
#Karpenter Instance Profile 생성
#############################
resource "aws_iam_instance_profile" "karpenter_instance_profile" {
  name = "${var.project_name}-karpenter-instance-profile"
  role = aws_iam_role.karpenter_node_role.name
}


--- [FILE]: ./ap_south_1/iam/iam_outputs.tf ---
output "iam_roles" {
  value = {
    ssm_role             = aws_iam_role.ssm_role
    eks_node_role        = aws_iam_role.eks_node_role
    eks_cluster_role     = aws_iam_role.eks_cluster_role
    ssm_instance_profile = aws_iam_instance_profile.ssm_instance_profile
    # karpenter_node_role = aws_iam_role.karpenter_node_role
  }
}

output "iam_users" {
  value = {
    velero_user = aws_iam_access_key.velero
  }
}


--- [FILE]: ./ap_south_1/iam/iam_role_eksCluster.tf ---
resource "aws_iam_role" "eks_cluster_role" {
  name               = "${var.project_name}-cluster-role"
  assume_role_policy = data.aws_iam_policy_document.eks_cluster_trust.json
}

data "aws_iam_policy_document" "eks_cluster_trust" {
  statement {
    actions = ["sts:AssumeRole"]
    principals {
      type        = "Service"
      identifiers = ["eks.amazonaws.com"]
    }
  }
}

# EKS 기본 정책 연결
resource "aws_iam_role_policy_attachment" "eks_cluster_policy_attach" {
  role       = aws_iam_role.eks_cluster_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSClusterPolicy"
}

resource "aws_iam_role_policy_attachment" "eks_service_policy_attach" {
  role       = aws_iam_role.eks_cluster_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSServicePolicy"
}

resource "aws_iam_role_policy_attachment" "eks_vpc_policy_attach" {
  role       = aws_iam_role.eks_cluster_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSVPCResourceController"
}


--- [FILE]: ./ap_south_1/iam/iam_user_velero.tf ---
resource "aws_iam_user" "velero" {
  name = "${var.project_name}-velero"
}

resource "aws_iam_policy" "velero_policy" {
  name        = "${var.project_name}_VeleroPolicy"
  description = "Policy for Velero to backup and restore EKS resources."
  policy      = jsonencode({
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "ec2:DescribeVolumes",
                "ec2:DescribeSnapshots",
                "ec2:CreateTags",
                "ec2:CreateVolume",
                "ec2:CreateSnapshot",
                "ec2:DeleteSnapshot"
            ],
            "Resource": "*"
        },
        {
            "Effect": "Allow",
            "Action": [
                "s3:GetObject",
                "s3:DeleteObject",
                "s3:PutObject",
                "s3:PutObjectTagging",
                "s3:AbortMultipartUpload",
                "s3:ListMultipartUploadParts"
            ],
            "Resource": [
                "${var.s3_list.velero_bucket.arn}/*"
            ]
        },
        {
            "Effect": "Allow",
            "Action": [
                "s3:ListBucket"
            ],
            "Resource": [
                "${var.s3_list.velero_bucket.arn}"
            ]
        }
    ]
})
}

resource "aws_iam_user_policy_attachment" "velero_policy_attach" {
  user       = aws_iam_user.velero.name
  policy_arn = aws_iam_policy.velero_policy.arn
}

resource "aws_iam_access_key" "velero" {
  user = aws_iam_user.velero.name
}

--- [FILE]: ./ap_south_1/eks/eks_variables.tf ---
variable "vpc_id" {
  description = "라우팅 테이블에 연결할 VPC ID"
  type        = string
}

variable "subnets" {
  description = "생성된 서브넷 목록"
  type        = any
}

variable "project_name" {
  description = "Project Name"
  type        = string
}
variable "iam_roles" {
  description = "IAM Role 목록"
  type        = any
}

variable "security_groups" {
  type = any
}


--- [FILE]: ./ap_south_1/eks/eks_main.tf ---
provider "helm" {
  kubernetes {
    host                   = module.eks.cluster_endpoint
    cluster_ca_certificate = base64decode(module.eks.cluster_certificate_authority_data)

    exec {
      api_version = "client.authentication.k8s.io/v1beta1"
      command     = "aws"
      # This requires the awscli to be installed locally where Terraform is executed
      args = ["eks", "get-token", "--cluster-name", module.eks.cluster_name]
    }
  }
}

# provider "kubectl" {
#   host                   = module.eks.cluster_endpoint
#   cluster_ca_certificate = base64decode(module.eks.cluster_certificate_authority_data)
#   load_config_file       = false

#   exec {
#     api_version = "client.authentication.k8s.io/v1beta1"
#     command     = "aws"
#     # This requires the awscli to be installed locally where Terraform is executed
#     args = ["eks", "get-token", "--cluster-name", module.eks.cluster_name]
#   }
# }

provider "aws" {
  region = "us-east-1"
  alias  = "us_east"
}

data "aws_ecrpublic_authorization_token" "token" {
  provider = aws.us_east
}

locals {
  oidc_provider = replace(module.eks.cluster_oidc_issuer_url, "https://", "")
  name          = "${var.project_name}-eks-cluster-01"
  tags = {
    Example    = "${var.project_name}-eks-karpenter"
    GithubRepo = "terraform-aws-eks"
    GithubOrg  = "terraform-aws-modules"
    Project    = "${var.project_name}-eks-cluster"
    # "karpenter.sh/discovery" = "${var.project_name}-eks-cluster-01"
  }
}

# EKS Cluster 생성
module "eks" {
  source  = "terraform-aws-modules/eks/aws"
  version = "~> 20.31"

  cluster_name    = local.name
  vpc_id          = var.vpc_id
  subnet_ids      = [for subnet in var.subnets.eks_subnets : subnet.id]
  cluster_version = "1.30"

  cluster_endpoint_public_access  = true
  cluster_endpoint_private_access = true

  create_iam_role = false
  iam_role_arn    = var.iam_roles.eks_cluster_role.arn

  # 자동으로 cluster creator(현재 Terraform을 실행하는 주체)를 admin으로 등록하지 않도록 하려면 false로 설정
  enable_cluster_creator_admin_permissions = true

  # Bastion에서 EKS API 서버(443)에 접근할 수 있도록 추가 규칙 추가
  cluster_security_group_additional_rules = {
    bastion_access = {
      description              = "Allow Bastion SG access to EKS API server"
      type                     = "ingress"
      protocol                 = "tcp"
      from_port                = 443
      to_port                  = 443
      source_security_group_id = var.security_groups.bastion_sg.id
    }
  }

  eks_managed_node_groups = {
    "${var.project_name}-nodegroup-1" = {
      # Starting on 1.30, AL2023 is the default AMI type for EKS managed node groups
      ami_type = "BOTTLEROCKET_x86_64"
      # ami_type = "AL2_x86_64"

      instance_types = ["t3a.large"]

      create_worker_iam_role = false
      worker_iam_role_arn    = var.iam_roles.eks_node_role.arn
      name                   = "${var.project_name}-ng-1"
      create_launch_template = true
      launch_template_name   = "${var.project_name}-ng-1-lt"

      subnet_ids = [for subnet in var.subnets.private_pri_subnets : subnet.id]
      # Amazon Linux 2 node groups do not require a specific settings block.
      # Any custom configuration (e.g. user data modifications) should be handled via a launch template if needed.
      min_size     = 3
      max_size     = 3
      desired_size = 3
      labels = {
        # Used to ensure Karpenter runs on nodes that it does not manage
        "karpenter.sh/controller" = "true"
      }
    }
  }

  access_entries = {
    # One access entry with a policy associated
    ssm_role_access = {
      principal_arn = var.iam_roles.ssm_role.arn

      policy_associations = {
        eks_admin_policy = {
          policy_arn = "arn:aws:eks::aws:cluster-access-policy/AmazonEKSAdminPolicy"
          access_scope = {
            type = "cluster"
          }
        }
        cluster_admin_policy = {
          policy_arn = "arn:aws:eks::aws:cluster-access-policy/AmazonEKSClusterAdminPolicy"
          access_scope = {
            type = "cluster"
          }
        }
      }
    }
  }

  node_security_group_tags = merge(local.tags, {
    # NOTE - if creating multiple security groups with this module, only tag the
    # security group that Karpenter should utilize with the following tag
    # (i.e. - at most, only one security group should have this tag in your account)
    "karpenter.sh/discovery" = local.name
  })
  tags = local.tags
}

module "karpenter" {
  source                 = "terraform-aws-modules/eks/aws//modules/karpenter"
  enable_irsa            = true
  irsa_oidc_provider_arn = module.eks.oidc_provider_arn
  cluster_name           = local.name
  enable_v1_permissions  = true

  # Name needs to match role name passed to the EC2NodeClass
  node_iam_role_use_name_prefix   = false
  node_iam_role_arn               = module.eks.eks_managed_node_groups["${var.project_name}-nodegroup-1"].iam_role_arn
  create_pod_identity_association = true

  # Used to attach additional IAM policies to the Karpenter node IAM role
  node_iam_role_additional_policies = {
    AmazonSSMManagedInstanceCore = "arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore"
  }

  tags = local.tags
}

module "karpenter_disabled" {
  source = "terraform-aws-modules/eks/aws//modules/karpenter"

  create = false
}

################################################################################
# Karpenter Helm chart & manifests
# Not required; just to demonstrate functionality of the sub-module
################################################################################

resource "helm_release" "karpenter" {
  namespace           = "kube-system"
  name                = "karpenter"
  repository          = "oci://public.ecr.aws/karpenter"
  repository_username = data.aws_ecrpublic_authorization_token.token.user_name
  repository_password = data.aws_ecrpublic_authorization_token.token.password
  chart               = "karpenter"
  version             = "1.3.3"
  wait                = false

  values = [
    <<-EOT
    nodeSelector:
      karpenter.sh/controller: 'true'
    dnsPolicy: Default
    settings:
      clusterName: ${module.eks.cluster_name}
      clusterEndpoint: ${module.eks.cluster_endpoint}
      interruptionQueue: ${module.karpenter.queue_name}
    webhook:
      enabled: false
    EOT
  ]

  set {
    name  = "serviceAccount.annotations.eks\\.amazonaws\\.com/role-arn"
    value = module.karpenter.iam_role_arn
  }

}

resource "aws_eks_addon" "ebs_csi" {
  cluster_name                = module.eks.cluster_name
  addon_name                  = "aws-ebs-csi-driver"
  addon_version               = "v1.41.0-eksbuild.1"
  resolve_conflicts_on_create = "OVERWRITE"
  depends_on                  = [module.eks]
}

resource "aws_eks_addon" "efs_csi" {
  cluster_name                = module.eks.cluster_name
  addon_name                  = "aws-efs-csi-driver"
  addon_version               = "v2.1.7-eksbuild.1"
  resolve_conflicts_on_create = "OVERWRITE"
  depends_on                  = [module.eks]
}

resource "aws_eks_addon" "coredns" {
  cluster_name                = module.eks.cluster_name
  addon_name                  = "coredns"
  addon_version               = "v1.11.1-eksbuild.9"
  resolve_conflicts_on_create = "OVERWRITE"
  depends_on                  = [module.eks]
}

resource "aws_eks_addon" "kube_proxy" {
  cluster_name                = module.eks.cluster_name
  addon_name                  = "kube-proxy"
  addon_version               = "v1.30.6-eksbuild.3"
  resolve_conflicts_on_create = "OVERWRITE"
  depends_on                  = [module.eks]
}

resource "aws_eks_addon" "vpc_cni" {
  cluster_name                = module.eks.cluster_name
  addon_name                  = "vpc-cni"
  addon_version               = "v1.19.0-eksbuild.1"
  resolve_conflicts_on_create = "OVERWRITE"
  depends_on                  = [module.eks]
  configuration_values = jsonencode({
    env = {
      ENABLE_PREFIX_DELEGATION = "true"
      WARM_PREFIX_TARGET       = "1"
    }
  })
}
resource "kubectl_manifest" "karpenter_node_template" {
  yaml_body = <<-YAML
    apiVersion: karpenter.k8s.aws/v1alpha1
    kind: AWSNodeTemplate
    metadata:
      name: default
    spec:
      blockDeviceMappings:
        - deviceName: /dev/xvda
          ebs:
            volumeSize: 10
            volumeType: gp3
      subnetSelector:
        karpenter.sh/discovery: ${module.eks.cluster_name}
      securityGroupSelector:
        karpenter.sh/discovery: ${module.eks.cluster_name}
      tags:
        karpenter.sh/discovery: ${module.eks.cluster_name}
  YAML

  depends_on = [
    helm_release.karpenter
  ]
}

resource "kubectl_manifest" "karpenter_provisioner" {
  yaml_body = <<-YAML
apiVersion: karpenter.sh/v1alpha5
kind: Provisioner
metadata:
  name: default
spec:
  labels:
    role: webapp
  requirements:
    - key: "node.kubernetes.io/instance-type"
      operator: In
      values: ["t3a.large"]
    - key: karpenter.sh/capacity-type
      operator: In
      values: ["on-demand", "spot"]
  limits:
    resources:
      cpu: 50
      memory: 192Gi
  providerRef:
    name: default
  ttlSecondsUntilExpired: 2592000 # 30d
  ttlSecondsAfterEmpty: 30
YAML

  depends_on = [helm_release.karpenter]
}
# #############################
# # Karpenter Controller IAM Role 생성
# #############################
# resource "aws_iam_role" "karpenter_controller_role" {
#   name = "${var.project_name}-karpenter-controller-role"
#   assume_role_policy = jsonencode({
#     "Version" : "2012-10-17",
#     "Statement" : [
#       {
#         "Effect" : "Allow",
#         "Principal" : {
#           "Federated" : module.eks.oidc_provider_arn
#         },
#         "Action" : "sts:AssumeRoleWithWebIdentity",
#         "Condition" : {
#           "StringEquals" : {
#             "${local.oidc_provider}:sub" : "system:serviceaccount:karpenter:karpenter",
#             "${local.oidc_provider}:aud" : "sts.amazonaws.com"
#           }
#         }
#       }
#     ]
#   })
# }
# #############################
# # Karpenter Controller IAM Role에 Policy Attach
# #############################
# resource "aws_iam_role_policy_attachment" "karpenter_controller_role_attach" {
#   role       = aws_iam_role.karpenter_controller_role.name
#   policy_arn = aws_iam_policy.karpenter_controller_policy.arn
# }

# resource "aws_iam_policy" "karpenter_controller_policy" {
#   name        = "${var.project_name}-karpenter-controller-policy"
#   description = "Policy for AWS Karpenter Controller to manage Karpenter Nodes."
#   policy = jsonencode({
#     "Statement" : [
#       {
#         "Action" : [
#           "ssm:GetParameter",
#           "ec2:DescribeImages",
#           "ec2:RunInstances",
#           "ec2:DescribeSubnets",
#           "ec2:DescribeSecurityGroups",
#           "ec2:DescribeLaunchTemplates",
#           "ec2:DescribeInstances",
#           "ec2:DescribeInstanceTypes",
#           "ec2:DescribeInstanceTypeOfferings",
#           "ec2:DescribeAvailabilityZones",
#           "ec2:DeleteLaunchTemplate",
#           "ec2:CreateTags",
#           "ec2:CreateLaunchTemplate",
#           "ec2:CreateFleet",
#           "ec2:DescribeSpotPriceHistory",
#           "pricing:GetProducts"
#         ],
#         "Effect" : "Allow",
#         "Resource" : "*",
#         "Sid" : "Karpenter"
#       },
#       {
#         "Action" : "ec2:TerminateInstances",
#         "Condition" : {
#           "StringLike" : {
#             "ec2:ResourceTag/karpenter.sh/provisioner-name" : "*"
#           }
#         },
#         "Effect" : "Allow",
#         "Resource" : "*",
#         "Sid" : "ConditionalEC2Termination"
#       },
#       {
#         "Effect" : "Allow",
#         "Action" : "iam:PassRole",
#         "Resource" : module.karpenter.node_iam_role_arn,
#         "Sid" : "PassNodeIAMRole"
#       },
#       {
#         "Effect" : "Allow",
#         "Action" : "eks:DescribeCluster",
#         "Resource" : "${module.eks.cluster_arn}",
#         "Sid" : "EKSClusterEndpointLookup"
#       }
#     ],
#     "Version" : "2012-10-17"
#   })
# }

#############################
# ALB Controller IAM Role 생성
#############################
resource "aws_iam_role" "alb_controller_role" {
  name = "${var.project_name}-alb-controller-role"
  assume_role_policy = jsonencode({
    "Version" : "2012-10-17",
    "Statement" : [
      {
        "Effect" : "Allow",
        "Principal" : {
          "Federated" : module.eks.oidc_provider_arn
        },
        "Action" : "sts:AssumeRoleWithWebIdentity",
        "Condition" : {
          "StringEquals" : {
            "${local.oidc_provider}:sub" : "system:serviceaccount:kube-system:aws-load-balancer-controller",
            "${local.oidc_provider}:aud" : "sts.amazonaws.com"
          }
        }
      }
    ]
  })
}

#############################
#Role에 Policy Attach
#############################

resource "aws_iam_role_policy_attachment" "alb_controller_role_attach" {
  role       = aws_iam_role.alb_controller_role.name
  policy_arn = aws_iam_policy.alb_controller_policy.arn
}

resource "aws_iam_policy" "alb_controller_policy" {
  name        = "${var.project_name}_ALBControllerPolicy"
  description = "Policy for AWS ALB Controller to manage ELB resources."
  policy = jsonencode({
    "Version" : "2012-10-17",
    "Statement" : [
      {
        "Effect" : "Allow",
        "Action" : [
          "iam:CreateServiceLinkedRole"
        ],
        "Resource" : "*",
        "Condition" : {
          "StringEquals" : {
            "iam:AWSServiceName" : "elasticloadbalancing.amazonaws.com"
          }
        }
      },
      {
        "Effect" : "Allow",
        "Action" : [
          "ec2:DescribeAccountAttributes",
          "ec2:DescribeAddresses",
          "ec2:DescribeAvailabilityZones",
          "ec2:DescribeInternetGateways",
          "ec2:DescribeVpcs",
          "ec2:DescribeVpcPeeringConnections",
          "ec2:DescribeSubnets",
          "ec2:DescribeSecurityGroups",
          "ec2:DescribeInstances",
          "ec2:DescribeNetworkInterfaces",
          "ec2:DescribeTags",
          "ec2:GetCoipPoolUsage",
          "ec2:DescribeCoipPools",
          "ec2:GetSecurityGroupsForVpc",
          "elasticloadbalancing:DescribeLoadBalancers",
          "elasticloadbalancing:DescribeLoadBalancerAttributes",
          "elasticloadbalancing:DescribeListeners",
          "elasticloadbalancing:DescribeListenerCertificates",
          "elasticloadbalancing:DescribeSSLPolicies",
          "elasticloadbalancing:DescribeRules",
          "elasticloadbalancing:DescribeTargetGroups",
          "elasticloadbalancing:DescribeTargetGroupAttributes",
          "elasticloadbalancing:DescribeTargetHealth",
          "elasticloadbalancing:DescribeTags",
          "elasticloadbalancing:DescribeTrustStores",
          "elasticloadbalancing:DescribeListenerAttributes",
          "elasticloadbalancing:DescribeCapacityReservation"
        ],
        "Resource" : "*"
      },
      {
        "Effect" : "Allow",
        "Action" : [
          "cognito-idp:DescribeUserPoolClient",
          "acm:ListCertificates",
          "acm:DescribeCertificate",
          "iam:ListServerCertificates",
          "iam:GetServerCertificate",
          "waf-regional:GetWebACL",
          "waf-regional:GetWebACLForResource",
          "waf-regional:AssociateWebACL",
          "waf-regional:DisassociateWebACL",
          "wafv2:GetWebACL",
          "wafv2:GetWebACLForResource",
          "wafv2:AssociateWebACL",
          "wafv2:DisassociateWebACL",
          "shield:GetSubscriptionState",
          "shield:DescribeProtection",
          "shield:CreateProtection",
          "shield:DeleteProtection"
        ],
        "Resource" : "*"
      },
      {
        "Effect" : "Allow",
        "Action" : [
          "ec2:AuthorizeSecurityGroupIngress",
          "ec2:RevokeSecurityGroupIngress"
        ],
        "Resource" : "*"
      },
      {
        "Effect" : "Allow",
        "Action" : [
          "ec2:CreateSecurityGroup"
        ],
        "Resource" : "*"
      },
      {
        "Effect" : "Allow",
        "Action" : [
          "ec2:CreateTags"
        ],
        "Resource" : "arn:aws:ec2:*:*:security-group/*",
        "Condition" : {
          "StringEquals" : {
            "ec2:CreateAction" : "CreateSecurityGroup"
          },
          "Null" : {
            "aws:RequestTag/elbv2.k8s.aws/cluster" : "false"
          }
        }
      },
      {
        "Effect" : "Allow",
        "Action" : [
          "ec2:CreateTags",
          "ec2:DeleteTags"
        ],
        "Resource" : "arn:aws:ec2:*:*:security-group/*",
        "Condition" : {
          "Null" : {
            "aws:RequestTag/elbv2.k8s.aws/cluster" : "true",
            "aws:ResourceTag/elbv2.k8s.aws/cluster" : "false"
          }
        }
      },
      {
        "Effect" : "Allow",
        "Action" : [
          "ec2:AuthorizeSecurityGroupIngress",
          "ec2:RevokeSecurityGroupIngress",
          "ec2:DeleteSecurityGroup"
        ],
        "Resource" : "*",
        "Condition" : {
          "Null" : {
            "aws:ResourceTag/elbv2.k8s.aws/cluster" : "false"
          }
        }
      },
      {
        "Effect" : "Allow",
        "Action" : [
          "elasticloadbalancing:CreateLoadBalancer",
          "elasticloadbalancing:CreateTargetGroup"
        ],
        "Resource" : "*",
        "Condition" : {
          "Null" : {
            "aws:RequestTag/elbv2.k8s.aws/cluster" : "false"
          }
        }
      },
      {
        "Effect" : "Allow",
        "Action" : [
          "elasticloadbalancing:CreateListener",
          "elasticloadbalancing:DeleteListener",
          "elasticloadbalancing:CreateRule",
          "elasticloadbalancing:DeleteRule"
        ],
        "Resource" : "*"
      },
      {
        "Effect" : "Allow",
        "Action" : [
          "elasticloadbalancing:AddTags",
          "elasticloadbalancing:RemoveTags"
        ],
        "Resource" : [
          "arn:aws:elasticloadbalancing:*:*:targetgroup/*/*",
          "arn:aws:elasticloadbalancing:*:*:loadbalancer/net/*/*",
          "arn:aws:elasticloadbalancing:*:*:loadbalancer/app/*/*"
        ],
        "Condition" : {
          "Null" : {
            "aws:RequestTag/elbv2.k8s.aws/cluster" : "true",
            "aws:ResourceTag/elbv2.k8s.aws/cluster" : "false"
          }
        }
      },
      {
        "Effect" : "Allow",
        "Action" : [
          "elasticloadbalancing:AddTags",
          "elasticloadbalancing:RemoveTags"
        ],
        "Resource" : [
          "arn:aws:elasticloadbalancing:*:*:listener/net/*/*/*",
          "arn:aws:elasticloadbalancing:*:*:listener/app/*/*/*",
          "arn:aws:elasticloadbalancing:*:*:listener-rule/net/*/*/*",
          "arn:aws:elasticloadbalancing:*:*:listener-rule/app/*/*/*"
        ]
      },
      {
        "Effect" : "Allow",
        "Action" : [
          "elasticloadbalancing:ModifyLoadBalancerAttributes",
          "elasticloadbalancing:SetIpAddressType",
          "elasticloadbalancing:SetSecurityGroups",
          "elasticloadbalancing:SetSubnets",
          "elasticloadbalancing:DeleteLoadBalancer",
          "elasticloadbalancing:ModifyTargetGroup",
          "elasticloadbalancing:ModifyTargetGroupAttributes",
          "elasticloadbalancing:DeleteTargetGroup",
          "elasticloadbalancing:ModifyListenerAttributes",
          "elasticloadbalancing:ModifyCapacityReservation"
        ],
        "Resource" : "*",
        "Condition" : {
          "Null" : {
            "aws:ResourceTag/elbv2.k8s.aws/cluster" : "false"
          }
        }
      },
      {
        "Effect" : "Allow",
        "Action" : [
          "elasticloadbalancing:AddTags"
        ],
        "Resource" : [
          "arn:aws:elasticloadbalancing:*:*:targetgroup/*/*",
          "arn:aws:elasticloadbalancing:*:*:loadbalancer/net/*/*",
          "arn:aws:elasticloadbalancing:*:*:loadbalancer/app/*/*"
        ],
        "Condition" : {
          "StringEquals" : {
            "elasticloadbalancing:CreateAction" : [
              "CreateTargetGroup",
              "CreateLoadBalancer"
            ]
          },
          "Null" : {
            "aws:RequestTag/elbv2.k8s.aws/cluster" : "false"
          }
        }
      },
      {
        "Effect" : "Allow",
        "Action" : [
          "elasticloadbalancing:RegisterTargets",
          "elasticloadbalancing:DeregisterTargets"
        ],
        "Resource" : "arn:aws:elasticloadbalancing:*:*:targetgroup/*/*"
      },
      {
        "Effect" : "Allow",
        "Action" : [
          "elasticloadbalancing:SetWebAcl",
          "elasticloadbalancing:ModifyListener",
          "elasticloadbalancing:AddListenerCertificates",
          "elasticloadbalancing:RemoveListenerCertificates",
          "elasticloadbalancing:ModifyRule"
        ],
        "Resource" : "*"
      }
    ]
  })
}


--- [FILE]: ./ap_south_1/eks/eks_output.tf ---
output "eks" {
  value = {
    cluster_name     = module.eks.cluster_name
    cluster_version  = module.eks.cluster_version
  }
}


--- [FILE]: ./ap_south_1/endpoint/endpoint_main.tf ---
module "endpoints" {
  source = "terraform-aws-modules/vpc/aws//modules/vpc-endpoints"
  vpc_id = var.vpc_id

  endpoints = {
    s3 = {
      service         = "s3"
      endpoints       = "gateway"
      route_table_ids = ["${var.route_table.id}"]
      tags            = { Name = "${var.project_name}-youngjin-s3-endpoint" }
    },
  }
}


--- [FILE]: ./ap_south_1/endpoint/endpoint_variables.tf ---
variable "vpc_id" {
  description = "VPC ID"
  type        = string
}

variable "project_name" {
  description = "Project Name"
  type        = string
}

variable "route_table" {
  description = "Route table list"
  type        = any
}


--- [FILE]: ./ap_south_1/rds/rds_variabels.tf ---
variable "vpc_id" {
  description = "VPC ID"
  type        = string
}
variable "project_name" {
  description = "Project Name"
  type        = string
}

variable "subnets" {
  description = "생성된 서브넷 목록"
  type        = any
}

variable "db_subnet" {
  description = "DB 서브넷 그룹"
  type        = any
}


--- [FILE]: ./ap_south_1/rds/rds_aurora_mysql.tf_ ---
module "cluster" {
  source = "terraform-aws-modules/rds-aurora/aws"

  name           = "${var.project_name}-aurora-mysql"
  engine         = "aurora-mysql"
  engine_version = "8.0.mysql_aurora.3.05.2"
  instance_class = "db.r7g.large"
  instances = {
    01 = {}
    02 = {}
  }
  autoscaling_enabled      = true
  autoscaling_min_capacity = 2
  autoscaling_max_capacity = 5
  skip_final_snapshot      = true
  database_name            = {put-your-db-name-here}
  master_username          = {put-your-db-username-here}
  master_password          = {put-your-db-password-here}

  vpc_id               = var.vpc_id
  db_subnet_group_name = var.db_subnet.db_subnet.name
  security_group_rules = {
    ex1_ingress = {
      cidr_blocks = ["150.0.40.0/24"]
    }
    ex2_ingress = {
      cidr_blocks = ["150.0.30.0/24"]
    }
  }

  storage_encrypted   = true
  apply_immediately   = true
  monitoring_interval = 10


  tags = {
    Environment = "prd"
    Terraform   = "true"
  }
}


--- [FILE]: ./ap_south_1/subnets/private_subnets.tf ---
resource "aws_subnet" "private_pri_subnet_a" {
  vpc_id            = var.vpc_id
  availability_zone = "${var.region_code}a"
  cidr_block        = "150.0.30.0/24"

  tags = {
    Name                              = "${var.project_name}-pri-subnet-${var.region_code}a"
    type                              = "private"
    "kubernetes.io/role/internal-elb" = "1"
    "karpenter.sh/discovery"          = "${var.project_name}-eks-cluster-01"
  }
}

resource "aws_subnet" "private_pri_subnet_c" {
  vpc_id            = var.vpc_id
  availability_zone = "${var.region_code}c"
  cidr_block        = "150.0.40.0/24"

  tags = {
    Name                              = "${var.project_name}-pri-subnet-${var.region_code}c"
    type                              = "private"
    "kubernetes.io/role/internal-elb" = "1"
    "karpenter.sh/discovery"          = "${var.project_name}-eks-cluster-01"
  }
}


--- [FILE]: ./ap_south_1/subnets/subnets_variables.tf ---
variable "project_name" {
  description = "Project Name"
  type        = string
}

variable "vpc_id" {
  description = "라우팅 테이블에 연결할 VPC ID"
  type        = string
}

variable "region_code" {
  description = "Region code"
  type        = string
}

variable "subnets" {
  description = "subnet_list"
  type        = any
}

variable "eks_output"{
  description = "eks_list"
  type        = any
}


--- [FILE]: ./ap_south_1/subnets/subnets_outputs.tf ---
output "subnet_list" {
  value = {
    public_pub_subnets    = [aws_subnet.public_subnet_a, aws_subnet.public_subnet_c]
    private_pri_subnets   = [aws_subnet.private_pri_subnet_a, aws_subnet.private_pri_subnet_c]
    private_pri_a_subnets = aws_subnet.private_pri_subnet_a
    private_pri_c_subnets = aws_subnet.private_pri_subnet_c
    private_nat_subnets   = [aws_subnet.public_subnet_a]
    db_subnets            = [aws_subnet.db_subnet_a, aws_subnet.db_subnet_c]
    eks_subnets           = [aws_subnet.public_subnet_a, aws_subnet.public_subnet_c, aws_subnet.private_pri_subnet_a, aws_subnet.private_pri_subnet_c]
  }
}

output "db_subnet" {
  value = {
    db_subnet    = aws_db_subnet_group.db_subnet
    redis_subnet = aws_elasticache_subnet_group.cache_subnet
  }
}


--- [FILE]: ./ap_south_1/subnets/public_subnets.tf ---
resource "aws_subnet" "public_subnet_a" {
  vpc_id                  = var.vpc_id
  availability_zone       = "${var.region_code}a"
  cidr_block              = "150.0.10.0/24"
  map_public_ip_on_launch = true
  # depends_on              = [ module.igw ]

  tags = {
    Name                     = "${var.project_name}-pub-subnet-${var.region_code}a"
    nat                      = "true"
    type                     = "public"
    "kubernetes.io/role/elb" = "1"
  }
}

resource "aws_subnet" "public_subnet_c" {
  vpc_id                  = var.vpc_id
  availability_zone       = "${var.region_code}c"
  cidr_block              = "150.0.20.0/24"
  map_public_ip_on_launch = true
  # depends_on              = [ module.igw ]

  tags = {
    Name                     = "${var.project_name}-pub-subnet-${var.region_code}c"
    nat                      = "false"
    type                     = "public"
    "kubernetes.io/role/elb" = "1"
  }
}


--- [FILE]: ./ap_south_1/subnets/db_subnets.tf ---
resource "aws_subnet" "db_subnet_a" {
  vpc_id                  = var.vpc_id
  availability_zone       = "${var.region_code}a"
  cidr_block              = "150.0.50.0/24"
  map_public_ip_on_launch = true
  //depends_on              = [ module.igw ]

  tags = {
    Name = "${var.project_name}-db-subnet-${var.region_code}a"
    nat  = "true"
    type = "private"
  }
}

resource "aws_subnet" "db_subnet_c" {
  vpc_id                  = var.vpc_id
  availability_zone       = "${var.region_code}c"
  cidr_block              = "150.0.60.0/24"
  map_public_ip_on_launch = true
  //depends_on              = [ module.igw ]

  tags = {
    Name = "${var.project_name}-db-subnet-${var.region_code}c"
    nat  = "false"
    type = "private"
  }
}

resource "aws_db_subnet_group" "db_subnet" {
  name       = "${var.project_name}-db-subnet"
  subnet_ids = [for subnet in var.subnets.db_subnets : subnet.id]
}


--- [FILE]: ./ap_south_1/subnets/redis_subnets.tf ---
resource "aws_elasticache_subnet_group" "cache_subnet" {
  name       = "${var.project_name}-cache-subnet"
  subnet_ids = [for subnet in var.subnets.db_subnets : subnet.id]
}


--- [FILE]: ./ap_south_1/eip/eip_variables.tf ---
variable "vpc_id" {
  description = "인터넷 게이트웨이에 연결할 VPC ID"
  type        = string
}

variable "nat_subnets" {
  description = "NAT가 적용될 서브넷 목록"
  type        = any
}
variable "project_name" {
  description = "Project Name"
  type        = string
}


--- [FILE]: ./ap_south_1/eip/eip_main.tf ---
resource "aws_eip" "nat_eips" {
  count  = length(var.nat_subnets)
  domain = "vpc"

  tags = {
    Name = "${var.project_name}-nat-${substr(element(var.nat_subnets.*.availability_zone, count.index), 14, 1)}-eip"
    nat  = "true"
  }
}


--- [FILE]: ./ap_south_1/eip/eip_outputs.tf ---
output "nat_eip" {
  value = aws_eip.nat_eips
}


--- [FILE]: ./ap_south_1/efs/efs_main.tf ---
module "efs" {
  source = "terraform-aws-modules/efs/aws"

  name      = "${var.project_name}-efs-01"
  encrypted = true

  performance_mode = "generalPurpose"
  throughput_mode  = "bursting"

  lifecycle_policy = {
    transition_to_ia = "AFTER_30_DAYS"
  }

  attach_policy                      = true
  bypass_policy_lockout_safety_check = false

  mount_targets = {
    "${var.region_code}a" = {
      subnet_id = var.subnets.private_pri_a_subnets.id
    }
    "${var.region_code}c" = {
      subnet_id = var.subnets.private_pri_c_subnets.id
    }
  }
  security_group_description = "Youngjin EFS SG"
  security_group_vpc_id      = var.vpc_id
  security_group_rules = {
    vpc = {
      description = "EFS Inbound SG Rule"
      cidr_blocks = ["150.0.30.0/24", "150.0.40.0/24"]
    }
  }

  access_points = {
    root_example = {
      root_directory = {
        path = "/example"
        creation_info = {
          owner_gid   = 1001
          owner_uid   = 1001
          permissions = "755"
        }
      }
    }
  }

  enable_backup_policy = true

  tags = {
    Terraform = "true"
  }
}


--- [FILE]: ./ap_south_1/efs/efs_variables.tf ---
variable "vpc_id" {
  description = "VPC ID"
  type        = string
}
variable "project_name" {
  description = "Project Name"
  type        = string
}

variable "subnets" {
  description = "생성된 서브넷 목록"
  type        = any
}

variable "region_code" {
  description = "Region Code"
  type        = string
}

variable "security_groups" {
  type = any
}  


--- [FILE]: ./ap_south_1/ec2/ec2_bastion.tf ---
# 1. Amazon Linux 2023 AMI를 조회하는 Data Source
data "aws_ami" "amazon_linux_2023" {
  most_recent = true
  owners      = ["amazon"] # AWS 공식 AMI
  filter {
    name   = "name"
    values = ["al2023-ami-2023.*"] # Amazon Linux 2023 이미지 이름 패턴
  }
  filter {
    name   = "state"
    values = ["available"]
  }
  filter {
    name   = "architecture"
    values = ["x86_64"]
  }
}

# 2. EC2 인스턴스 생성 모듈
module "ec2_instance" {
  source = "terraform-aws-modules/ec2-instance/aws"

  name = "${var.project_name}-ec2-bastion"

  instance_type          = "t3a.nano"
  monitoring             = true
  vpc_security_group_ids = [var.security_groups.bastion_sg.id]
  subnet_id              = var.subnets.private_pri_a_subnets.id
  iam_instance_profile   = var.iam_ssm_profile.name

  # 여기서 ami 파라미터에 Data Source의 id를 전달
  ami = data.aws_ami.amazon_linux_2023.id

  # IMDSv2만 사용하도록 metadata_options 설정 (IMDSv1 비활성화)
  metadata_options = {
    http_tokens                 = "required" # 토큰 없이 접근 불가 → IMDSv2 강제
    http_put_response_hop_limit = 2          # 응답 hop 제한 (필요에 따라 조정)
    http_endpoint               = "enabled"  # 메타데이터 엔드포인트 활성화
  }

  # 사용자 데이터 (user_data) 스크립트: kubectl 설치 및 kubeconfig 업데이트  
  user_data = <<-EOF
  #!/bin/bash
  # set -e 제거, 대신 -u와 -o pipefail만 사용
  set -uo pipefail

  LOG_FILE="/root/user_data.log"
  success_list=()
  fail_list=()

  # 명령어를 실행하면서 로그를 남기고 성공/실패를 배열에 저장
  run_command() {
    local cmd="$*"
    echo "---------------------------------------------------" | tee -a $LOG_FILE
    echo "[INFO] Running: $cmd" | tee -a $LOG_FILE
    eval "$cmd" >> $LOG_FILE 2>&1
    if [ $? -eq 0 ]; then
      echo "[SUCCESS] $cmd" | tee -a $LOG_FILE
      success_list+=("$cmd")
    else
      echo "[FAILURE] $cmd" | tee -a $LOG_FILE
      fail_list+=("$cmd")
    fi
  }

  # 예시: yum 업데이트
  run_command "yum update -y"
  run_command "yum install -y awscli vim git"

  # kubectl 설치
  run_command "curl -O https://s3.us-west-2.amazonaws.com/amazon-eks/1.31.3/2024-12-12/bin/linux/amd64/kubectl"
  run_command "chmod +x ./kubectl"
  run_command "mkdir -p /root/bin && cp ./kubectl /root/bin/kubectl && export PATH=/root/bin:\$PATH"
  run_command "echo 'export PATH=/root/bin:\$PATH' >> /root/.bashrc"

  # kubeconfig 업데이트
  run_command "aws eks update-kubeconfig --region ${var.region_code} --name ${var.eks_cluster.cluster_name}"

  # k9s 설치
  run_command "curl -LO https://github.com/derailed/k9s/releases/download/v0.40.7/k9s_Linux_amd64.tar.gz"
  run_command "tar -xvf k9s_Linux_amd64.tar.gz"
  run_command "mkdir -p /root/.local/bin && mv ./k9s /root/.local/bin && chmod +x /root/.local/bin/k9s"
  run_command "rm -f k9s_Linux_amd64.tar.gz"
  run_command "echo 'export PATH=\$PATH:/root/.local/bin' >> /root/.bashrc"

  # helm 설치
  run_command "curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3"
  run_command "chmod 700 get_helm.sh"
  run_command "./get_helm.sh"

  # eksctl 설치
  run_command "ARCH=amd64 && PLATFORM=\$(uname -s)_\$ARCH"
  run_command "curl -sLO https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_\$PLATFORM.tar.gz"
  run_command "curl -sL https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_checksums.txt | grep \$PLATFORM | sha256sum --check"
  run_command "tar -xzf eksctl_\$PLATFORM.tar.gz -C /tmp && rm -f eksctl_\$PLATFORM.tar.gz"
  run_command "mv /tmp/eksctl /usr/local/bin"

  # git clone
  run_command "mkdir /root/git && cd /root/git && git clone https://github.com/PYJ-zero/youngjin_oss.git"

  # velero 설치
  run_command "curl -L https://github.com/vmware-tanzu/velero/releases/download/v1.15.2/velero-v1.15.2-linux-amd64.tar.gz -o /tmp/velero.tar.gz"
  run_command "tar -xzvf /tmp/velero.tar.gz -C /tmp && rm -f /tmp/velero.tar.gz"
  run_command "mv /tmp/velero-v1.15.2-linux-amd64/velero /usr/local/bin/"
  run_command "chmod +x /usr/local/bin/velero"

  # credentials-velero 파일 생성
  run_command "cat <<CRED > /root/credentials-velero
  [default]
  aws_access_key_id=${var.iam_users.velero_user.id}
  aws_secret_access_key=${var.iam_users.velero_user.secret}
  CRED
  "

  # velero instal
  run_command "velero install \
    --provider aws \
    --plugins velero/velero-plugin-for-aws:v1.11.1 \
    --bucket ${var.s3_list.velero_bucket.id} \
    --backup-location-config region=${var.region_code} \
    --secret-file /root/credentials-velero \
    --snapshot-location-config region=${var.region_code}"

  # 스크립트 마지막에 요약 로그 출력
  echo "---------------------------------------------------" | tee -a $LOG_FILE
  echo "[INFO] Summary of commands" | tee -a $LOG_FILE

  if [ $${#success_list[@]} -gt 0 ]; then
    echo "[INFO] Succeeded: $${success_list[*]}" | tee -a $LOG_FILE
  fi

  if [ $${#fail_list[@]} -gt 0 ]; then
    echo "[INFO] Failed: $${fail_list[*]}" | tee -a $LOG_FILE
  fi
  source /root/.bashrc
  EOF

  tags = {
    Terraform   = "true"
    Environment = "dev"
  }
}


--- [FILE]: ./ap_south_1/ec2/ec2_variables.tf ---
variable "vpc_id" {
  description = "VPC ID"
  type        = string
}
variable "project_name" {
  description = "Project Name"
  type        = string
}

variable "subnets" {
  description = "생성된 서브넷 목록"
  type        = any
}

variable "region_code" {
  description = "Region Code"
  type        = string
}

variable "security_groups" {
  type = any
}

variable "iam_ssm_profile" {
  type = any
}

variable "eks_cluster" {
  type = any
}

variable "iam_users" {
  type = any
}

variable "s3_list" {
  type = any
  
}


--- [FILE]: ./backend.tf ---
### 백엔드 지정, terraform cloud 사용시 필요없음###
###########################################################

# terraform {
#   backend "s3" {
#     # bucket = "{your_bucket_name}"
#     key    = "testtf"
#     region = "ap-south-1"
#   }
# }
terraform {
  cloud {

    organization = "youngjinOrg"

    workspaces {
      name = "youngjin-ws-local"
    }
  }
}


--- [FILE]: ./output.txt ---
=== Project Directory Structure ===
.gitignore
.terraform.lock.hcl
README.md
ap_south_1/
    ap_south_1_main.tf
    ap_south_1_outputs.tf
    ap_south_1_variables.tf
    ec2/
        ec2_bastion.tf
        ec2_variables.tf
    efs/
        efs_main.tf
        efs_variables.tf
    eip/
        eip_main.tf
        eip_outputs.tf
        eip_variables.tf
    eks/
        eks_main.tf
        eks_output.tf
        eks_variables.tf
        test.yaml
    elb/
        elb_main.tf
        elb_variables.tf
    endpoint/
        endpoint_main.tf
        endpoint_variables.tf
    iam/
        iam_outputs.tf
        iam_role_eksCluster.tf
        iam_role_eksNodegroup.tf
        iam_role_karpenterNode.tf_
        iam_role_ssm.tf
        iam_user_velero.tf
        iam_variables.tf
    igw/
        igw_main.tf
        igw_outputs.tf
        igw_variables.tf
    nat/
        nat_main.tf
        nat_outputs.tf
        nat_variables.tf
    rds/
        rds_aurora_mysql.tf_
        rds_variabels.tf
    redis/
        redis_main.tf
        redis_variables.tf
    route_table/
        rt_db_route.tf
        rt_outputs.tf
        rt_private_route.tf
        rt_public_route.tf
        rt_variables.tf
    s3/
        s3_bucket_main.tf_
        s3_bucket_velero.tf
        s3_outputs.tf
        s3_variables.tf
    security_groups/
        alb_sg.tf
        bastion_sg.tf
        locals.tf
        redis_sg.tf
        sg_outputs.tf
        sg_variables.tf
    subnets/
        db_subnets.tf
        private_subnets.tf
        public_subnets.tf
        redis_subnets.tf
        subnets_outputs.tf
        subnets_variables.tf
aws.tf
backend.tf
gather_files.py
main.tf
output.txt
test_output.txt
variables.tf


=== File Contents ===


--- [FILE]: ./main.tf ---
### 리전별 리소스 관리를 위해 모듈 생성 ###
####################################

module "ap_south_1" {
  source       = "./ap_south_1"
  project_name = var.project_name
  region_code  = var.region_code

}


--- [FILE]: ./gather_files.py ---
#!/usr/bin/env python3
import os
import sys

# 무시할 디렉토리/파일 이름 목록 (간단 버전)
IGNORE_PATTERNS = [
    ".terraform",  # Terraform 모듈 디렉토리
    ".git",        # Git 디렉토리
    ".terrafofm.lock.hcl",  # Terraform lock 파일
    "__pycache__", # 파이썬 캐시 폴더
    ".DS_Store"
    # 필요시 추가 ...
]

def should_ignore(name: str) -> bool:
    """
    간단히 name이 IGNORE_PATTERNS 중 하나와 정확히 일치하면 True 반환.
    더 정교하게 하려면 fnmatch 등을 사용할 수 있음.
    """
    return name in IGNORE_PATTERNS

def generate_tree_structure(root_dir, indent=""):
    """
    root_dir 하위의 디렉토리 구조를 재귀적으로 순회하며
    트리 형태의 문자열 리스트를 만들어 반환합니다.
    무시 패턴에 해당하면 스킵합니다.
    """
    entries = sorted(os.listdir(root_dir))
    lines = []

    for i, entry in enumerate(entries):
        if should_ignore(entry):
            continue  # 무시 목록에 있는 항목은 스킵

        path = os.path.join(root_dir, entry)
        if os.path.isdir(path):
            lines.append(f"{indent}{entry}/")
            lines += generate_tree_structure(path, indent + "    ")
        else:
            lines.append(f"{indent}{entry}")

    return lines

def gather_all_files(root_dir, output_file):
    """
    1) 트리 구조를 출력
    2) 모든 파일을 순회하며, 파일명과 파일 내용을 output_file에 기록
    무시 목록에 해당하면 스킵.
    """
    with open(output_file, "w", encoding="utf-8") as out:
        # 1. 트리 구조 기록
        out.write("=== Project Directory Structure ===\n")
        structure_lines = generate_tree_structure(root_dir)
        for line in structure_lines:
            out.write(line + "\n")

        out.write("\n\n=== File Contents ===\n")

        # 2. 모든 파일 내용을 순회
        for dirpath, dirnames, filenames in os.walk(root_dir):
            # dirnames, filenames 리스트도 필터링 (무시 목록에 있는 것 제거)
            # 파이썬 os.walk()는 in-place 수정으로도 동작 가능
            dirnames[:] = [d for d in dirnames if not should_ignore(d)]
            filenames[:] = [f for f in filenames if not should_ignore(f)]

            for filename in filenames:
                file_path = os.path.join(dirpath, filename)

                out.write(f"\n\n--- [FILE]: {file_path} ---\n")

                try:
                    with open(file_path, "r", encoding="utf-8", errors="replace") as f:
                        out.write(f.read())
                except Exception as e:
                    out.write(f"\n[ERROR] Cannot read file: {e}\n")

    print(f"Done! Output saved to {output_file}")

def main():
    if len(sys.argv) < 3:
        print("Usage: python gather_files.py <root_dir> <output_file>")
        sys.exit(1)

    root_dir = sys.argv[1]
    output_file = sys.argv[2]

    if not os.path.isdir(root_dir):
        print(f"Error: '{root_dir}' is not a directory or does not exist.")
        sys.exit(1)

    gather_all_files(root_dir, output_file)

if __name__ == "__main__":
    main()

--- [FILE]: ./README.md ---
# youngjin_terraform

- 이 저장소는 Terraform을 사용하여 AWS 인프라를 코드로 관리하기 위한 예제 프로젝트입니다.
- module을 삭제한 상태이기에 실제 작동시 모듈 설치가 동반됩니다.

# 목차
 1. 프로젝트 개요 
 2. 구조 및 구성 요소
 3. 사용 방법
 4. 주요 파일/디렉토리 설명
 5. 구동 예시
 6. 주의 사항

# 프로젝트 개요 및 기능

• 목적:
 • AWS 상에서 EKS 클러스터를 자동으로 구성하고, Bastion EC2, RDS, Redis, S3 버킷 등을 원하는 대로 생성할 수 있습니다.
 • 모든 리소스는 모듈화되어 있어, 필요한 부분만 활성화/비활성화하여 확장 가능합니다.

• 특징:
 • Terraform 모듈을 통해 VPC, 서브넷, NAT, 보안 그룹, IAM 등 AWS 리소스를 체계적으로 관리
 • EKS Managed Node Group를 이용한 Kubernetes 노드 자동 관리
 • **Add-ons (vpc-cni, kube-proxy, coredns)**를 Terraform에서 함께 설정
 • Velero 백업용 IAM User & S3 버킷(옵션)
 • 보안 그룹 및 IAM Role 등 기본 정책 설정
 - Karpenter 설치

# 구조 및 구성 요소

## 특이사항

- 테스트 용도 목적으로 비용이 저렴한 뭄바이(ap-south-1)을 기준으로 리소스를 생성합니다.

## 주요 파일 및 디렉토리

- **main.tf**: AWS 리소스를 정의하는 메인 구성 파일.
- **variables.tf**: 프로젝트에서 사용되는 변수들을 선언하는 파일.
- **backend.tf**: Terraform 상태 파일의 백엔드를 설정하는 파일.
- **aws.tf**: AWS 프로바이더를 설정하는 파일.
- **ap_south_1/**: 뭄바이 리전에 대한 환경별 구성 디렉토리.

## 시작하기

### 사전 요구 사항
- Terraform이 설치되어 있어야 합니다. 
- [Terraform 설치 가이드](https://developer.hashicorp.com/terraform/tutorials/aws-get-started/install-cli)를 참고하세요.
- AWS 계정 및 적절한 IAM 권한이 필요합니다.
- AWS IAM User 및 Access key가 필요합니다.

### 초기 설정

1. 저장소를 클론합니다

   ```bash
   git clone https://github.com/PYJ-zero/youngjin_terraform.git
   cd youngjin_terraform

2. IAM User 생성
   - Terraform 사용을 위해 IAM User 및 Access Key를 생성합니다.

3. Terraform Cloud 연동
   - <https://developer.hashicorp.com/terraform/cli/cloud/settings>
   - Backend는 Terraform Cloud를 사용하며 워크스페이스 및 프로젝트를 생성합니다.
   - Variables에 AWS Access Key 및 Secret Key를 등록하여 사용합니다.

4. 코드 수정
   - backend.tf : terraform cloud의 org 및 workspace의 이름을 넣어줍니다.
   - variables.tf :  project_name을 변경해 줍니다.

5. 테라폼 초기화

   ```bash
   terraform init


--- [FILE]: ./aws.tf ---
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "5.89.0"
    }
    tls = {
      source  = "hashicorp/tls"
      version = "4.0.6"
    }
    time = {
      source  = "hashicorp/time"
      version = "0.13.0"
    }
    null = {
      source  = "hashicorp/null"
      version = "3.2.3"
    }
    cloudinit = {
      source  = "hashicorp/cloudinit"
      version = "2.3.6"
    }
    kubectl = {
      source  = "gavinbunney/kubectl"
      version = ">= 1.7.0"
    }    
  
    }
}

### 환경에 따라 자격 증명 변경 필요###
############################################
# provider "aws" {
#     region = "ap-south-1"
#   assume_role {  
#     # role_arn     = "{your_role_arn}"
#   }
# }

provider "aws" {
  region = "ap-south-1"
}


--- [FILE]: ./.gitignore ---
.terraform.lock.hcl
.terraform/
backend.tf
gather_files.py
.DS_Store
test_output.txt

--- [FILE]: ./variables.tf ---
### 전체 코드에 사용될 변수 선언 ###

variable "project_name" {
  description = "Project Name"
  type        = string
  default     = "tf-dev-pyj"
}

variable "region_code" {
  description = "Region code"
  type        = string
  default     = "ap-south-1"
}


--- [FILE]: ./test_output.txt ---
=== Project Directory Structure ===
.gitignore
.terraform.lock.hcl
README.md
ap_south_1/
    ap_south_1_main.tf
    ap_south_1_outputs.tf
    ap_south_1_variables.tf
    ec2/
        ec2_bastion.tf
        ec2_variables.tf
    efs/
        efs_main.tf
        efs_variables.tf
    eip/
        eip_main.tf
        eip_outputs.tf
        eip_variables.tf
    eks/
        eks_main.tf
        eks_output.tf
        eks_variables.tf
    elb/
        elb_main.tf
        elb_variables.tf
    endpoint/
        endpoint_main.tf
        endpoint_variables.tf
    iam/
        iam_outputs.tf
        iam_role_eksCluster.tf
        iam_role_eksNodegroup.tf
        iam_role_karpenterNode.tf_
        iam_role_ssm.tf
        iam_user_velero.tf
        iam_variables.tf
    igw/
        igw_main.tf
        igw_outputs.tf
        igw_variables.tf
    nat/
        nat_main.tf
        nat_outputs.tf
        nat_variables.tf
    rds/
        rds_aurora_mysql.tf_
        rds_variabels.tf
    redis/
        redis_main.tf
        redis_variables.tf
    route_table/
        rt_db_route.tf
        rt_outputs.tf
        rt_private_route.tf
        rt_public_route.tf
        rt_variables.tf
    s3/
        s3_bucket_main.tf_
        s3_bucket_velero.tf
        s3_outputs.tf
        s3_variables.tf
    security_groups/
        alb_sg.tf
        bastion_sg.tf
        locals.tf
        redis_sg.tf
        sg_outputs.tf
        sg_variables.tf
    subnets/
        db_subnets.tf
        private_subnets.tf
        public_subnets.tf
        redis_subnets.tf
        subnets_outputs.tf
        subnets_variables.tf
aws.tf
backend.tf
gather_files.py
main.tf
test_output.txt
variables.tf


=== File Contents ===


--- [FILE]: ./main.tf ---
### 리전별 리소스 관리를 위해 모듈 생성 ###
####################################

module "ap_south_1" {
  source       = "./ap_south_1"
  project_name = var.project_name
  region_code  = var.region_code
}


--- [FILE]: ./gather_files.py ---
#!/usr/bin/env python3
import os
import sys

# 무시할 디렉토리/파일 이름 목록 (간단 버전)
IGNORE_PATTERNS = [
    ".terraform",  # Terraform 모듈 디렉토리
    ".git",        # Git 디렉토리
    ".terrafofm.lock.hcl",  # Terraform lock 파일
    "__pycache__", # 파이썬 캐시 폴더
    ".DS_Store"
    # 필요시 추가 ...
]

def should_ignore(name: str) -> bool:
    """
    간단히 name이 IGNORE_PATTERNS 중 하나와 정확히 일치하면 True 반환.
    더 정교하게 하려면 fnmatch 등을 사용할 수 있음.
    """
    return name in IGNORE_PATTERNS

def generate_tree_structure(root_dir, indent=""):
    """
    root_dir 하위의 디렉토리 구조를 재귀적으로 순회하며
    트리 형태의 문자열 리스트를 만들어 반환합니다.
    무시 패턴에 해당하면 스킵합니다.
    """
    entries = sorted(os.listdir(root_dir))
    lines = []

    for i, entry in enumerate(entries):
        if should_ignore(entry):
            continue  # 무시 목록에 있는 항목은 스킵

        path = os.path.join(root_dir, entry)
        if os.path.isdir(path):
            lines.append(f"{indent}{entry}/")
            lines += generate_tree_structure(path, indent + "    ")
        else:
            lines.append(f"{indent}{entry}")

    return lines

def gather_all_files(root_dir, output_file):
    """
    1) 트리 구조를 출력
    2) 모든 파일을 순회하며, 파일명과 파일 내용을 output_file에 기록
    무시 목록에 해당하면 스킵.
    """
    with open(output_file, "w", encoding="utf-8") as out:
        # 1. 트리 구조 기록
        out.write("=== Project Directory Structure ===\n")
        structure_lines = generate_tree_structure(root_dir)
        for line in structure_lines:
            out.write(line + "\n")

        out.write("\n\n=== File Contents ===\n")

        # 2. 모든 파일 내용을 순회
        for dirpath, dirnames, filenames in os.walk(root_dir):
            # dirnames, filenames 리스트도 필터링 (무시 목록에 있는 것 제거)
            # 파이썬 os.walk()는 in-place 수정으로도 동작 가능
            dirnames[:] = [d for d in dirnames if not should_ignore(d)]
            filenames[:] = [f for f in filenames if not should_ignore(f)]

            for filename in filenames:
                file_path = os.path.join(dirpath, filename)

                out.write(f"\n\n--- [FILE]: {file_path} ---\n")

                try:
                    with open(file_path, "r", encoding="utf-8", errors="replace") as f:
                        out.write(f.read())
                except Exception as e:
                    out.write(f"\n[ERROR] Cannot read file: {e}\n")

    print(f"Done! Output saved to {output_file}")

def main():
    if len(sys.argv) < 3:
        print("Usage: python gather_files.py <root_dir> <output_file>")
        sys.exit(1)

    root_dir = sys.argv[1]
    output_file = sys.argv[2]

    if not os.path.isdir(root_dir):
        print(f"Error: '{root_dir}' is not a directory or does not exist.")
        sys.exit(1)

    gather_all_files(root_dir, output_file)

if __name__ == "__main__":
    main()

--- [FILE]: ./README.md ---
# youngjin_terraform

- 이 저장소는 Terraform을 사용하여 AWS 인프라를 코드로 관리하기 위한 예제 프로젝트입니다.
- module을 삭제한 상태이기에 실제 작동시 모듈 설치가 동반됩니다.

# 목차
 1. 프로젝트 개요 
 2. 구조 및 구성 요소
 3. 사용 방법
 4. 주요 파일/디렉토리 설명
 5. 구동 예시
 6. 주의 사항

# 프로젝트 개요 및 기능

• 목적:
 • AWS 상에서 EKS 클러스터를 자동으로 구성하고, Bastion EC2, RDS, Redis, S3 버킷 등을 원하는 대로 생성할 수 있습니다.
 • 모든 리소스는 모듈화되어 있어, 필요한 부분만 활성화/비활성화하여 확장 가능합니다.

• 특징:
 • Terraform 모듈을 통해 VPC, 서브넷, NAT, 보안 그룹, IAM 등 AWS 리소스를 체계적으로 관리
 • EKS Managed Node Group를 이용한 Kubernetes 노드 자동 관리
 • **Add-ons (vpc-cni, kube-proxy, coredns)**를 Terraform에서 함께 설정
 • Velero 백업용 IAM User & S3 버킷(옵션)
 • 보안 그룹 및 IAM Role 등 기본 정책 설정

# 구조 및 구성 요소

## 특이사항

- 테스트 용도 목적으로 비용이 저렴한 뭄바이(ap-south-1)을 기준으로 리소스를 생성합니다.

## 주요 파일 및 디렉토리

- **main.tf**: AWS 리소스를 정의하는 메인 구성 파일.
- **variables.tf**: 프로젝트에서 사용되는 변수들을 선언하는 파일.
- **backend.tf**: Terraform 상태 파일의 백엔드를 설정하는 파일.
- **aws.tf**: AWS 프로바이더를 설정하는 파일.
- **ap_south_1/**: 뭄바이 리전에 대한 환경별 구성 디렉토리.

## 시작하기

### 사전 요구 사항
- Terraform이 설치되어 있어야 합니다. 
- [Terraform 설치 가이드](https://developer.hashicorp.com/terraform/tutorials/aws-get-started/install-cli)를 참고하세요.
- AWS 계정 및 적절한 IAM 권한이 필요합니다.
- AWS IAM User 및 Access key가 필요합니다.

### 초기 설정

1. 저장소를 클론합니다:

   ```bash
   git clone https://github.com/PYJ-zero/youngjin_terraform.git
   cd youngjin_terraform

2. IAM User 생성
   - Terraform 사용을 위해 IAM User 및 Access Key를 생성합니다.

3. Terraform Cloud 연동
   - <https://developer.hashicorp.com/terraform/cli/cloud/settings>
   - Backend는 Terraform Cloud를 사용하며 워크스페이스 및 프로젝트를 생성합니다.
   - Variables에 AWS Access Key 및 Secret Key를 등록하여 사용합니다.

4. 코드 수정
   - backend.tf : terraform cloud의 org 및 workspace의 이름을 넣어줍니다.
   - variables.tf :  project_name을 변경해 줍니다.

5. 테라폼 초기화

   ```bash
   terraform init


--- [FILE]: ./aws.tf ---
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "5.89.0"
    }
    tls = {
      source  = "hashicorp/tls"
      version = "4.0.6"
    }
    time = {
      source  = "hashicorp/time"
      version = "0.13.0"
    }
    null = {
      source  = "hashicorp/null"
      version = "3.2.3"
    }
    cloudinit = {
      source  = "hashicorp/cloudinit"
      version = "2.3.6"
    }
  
    }
}

### 환경에 따라 자격 증명 변경 필요###
############################################
# provider "aws" {
#     region = "ap-south-1"
#   assume_role {  
#     # role_arn     = "{your_role_arn}"
#   }
# }

provider "aws" {
  region = "ap-south-1"
}


--- [FILE]: ./.gitignore ---
.terraform.lock.hcl
.terraform/
backend.tf
gather_files.py
.DS_Store
test_output.txt

--- [FILE]: ./variables.tf ---
### 전체 코드에 사용될 변수 선언 ###

variable "project_name" {
  description = "Project Name"
  type        = string
  default     = "tf-dev-pyj"
}

variable "region_code" {
  description = "Region code"
  type        = string
  default     = "ap-south-1"
}


--- [FILE]: ./test_output.txt ---
=== Project Directory Structure ===
.gitignore
.terraform.lock.hcl
README.md
ap_south_1/
    ap_south_1_main.tf
    ap_south_1_outputs.tf
    ap_south_1_variables.tf
    ec2/
        ec2_bastion.tf
        ec2_variables.tf
    efs/
        efs_main.tf
        efs_variables.tf
    eip/
        eip_main.tf
        eip_outputs.tf
        eip_variables.tf
    eks/
        eks_main.tf
        eks_output.tf
        eks_variables.tf
    elb/
        elb_main.tf
        elb_variables.tf
    endpoint/
        endpoint_main.tf
        endpoint_variables.tf
    iam/
        iam_outputs.tf
        iam_role_eksCluster.tf
        iam_role_eksNodegroup.tf
        iam_role_karpenterNode.tf_
        iam_role_ssm.tf
        iam_user_velero.tf
        iam_variables.tf
    igw/
        igw_main.tf
        igw_outputs.tf
        igw_variables.tf
    nat/
        nat_main.tf
        nat_outputs.tf
        nat_variables.tf
    rds/
        rds_aurora_mysql.tf_
        rds_variabels.tf
    redis/
        redis_main.tf
        redis_variables.tf
    route_table/
        rt_db_route.tf
        rt_outputs.tf
        rt_private_route.tf
        rt_public_route.tf
        rt_variables.tf
    s3/
        s3_bucket_main.tf_
        s3_bucket_velero.tf
        s3_outputs.tf
        s3_variables.tf
    security_groups/
        alb_sg.tf
        bastion_sg.tf
        locals.tf
        redis_sg.tf
        sg_outputs.tf
        sg_variables.tf
    subnets/
        db_subnets.tf
        private_subnets.tf
        public_subnets.tf
        redis_subnets.tf
        subnets_outputs.tf
        subnets_variables.tf
aws.tf
backend.tf
gather_files.py
main.tf
test_output.txt
variables.tf


=== File Contents ===


--- [FILE]: ./main.tf ---
### 리전별 리소스 관리를 위해 모듈 생성 ###
####################################

module "ap_south_1" {
  source       = "./ap_south_1"
  project_name = var.project_name
  region_code  = var.region_code
}


--- [FILE]: ./gather_files.py ---
#!/usr/bin/env python3
import os
import sys

# 무시할 디렉토리/파일 이름 목록 (간단 버전)
IGNORE_PATTERNS = [
    ".terraform",  # Terraform 모듈 디렉토리
    ".git",        # Git 디렉토리
    ".terrafofm.lock.hcl",  # Terraform lock 파일
    "__pycache__", # 파이썬 캐시 폴더
    ".DS_Store"
    # 필요시 추가 ...
]

def should_ignore(name: str) -> bool:
    """
    간단히 name이 IGNORE_PATTERNS 중 하나와 정확히 일치하면 True 반환.
    더 정교하게 하려면 fnmatch 등을 사용할 수 있음.
    """
    return name in IGNORE_PATTERNS

def generate_tree_structure(root_dir, indent=""):
    """
    root_dir 하위의 디렉토리 구조를 재귀적으로 순회하며
    트리 형태의 문자열 리스트를 만들어 반환합니다.
    무시 패턴에 해당하면 스킵합니다.
    """
    entries = sorted(os.listdir(root_dir))
    lines = []

    for i, entry in enumerate(entries):
        if should_ignore(entry):
            continue  # 무시 목록에 있는 항목은 스킵

        path = os.path.join(root_dir, entry)
        if os.path.isdir(path):
            lines.append(f"{indent}{entry}/")
            lines += generate_tree_structure(path, indent + "    ")
        else:
            lines.append(f"{indent}{entry}")

    return lines

def gather_all_files(root_dir, output_file):
    """
    1) 트리 구조를 출력
    2) 모든 파일을 순회하며, 파일명과 파일 내용을 output_file에 기록
    무시 목록에 해당하면 스킵.
    """
    with open(output_file, "w", encoding="utf-8") as out:
        # 1. 트리 구조 기록
        out.write("=== Project Directory Structure ===\n")
        structure_lines = generate_tree_structure(root_dir)
        for line in structure_lines:
            out.write(line + "\n")

        out.write("\n\n=== File Contents ===\n")

        # 2. 모든 파일 내용을 순회
        for dirpath, dirnames, filenames in os.walk(root_dir):
            # dirnames, filenames 리스트도 필터링 (무시 목록에 있는 것 제거)
            # 파이썬 os.walk()는 in-place 수정으로도 동작 가능
            dirnames[:] = [d for d in dirnames if not should_ignore(d)]
            filenames[:] = [f for f in filenames if not should_ignore(f)]

            for filename in filenames:
                file_path = os.path.join(dirpath, filename)

                out.write(f"\n\n--- [FILE]: {file_path} ---\n")

                try:
                    with open(file_path, "r", encoding="utf-8", errors="replace") as f:
                        out.write(f.read())
                except Exception as e:
                    out.write(f"\n[ERROR] Cannot read file: {e}\n")

    print(f"Done! Output saved to {output_file}")

def main():
    if len(sys.argv) < 3:
        print("Usage: python gather_files.py <root_dir> <output_file>")
        sys.exit(1)

    root_dir = sys.argv[1]
    output_file = sys.argv[2]

    if not os.path.isdir(root_dir):
        print(f"Error: '{root_dir}' is not a directory or does not exist.")
        sys.exit(1)

    gather_all_files(root_dir, output_file)

if __name__ == "__main__":
    main()

--- [FILE]: ./README.md ---
# youngjin_terraform

- 이 저장소는 Terraform을 사용하여 AWS 인프라를 코드로 관리하기 위한 예제 프로젝트입니다.
- module을 삭제한 상태이기에 실제 작동시 모듈 설치가 동반됩니다.

# 목차
 1. 프로젝트 개요 
 2. 구조 및 구성 요소
 3. 사용 방법
 4. 주요 파일/디렉토리 설명
 5. 구동 예시
 6. 주의 사항

# 프로젝트 개요 및 기능

• 목적:
 • AWS 상에서 EKS 클러스터를 자동으로 구성하고, Bastion EC2, RDS, Redis, S3 버킷 등을 원하는 대로 생성할 수 있습니다.
 • 모든 리소스는 모듈화되어 있어, 필요한 부분만 활성화/비활성화하여 확장 가능합니다.

• 특징:
 • Terraform 모듈을 통해 VPC, 서브넷, NAT, 보안 그룹, IAM 등 AWS 리소스를 체계적으로 관리
 • EKS Managed Node Group를 이용한 Kubernetes 노드 자동 관리
 • **Add-ons (vpc-cni, kube-proxy, coredns)**를 Terraform에서 함께 설정
 • Velero 백업용 IAM User & S3 버킷(옵션)
 • 보안 그룹 및 IAM Role 등 기본 정책 설정

# 구조 및 구성 요소

## 특이사항

- 테스트 용도 목적으로 비용이 저렴한 뭄바이(ap-south-1)을 기준으로 리소스를 생성합니다.

## 주요 파일 및 디렉토리

- **main.tf**: AWS 리소스를 정의하는 메인 구성 파일.
- **variables.tf**: 프로젝트에서 사용되는 변수들을 선언하는 파일.
- **backend.tf**: Terraform 상태 파일의 백엔드를 설정하는 파일.
- **aws.tf**: AWS 프로바이더를 설정하는 파일.
- **ap_south_1/**: 뭄바이 리전에 대한 환경별 구성 디렉토리.

## 시작하기

### 사전 요구 사항
- Terraform이 설치되어 있어야 합니다. 
- [Terraform 설치 가이드](https://developer.hashicorp.com/terraform/tutorials/aws-get-started/install-cli)를 참고하세요.
- AWS 계정 및 적절한 IAM 권한이 필요합니다.
- AWS IAM User 및 Access key가 필요합니다.

### 초기 설정

1. 저장소를 클론합니다:

   ```bash
   git clone https://github.com/PYJ-zero/youngjin_terraform.git
   cd youngjin_terraform

2. IAM User 생성
   - Terraform 사용을 위해 IAM User 및 Access Key를 생성합니다.

3. Terraform Cloud 연동
   - <https://developer.hashicorp.com/terraform/cli/cloud/settings>
   - Backend는 Terraform Cloud를 사용하며 워크스페이스 및 프로젝트를 생성합니다.
   - Variables에 AWS Access Key 및 Secret Key를 등록하여 사용합니다.

4. 코드 수정
   - backend.tf : terraform cloud의 org 및 workspace의 이름을 넣어줍니다.
   - variables.tf :  project_name을 변경해 줍니다.

5. 테라폼 초기화

   ```bash
   terraform init


--- [FILE]: ./aws.tf ---
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "5.89.0"
    }
    tls = {
      source  = "hashicorp/tls"
      version = "4.0.6"
    }
    time = {
      source  = "hashicorp/time"
      version = "0.13.0"
    }
    null = {
      source  = "hashicorp/null"
      version = "3.2.3"
    }
    cloudinit = {
      source  = "hashicorp/cloudinit"
      version = "2.3.6"
    }
  
    }
}

### 환경에 따라 자격 증명 변경 필요###
############################################
# provider "aws" {
#     region = "ap-south-1"
#   assume_role {  
#     # role_arn     = "{your_role_arn}"
#   }
# }

provider "aws" {
  region = "ap-south-1"
}


--- [FILE]: ./backend.tf ---
### 백엔드 지정, terraform cloud 사용시 필요없음###
###########################################################

# terraform {
#   backend "s3" {
#     # bucket = "{your_bucket_name}"
#     key    = "testtf"
#     region = "ap-south-1"
#   }
# }
terraform {
  cloud {

    organization = "youngjinOrg"

    workspaces {
      name = "youngjin-ws-local"
    }
  }
}


--- [FILE]: ./.terraform.lock.hcl ---
# This file is maintained automatically by "terraform init".
# Manual edits may be lost in future updates.

provider "registry.terraform.io/hashicorp/aws" {
  version     = "5.89.0"
  constraints = ">= 4.33.0, >= 4.66.0, >= 5.83.0, 5.89.0"
  hashes = [
    "h1:rFvk42jJEKiSUhK1cbERfNgYm4mD+8tq0ZcxCwpXSJs=",
    "zh:0e55784d6effc33b9098ffab7fb77a242e0223a59cdcf964caa0be94d14684af",
    "zh:23c64f3eaeffcafb007c89db3dfca94c8adf06b120af55abddaca55a6c6c924c",
    "zh:338f620133cb607ce980f1725a0a78f61cbd42f4c601808ec1ee01a6c16c9811",
    "zh:6ab0499172f17484d7b39924cf06782789df1473d31ebae0c7f3294f6e7a1227",
    "zh:6dcde3e29e538cdf80971cbdce3b285056fd0e31dd64b02d2dcdf4c02f21d0a9",
    "zh:75c9b594d77c9125bfb1aaf3fbd77a49e392841d53029b5726eb71d64de1233e",
    "zh:7b334c23091e7b4c142e378416586292197c40a31a5bdb3b29c4f9afddd286f0",
    "zh:991bbba72e5eb6eb351f466d68080992f5b0495f862a6723f386d1b4c965aa7d",
    "zh:9b12af85486a96aedd8d7984b0ff811a4b42e3d88dad1a3fb4c0b580d04fa425",
    "zh:9bd2f12eef4a5dceafc211ab3b9a63f0e3e224007a60c1bbb842f76e0377033d",
    "zh:b1ac1eb3b3e1a79fa5e5ad3364615f23b9ee0b093ceeb809fd386a4d40e7abb4",
    "zh:cea91f43151b30c428c441b97c3b98bf1e5fb72ef72f6971308e3895e23437f4",
    "zh:d3f000a1696a43d8186a516aace7d476d1fd76443627980504133477e19c8ecb",
    "zh:d6f526fbbb3e51b3acc3b9640a158f7acc4a089632fca8ec6db430b450673f25",
    "zh:e0c542950f96c93e761d50602e449fef8447f1389a6d5242a0a7dc9b06826d0b",
  ]
}

provider "registry.terraform.io/hashicorp/cloudinit" {
  version     = "2.3.6"
  constraints = ">= 2.0.0, 2.3.6"
  hashes = [
    "h1:afnqn3XPnO40laFt+SVHPPKsg1j3HXT0VAO0xBVvmrY=",
    "zh:1321b5ddede56be3f9b35bf75d7cda79adcb357fad62eb8677b6595e0baaa6cd",
    "zh:265d66e61b9cd16ca1182ebf094cc0a08fb3687e8193a1dbac6899b16c237151",
    "zh:3875c3a20e082ac55d5ff24bcaf7133ebc90c7f999fd0fb37cf0f0003474c94c",
    "zh:68ce41ccd07757c451682703840cae1ec270ed5275cd491bbf8279782dfcbb73",
    "zh:78d5eefdd9e494defcb3c68d282b8f96630502cac21d1ea161f53cfe9bb483b3",
    "zh:8dca3bb3f85ff8ac4d1b3f93975dcb751ed788396c56ebf0c3737ce1a4c60492",
    "zh:9339bdaa99939291cedf543861353c8e7171ec5231c0dfacaa9bdb3338978dab",
    "zh:a8510c2256e9a78697910bb5542aeca457c81225ea88130335f6d14a36a36c74",
    "zh:af7ed71b8fceb60a5e3b7fa663be171e0bd41bb0af30e0e1f06a004c7b584e4a",
    "zh:bc9de0f921b69d07f5fc1ea65f8af71d8d1a7053aafb500788b30bfce64b8fbe",
    "zh:bccd0a49f161a91660d7d30dd6b389e6820f29752ccf351f10a3297c96973823",
    "zh:c69321caca20009abead617f888a67aca990276cb7388b738b19157b88749190",
  ]
}

provider "registry.terraform.io/hashicorp/helm" {
  version = "2.17.0"
  hashes = [
    "h1:kQMkcPVvHOguOqnxoEU2sm1ND9vCHiT8TvZ2x6v/Rsw=",
    "zh:06fb4e9932f0afc1904d2279e6e99353c2ddac0d765305ce90519af410706bd4",
    "zh:104eccfc781fc868da3c7fec4385ad14ed183eb985c96331a1a937ac79c2d1a7",
    "zh:129345c82359837bb3f0070ce4891ec232697052f7d5ccf61d43d818912cf5f3",
    "zh:3956187ec239f4045975b35e8c30741f701aa494c386aaa04ebabffe7749f81c",
    "zh:66a9686d92a6b3ec43de3ca3fde60ef3d89fb76259ed3313ca4eb9bb8c13b7dd",
    "zh:88644260090aa621e7e8083585c468c8dd5e09a3c01a432fb05da5c4623af940",
    "zh:a248f650d174a883b32c5b94f9e725f4057e623b00f171936dcdcc840fad0b3e",
    "zh:aa498c1f1ab93be5c8fbf6d48af51dc6ef0f10b2ea88d67bcb9f02d1d80d3930",
    "zh:bf01e0f2ec2468c53596e027d376532a2d30feb72b0b5b810334d043109ae32f",
    "zh:c46fa84cc8388e5ca87eb575a534ebcf68819c5a5724142998b487cb11246654",
    "zh:d0c0f15ffc115c0965cbfe5c81f18c2e114113e7a1e6829f6bfd879ce5744fbb",
    "zh:f569b65999264a9416862bca5cd2a6177d94ccb0424f3a4ef424428912b9cb3c",
  ]
}

provider "registry.terraform.io/hashicorp/null" {
  version     = "3.2.3"
  constraints = ">= 3.0.0, 3.2.3"
  hashes = [
    "h1:I0Um8UkrMUb81Fxq/dxbr3HLP2cecTH2WMJiwKSrwQY=",
    "zh:22d062e5278d872fe7aed834f5577ba0a5afe34a3bdac2b81f828d8d3e6706d2",
    "zh:23dead00493ad863729495dc212fd6c29b8293e707b055ce5ba21ee453ce552d",
    "zh:28299accf21763ca1ca144d8f660688d7c2ad0b105b7202554ca60b02a3856d3",
    "zh:55c9e8a9ac25a7652df8c51a8a9a422bd67d784061b1de2dc9fe6c3cb4e77f2f",
    "zh:756586535d11698a216291c06b9ed8a5cc6a4ec43eee1ee09ecd5c6a9e297ac1",
    "zh:78d5eefdd9e494defcb3c68d282b8f96630502cac21d1ea161f53cfe9bb483b3",
    "zh:9d5eea62fdb587eeb96a8c4d782459f4e6b73baeece4d04b4a40e44faaee9301",
    "zh:a6355f596a3fb8fc85c2fb054ab14e722991533f87f928e7169a486462c74670",
    "zh:b5a65a789cff4ada58a5baffc76cb9767dc26ec6b45c00d2ec8b1b027f6db4ed",
    "zh:db5ab669cf11d0e9f81dc380a6fdfcac437aea3d69109c7aef1a5426639d2d65",
    "zh:de655d251c470197bcbb5ac45d289595295acb8f829f6c781d4a75c8c8b7c7dd",
    "zh:f5c68199f2e6076bce92a12230434782bf768103a427e9bb9abee99b116af7b5",
  ]
}

provider "registry.terraform.io/hashicorp/time" {
  version     = "0.13.0"
  constraints = ">= 0.9.0, 0.13.0"
  hashes = [
    "h1:iwR4JouIoeVPDabb8XCqsiaZlZ28IcB3tDD9MuPeSXE=",
    "zh:3776dd78ef3053562ccb2f8916d5d3f21a28f05e78859f0f1e4510525f891ecb",
    "zh:541ca0b56f808c15d208b9396f149563b133223c4b66cdefbcfe2d8f1c23497e",
    "zh:67ed315f3572eb20ce6778423b14fbb6faba3090f454bc20ec4146489b4738c0",
    "zh:69dc375845bcfc451426480119f2941ee28b9ef01273d228bb66918180863b3a",
    "zh:78d5eefdd9e494defcb3c68d282b8f96630502cac21d1ea161f53cfe9bb483b3",
    "zh:93c24b7c87b5db9721f60782ac784152599aa78b30fdea2fc9c594d46d92767c",
    "zh:95441cf14312041ae0b34640ff33975c09540125b01f9131358fca50e7be239d",
    "zh:a294103aeed868c58987e131357a3ec259316c937c909e8a726b862d5a227b82",
    "zh:adf6ded3f2e2f318e8aebf1040bc2791b448d006af7d12f7ddc3e8d40b22047a",
    "zh:b2d9c16b7acd20d3813060c4d3647dc5f40598ebbdf59f642d53d189e4e3870a",
    "zh:bc76a5161e9bcf74cadd76b3d4a51de508aa0c62e7f7ae536a87cd7595d81ebf",
    "zh:ce6df2c1052c60b4432cb5c0ead471d7cdb4b285b807c265328a358631fc3610",
  ]
}

provider "registry.terraform.io/hashicorp/tls" {
  version     = "4.0.6"
  constraints = ">= 3.0.0, 4.0.6"
  hashes = [
    "h1:n3M50qfWfRSpQV9Pwcvuse03pEizqrmYEryxKky4so4=",
    "zh:10de0d8af02f2e578101688fd334da3849f56ea91b0d9bd5b1f7a243417fdda8",
    "zh:37fc01f8b2bc9d5b055dc3e78bfd1beb7c42cfb776a4c81106e19c8911366297",
    "zh:4578ca03d1dd0b7f572d96bd03f744be24c726bfd282173d54b100fd221608bb",
    "zh:6c475491d1250050765a91a493ef330adc24689e8837a0f07da5a0e1269e11c1",
    "zh:81bde94d53cdababa5b376bbc6947668be4c45ab655de7aa2e8e4736dfd52509",
    "zh:abdce260840b7b050c4e401d4f75c7a199fafe58a8b213947a258f75ac18b3e8",
    "zh:b754cebfc5184873840f16a642a7c9ef78c34dc246a8ae29e056c79939963c7a",
    "zh:c928b66086078f9917aef0eec15982f2e337914c5c4dbc31dd4741403db7eb18",
    "zh:cded27bee5f24de6f2ee0cfd1df46a7f88e84aaffc2ecbf3ff7094160f193d50",
    "zh:d65eb3867e8f69aaf1b8bb53bd637c99c6b649ba3db16ded50fa9a01076d1a27",
    "zh:ecb0c8b528c7a619fa71852bb3fb5c151d47576c5aab2bf3af4db52588722eeb",
    "zh:f569b65999264a9416862bca5cd2a6177d94ccb0424f3a4ef424428912b9cb3c",
  ]
}


--- [FILE]: ./ap_south_1/ap_south_1_variables.tf ---
variable "project_name" {
  description = "Project Name"
  type        = string
}

variable "region_code" {
  description = "Region code"
  type        = string
}


--- [FILE]: ./ap_south_1/ap_south_1_main.tf ---
### 기초가 되는 AWS 리소스 제외하고 Terraform에서 제공하는 module 활용###
#########################################################################

resource "aws_vpc" "vpc" {
  cidr_block           = "150.0.0.0/16"
  enable_dns_hostnames = true
  enable_dns_support   = true
  tags = {
    Name = "${var.project_name}-EKS-CICD-TEST-VPC"
  }
}

module "subnets" {
  source       = "./subnets"
  project_name = var.project_name
  vpc_id       = aws_vpc.vpc.id
  region_code  = var.region_code
  subnets      = module.subnets.subnet_list
  eks_output   = module.eks.eks
}

module "security_group" {
  source       = "./security_groups"
  project_name = var.project_name
  vpc_id       = aws_vpc.vpc.id
}

module "igw" {
  source       = "./igw"
  depends_on   = [module.subnets]
  vpc_id       = aws_vpc.vpc.id
  project_name = var.project_name
}

module "eip" {
  source       = "./eip"
  project_name = var.project_name
  depends_on   = [module.subnets]
  vpc_id       = aws_vpc.vpc.id
  nat_subnets  = module.subnets.subnet_list.private_nat_subnets
}

module "nat" {
  source       = "./nat"
  project_name = var.project_name
  depends_on   = [module.eip]
  nat_subnets  = module.subnets.subnet_list.private_nat_subnets
  nat_eip      = module.eip.nat_eip
  vpc_id       = aws_vpc.vpc.id
}

module "route_table" {
  source       = "./route_table"
  project_name = var.project_name
  depends_on   = [module.subnets]
  vpc_id       = aws_vpc.vpc.id
  igw_id       = module.igw.igw_id
  nat          = module.nat.nat
  subnets      = module.subnets.subnet_list
}

module "s3"{
  source = "./s3"
  project_name  = var.project_name
  s3_list       = module.s3.s3_list
}

module "eks" {
  source          = "./eks"
  project_name    = var.project_name
  subnets         = module.subnets.subnet_list
  vpc_id          = aws_vpc.vpc.id
  iam_roles       = module.iam.iam_roles
  security_groups = module.security_group.security_groups
}

# module "rds" {
#   source         = "./rds"
#   project_name   = var.project_name
#   subnets        = module.subnets.subnet_list
#   vpc_id         = aws_vpc.vpc.id
#   db_subnet      = module.subnets.db_subnet
# }

# module "efs" {
#   source         = "./efs"
#   project_name   = var.project_name
#   subnets        = module.subnets.subnet_list
#   region_code    = var.region_code
#   vpc_id         = aws_vpc.vpc.id
#   security_groups = module.security_group.security_groups
# } 

# module "redis" {
#   source         = "./redis"
#   project_name   = var.project_name
#   db_subnet      = module.subnets.subnet_list
#   vpc_id         = aws_vpc.vpc.id
#   region_code    = var.region_code
#   security_groups   = module.security_group.security_groups
# }

# module "endpoint" {
#   source         = "./endpoint"
#   project_name   = var.project_name
#   vpc_id         = aws_vpc.vpc.id
#   route_table    = module.route_table.private_rtb
# }

module "ec2" {
  source          = "./ec2"
  project_name    = var.project_name
  subnets         = module.subnets.subnet_list
  region_code     = var.region_code
  vpc_id          = aws_vpc.vpc.id
  eks_cluster     = module.eks.eks
  security_groups = module.security_group.security_groups
  iam_ssm_profile = module.iam.iam_roles.ssm_instance_profile
  iam_users       = module.iam.iam_users
  s3_list         = module.s3.s3_list
}

module "iam" {
  source       = "./iam"
  project_name = var.project_name
  s3_list      = module.s3.s3_list
}

# module "elb" {
#   source         = "./elb"
#   project_name   = var.project_name
#   subnets        = module.subnets.subnet_list
#   vpc_id         = aws_vpc.vpc.id
#   security_groups = module.security_group.security_groups
# }


--- [FILE]: ./ap_south_1/ap_south_1_outputs.tf ---
# VPC
output "vpc_id" {
  description = "The ID of the VPC"
  value       = aws_vpc.vpc.id
}


--- [FILE]: ./ap_south_1/s3/s3_outputs.tf ---
output "s3_list" {
  value = {
    # youngjin_s3 = aws_s3_bucket.youngjin_s3
    velero_bucket = aws_s3_bucket.velero_bucket
  }
}


--- [FILE]: ./ap_south_1/s3/s3_bucket_velero.tf ---
resource "aws_s3_bucket" "velero_bucket" {
  bucket = "${var.project_name}-velero-s3"
}


--- [FILE]: ./ap_south_1/s3/s3_bucket_main.tf_ ---
resource "aws_s3_bucket" "youngjin_s3" {
  bucket = "${var.project_name}-s3"
}


--- [FILE]: ./ap_south_1/s3/s3_variables.tf ---
variable "project_name" {
  description = "Project Name"
  type        = string
}

variable "s3_list" {
  description = "s3 list"
  type        = any
}


--- [FILE]: ./ap_south_1/igw/igw_outputs.tf ---
output "igw_id" {
  value = aws_internet_gateway.igw.id
}


--- [FILE]: ./ap_south_1/igw/igw_variables.tf ---
variable "vpc_id" {
  description = "인터넷 게이트웨이에 연결할 VPC ID"
  type        = string
}

variable "project_name" {
  description = "Project Name"
  type        = string
}


--- [FILE]: ./ap_south_1/igw/igw_main.tf ---
//  인터넷 게이트웨이 생성
resource "aws_internet_gateway" "igw" {
  vpc_id = var.vpc_id
  tags = {
    Name = "${var.project_name}-igw"
  }
}


--- [FILE]: ./ap_south_1/redis/redis_main.tf ---
module "this" {
  source    = "cloudposse/label/null"
  namespace = "prd"
  stage     = "youngjin"
  name      = "redis-cluster-01"
}

module "redis" {
  source = "cloudposse/elasticache-redis/aws"

  availability_zones         = ["${var.region_code}a", "${var.region_code}c"]
  vpc_id                     = var.vpc_id
  allowed_security_group_ids = [var.security_groups.redis_sg.id]
  subnets                    = [for subnet in var.db_subnet.private_pri_subnets : subnet.id]
  cluster_size               = "2"
  instance_type              = "cache.m6g.large"
  apply_immediately          = true
  automatic_failover_enabled = true
  engine_version             = "6.2"
  family                     = "redis6.x"

  context = module.this.context
}


--- [FILE]: ./ap_south_1/redis/redis_variables.tf ---
variable "vpc_id" {
  description = "VPC ID"
  type        = string
}

variable "project_name" {
  description = "Project Name"
  type        = string
}

variable "db_subnet" {
  description = "DB 서브넷 그룹"
  type        = any
}

variable "region_code" {
  description = "Region code"
  type        = string
}

variable "security_groups" {
  type = any
}


--- [FILE]: ./ap_south_1/nat/nat_variables.tf ---
variable "nat_subnets" {
  description = "NAT가 적용될 서브넷 목록"
  type        = any
}

variable "nat_eip" {
  description = "NAT가 적용될 eip 목록"
  type        = any
}

variable "vpc_id" {
  description = "VPC ID"
  type        = string
}
variable "project_name" {
  description = "Project Name"
  type        = string
}


--- [FILE]: ./ap_south_1/nat/nat_outputs.tf ---
output "nat" {
  value = aws_nat_gateway.nat
}


--- [FILE]: ./ap_south_1/nat/nat_main.tf ---
resource "aws_nat_gateway" "nat" {
  count = length(var.nat_eip)

  allocation_id = element(var.nat_eip.*.id, count.index)
  subnet_id     = element(var.nat_subnets.*.id, count.index)

  tags = {
    Name = "${var.project_name}-nat"
  }
}


--- [FILE]: ./ap_south_1/route_table/rt_db_route.tf ---
resource "aws_route_table" "db_rtb" {
  vpc_id = var.vpc_id
  tags = {
    Name = "${var.project_name}-rtb-db"
  }
}

resource "aws_route_table_association" "db_subnet" {
  count          = length(var.subnets.db_subnets)
  subnet_id      = element(concat(var.subnets.db_subnets.*.id), count.index)
  route_table_id = aws_route_table.db_rtb.id
}


--- [FILE]: ./ap_south_1/route_table/rt_public_route.tf ---
resource "aws_route_table" "public_rtb" {
  vpc_id = var.vpc_id
  tags = {
    Name = "${var.project_name}-rtb-pub"
  }
}

resource "aws_route" "public_rtb_igw_route" {
  route_table_id         = aws_route_table.public_rtb.id
  destination_cidr_block = "0.0.0.0/0"
  gateway_id             = var.igw_id
}

resource "aws_route_table_association" "public_subnet" {
  count          = length(var.subnets.public_pub_subnets)
  subnet_id      = element(var.subnets.public_pub_subnets.*.id, count.index)
  route_table_id = aws_route_table.public_rtb.id
}


--- [FILE]: ./ap_south_1/route_table/rt_private_route.tf ---
resource "aws_route_table" "private_rtb" {
  vpc_id = var.vpc_id
  tags = {
    Name = "${var.project_name}-rtb-pri"
  }
}

resource "aws_route" "private_rtb_nat_route" {
  route_table_id         = aws_route_table.private_rtb.id
  destination_cidr_block = "0.0.0.0/0"
  nat_gateway_id         = element(var.nat.*.id, 0)
}

resource "aws_route_table_association" "private_subnet" {
  count          = length(var.subnets.private_pri_subnets)
  subnet_id      = element(concat(var.subnets.private_pri_subnets.*.id), count.index)
  route_table_id = aws_route_table.private_rtb.id
}


--- [FILE]: ./ap_south_1/route_table/rt_variables.tf ---
variable "vpc_id" {
  description = "라우팅 테이블에 연결할 VPC ID"
  type        = string
}

variable "igw_id" {
  description = "생성된 Internet Gateway ID"
  type        = any
}

variable "nat" {
  description = "생성된 NAT 게이트웨이 목록"
  type        = list(any)
}

variable "subnets" {
  description = "생성된 서브넷 목록"
  type        = any
}

variable "project_name" {
  description = "Project Name"
  type        = string
}


--- [FILE]: ./ap_south_1/route_table/rt_outputs.tf ---
output "rtb_list" {
  value = {
    public_rtb      = [aws_route_table.public_rtb]
    private_pri_rtb = [aws_route_table.private_rtb]
    db_rtb          = [aws_route_table.db_rtb]
  }
}

output "public_rtb" {
  value = aws_route_table.public_rtb
}

output "private_rtb" {
  value = aws_route_table.private_rtb
}

output "db_rtb" {
  value = aws_route_table.db_rtb
}


--- [FILE]: ./ap_south_1/elb/elb_variables.tf ---
variable "vpc_id" {
  description = "VPC ID"
  type        = string
}
variable "project_name" {
  description = "Project Name"
  type        = string
}

variable "subnets" {
  description = "생성된 서브넷 목록"
  type        = any
}

variable "security_groups" {
  type = any
}


--- [FILE]: ./ap_south_1/elb/elb_main.tf ---
# ALB 생성
module "alb" {
  source  = "terraform-aws-modules/alb/aws"
  version = "~> 8.0" # 사용 중인 모듈 버전에 맞게 수정

  name               = "${var.project_name}-alb"
  load_balancer_type = "application"
  internal           = false
  subnets            = [for subnet in var.subnets.public_pub_subnets.id : subnet.id] # 퍼블릭 서브넷 ID 목록
  security_groups    = [var.security_groups.alb_sg.id]                               # ALB에 적용할 보안 그룹 ID

}

# # ALB 리스너 생성 (HTTP:80)
# module "alb_listener" {
#   source  = "terraform-aws-modules/alb/aws//modules/listener"
#   version = "~> 8.0"

#   load_balancer_arn = module.alb.this_lb_arn
#   port              = 80
#   protocol          = "HTTP"

#   default_action = {
#     type = "fixed-response"
#     fixed_response = {
#       content_type = "text/plain"
#       message_body = "OK"
#       status_code  = "200"
#     }
#   }
# }

# # (옵션) ALB 타겟 그룹 생성 예시
# module "alb_target_group" {
#   source  = "terraform-aws-modules/alb/aws//modules/target-group"
#   version = "~> 8.0"

#   name     = "${var.project_name}-tg"
#   port     = 80
#   protocol = "HTTP"
#   vpc_id   = var.vpc_id

#   health_check = {
#     healthy_threshold   = 3
#     unhealthy_threshold = 3
#     timeout             = 5
#     interval            = 30
#     path                = "/"
#     matcher             = "200-299"
#   }

#   tags = {
#     Environment = var.environment
#     Project     = var.project_name
#   }
# }

# # (옵션) ALB 리스너 규칙을 통해 타겟 그룹으로 트래픽 전달
# module "alb_listener_rule" {
#   source  = "terraform-aws-modules/alb/aws//modules/listener-rule"
#   version = "~> 8.0"

#   listener_arn = module.alb_listener.this_listener_arn
#   priority     = 100

#   actions = [
#     {
#       type             = "forward"
#       target_group_arn = module.alb_target_group.this_target_group_arn
#     }
#   ]

#   conditions = [
#     {
#       field  = "path-pattern"
#       values = ["/app/*"]
#     }
#   ]
# }


--- [FILE]: ./ap_south_1/security_groups/locals.tf ---
locals {
  bastion_sg_name    = "${var.project_name}-bastion-sg"
  bastion_sg_desc    = local.bastion_sg_name
  redis_sg_name      = "${var.project_name}-redis-sg"
  redis_sg_desc      = local.redis_sg_name
  alb_sg_name        = "${var.project_name}-alb-sg"
  alb_sg_desc        = local.redis_sg_name
  description_suffix = "by terraform"
}


--- [FILE]: ./ap_south_1/security_groups/sg_variables.tf ---
variable "project_name" {
  description = "Project Name"
  type        = string
}

variable "vpc_id" {
  description = "The ID of the VPC"
  type        = string
  default     = ""
}


--- [FILE]: ./ap_south_1/security_groups/sg_outputs.tf ---
output "security_groups" {
  value = {
    redis_sg   = aws_security_group.redis_sg
    bastion_sg = aws_security_group.bastion_sg
    alb_sg     = aws_security_group.alb_sg
  }
}


--- [FILE]: ./ap_south_1/security_groups/redis_sg.tf ---
// 보안 그룹 생성
resource "aws_security_group" "redis_sg" {
  vpc_id      = var.vpc_id
  name        = local.redis_sg_name
  description = local.redis_sg_desc

  tags = {
    Name   = local.redis_sg_name
    t_addr = "${path.module}/redis_sg.tf"
  }
  lifecycle {
    ignore_changes = [
      tags["CreateDate"],
    ]
  }
}

### 인바운드
############

resource "aws_security_group_rule" "redis_sg_rule_ingress_0" {
  security_group_id = aws_security_group.redis_sg.id
  type              = "ingress"
  protocol          = "tcp"
  from_port         = 6379
  to_port           = 6379
  cidr_blocks       = ["150.0.0.0/16"]
  description       = "redis sg ${local.description_suffix}"
  lifecycle {
    ignore_changes = [
      # description,
    ]
  }
}

resource "aws_security_group_rule" "redis_sg_rule_egress_0" {
  security_group_id = aws_security_group.redis_sg.id
  type              = "egress"
  protocol          = "tcp"
  from_port         = 3306
  to_port           = 3306
  cidr_blocks       = ["150.0.0.0/16"]
  description       = "redis sg to rds ${local.description_suffix}"
  lifecycle {
    ignore_changes = [
      # description,
    ]
  }
}


--- [FILE]: ./ap_south_1/security_groups/alb_sg.tf ---
// 보안 그룹 생성
resource "aws_security_group" "alb_sg" {
  vpc_id      = var.vpc_id
  name        = local.alb_sg_name
  description = local.alb_sg_desc

  tags = {
    Name   = local.alb_sg_name
    t_addr = "${path.module}/alb.sg.tf"
  }
  lifecycle {
    ignore_changes = [
      tags["CreateDate"],
    ]
  }
}

### 인바운드
############
resource "aws_security_group_rule" "alb_sg_rule_ingress_0" {
  security_group_id = aws_security_group.alb_sg.id
  type              = "ingress"
  protocol          = "tcp"
  from_port         = 443
  to_port           = 443
  cidr_blocks       = ["0.0.0.0/0"]
  description       = "sg for alb ${local.description_suffix}"
  lifecycle {
    ignore_changes = [
      # description,
    ]
  }
}
resource "aws_security_group_rule" "alb_sg_rule_ingress_1" {
  security_group_id = aws_security_group.alb_sg.id
  type              = "ingress"
  protocol          = "tcp"
  from_port         = 80
  to_port           = 80
  cidr_blocks       = ["0.0.0.0/0"]
  description       = "sg for alb ${local.description_suffix}"
  lifecycle {
    ignore_changes = [
      # description,
    ]
  }
}
### 아웃바운드
############
resource "aws_security_group_rule" "alb_sg_rule_egress_0" {
  security_group_id = aws_security_group.alb_sg.id
  type              = "egress"
  protocol          = "tcp"
  from_port         = 443
  to_port           = 443
  cidr_blocks       = ["150.0.0.0/8"]
  description       = "sg for alb ${local.description_suffix}"
  lifecycle {
    ignore_changes = [
      # description,
    ]
  }
}

resource "aws_security_group_rule" "alb_sg_rule_egress_1" {
  security_group_id = aws_security_group.alb_sg.id
  type              = "egress"
  protocol          = "tcp"
  from_port         = 80
  to_port           = 80
  cidr_blocks       = ["150.0.0.0/8"]
  description       = "sg for alb ${local.description_suffix}"
  lifecycle {
    ignore_changes = [
      # description,
    ]
  }
}


--- [FILE]: ./ap_south_1/security_groups/bastion_sg.tf ---
// 보안 그룹 생성
resource "aws_security_group" "bastion_sg" {
  vpc_id      = var.vpc_id
  name        = local.bastion_sg_name
  description = local.bastion_sg_desc

  tags = {
    Name   = local.bastion_sg_name
    t_addr = "${path.module}/bastion.sg.tf"
  }
  lifecycle {
    ignore_changes = [
      tags["CreateDate"],
    ]
  }
}

### 아웃바운드
############
resource "aws_security_group_rule" "bastion_sg_rule_egress_0" {
  security_group_id = aws_security_group.bastion_sg.id
  type              = "egress"
  protocol          = "tcp"
  from_port         = 443
  to_port           = 443
  cidr_blocks       = ["0.0.0.0/0"]
  description       = "sg for bastion ${local.description_suffix}"
  lifecycle {
    ignore_changes = [
      # description,
    ]
  }
}


--- [FILE]: ./ap_south_1/iam/iam_role_eksNodegroup.tf ---
resource "aws_iam_role" "eks_node_role" {
  name               = "${var.project_name}-eks-node-role"
  assume_role_policy = data.aws_iam_policy_document.eks_node_trust.json
}

data "aws_iam_policy_document" "eks_node_trust" {
  statement {
    actions = ["sts:AssumeRole"]
    principals {
      type        = "Service"
      identifiers = ["ec2.amazonaws.com"]
    }
  }
}

# 노드 관련 정책 연결
resource "aws_iam_role_policy_attachment" "eks_workernode_policy" {
  role       = aws_iam_role.eks_node_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy"
}

resource "aws_iam_role_policy_attachment" "eks_cni_policy" {
  role       = aws_iam_role.eks_node_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy"
}

resource "aws_iam_role_policy_attachment" "ecr_read_policy" {
  role       = aws_iam_role.eks_node_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"
}


--- [FILE]: ./ap_south_1/iam/iam_variables.tf ---
variable "project_name" {
  description = "Project Name"
  type        = string
}

variable "s3_list" {
  description = "List of S3 Buckets"
  type        = any
}


--- [FILE]: ./ap_south_1/iam/iam_role_ssm.tf ---
# (1) IAM 역할 생성
resource "aws_iam_role" "ssm_role" {
  name = "${var.project_name}-role-ssm"
  assume_role_policy = jsonencode({
    Version = "2012-10-17",
    Statement = [
      {
        Effect = "Allow",
        Principal = {
          Service = "ec2.amazonaws.com"
        },
        Action = "sts:AssumeRole"
      }
    ]
  })
}

# (2) AmazonSSMManagedInstanceCore 정책을 역할에 연결
resource "aws_iam_role_policy_attachment" "bastion_attach_admin_policy" {
  role       = aws_iam_role.ssm_role.name
  policy_arn = "arn:aws:iam::aws:policy/AdministratorAccess"
}

# (3) 인스턴스 프로파일 생성
resource "aws_iam_instance_profile" "ssm_instance_profile" {
  name = "${var.project_name}-ssm-instance-profile"
  role = aws_iam_role.ssm_role.name
}


--- [FILE]: ./ap_south_1/iam/iam_role_karpenterNode.tf_ ---
#############################
#Karpenter IAM Role 생성
#############################
resource "aws_iam_role" "karpenter_node_role" {
  name = "${var.project_name}-karpenter-node-role"
  assume_role_policy = jsonencode({
    "Version" : "2012-10-17",
    "Statement" : [
      {
        "Effect" : "Allow",
        "Principal" : {
          "Service" : "ec2.amazonaws.com"
        },
        "Action" : "sts:AssumeRole"
      }
    ]
  })
}
#############################
#Role에 Policy Attach
#############################
resource "aws_iam_role_policy_attachment" "karpenter_worker_node_policy" {
  role       = aws_iam_role.karpenter_node_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy"
}

resource "aws_iam_role_policy_attachment" "karpenter_cni_policy" {
  role       = aws_iam_role.karpenter_node_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy"
}

resource "aws_iam_role_policy_attachment" "karpenter_ecr_policy" {
  role       = aws_iam_role.karpenter_node_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"
}

resource "aws_iam_role_policy_attachment" "karpenter_ssm_policy" {
  role       = aws_iam_role.karpenter_node_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore"
}

#############################
#Karpenter Instance Profile 생성
#############################
resource "aws_iam_instance_profile" "karpenter_instance_profile" {
  name = "${var.project_name}-karpenter-instance-profile"
  role = aws_iam_role.karpenter_node_role.name
}


--- [FILE]: ./ap_south_1/iam/iam_outputs.tf ---
output "iam_roles" {
  value = {
    ssm_role             = aws_iam_role.ssm_role
    eks_node_role        = aws_iam_role.eks_node_role
    eks_cluster_role     = aws_iam_role.eks_cluster_role
    ssm_instance_profile = aws_iam_instance_profile.ssm_instance_profile
    # karpenter_node_role = aws_iam_role.karpenter_node_role
  }
}

output "iam_users" {
  value = {
    velero_user = aws_iam_access_key.velero
  }
}


--- [FILE]: ./ap_south_1/iam/iam_role_eksCluster.tf ---
resource "aws_iam_role" "eks_cluster_role" {
  name               = "${var.project_name}-cluster-role"
  assume_role_policy = data.aws_iam_policy_document.eks_cluster_trust.json
}

data "aws_iam_policy_document" "eks_cluster_trust" {
  statement {
    actions = ["sts:AssumeRole"]
    principals {
      type        = "Service"
      identifiers = ["eks.amazonaws.com"]
    }
  }
}

# EKS 기본 정책 연결
resource "aws_iam_role_policy_attachment" "eks_cluster_policy_attach" {
  role       = aws_iam_role.eks_cluster_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSClusterPolicy"
}

resource "aws_iam_role_policy_attachment" "eks_service_policy_attach" {
  role       = aws_iam_role.eks_cluster_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSServicePolicy"
}

resource "aws_iam_role_policy_attachment" "eks_vpc_policy_attach" {
  role       = aws_iam_role.eks_cluster_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSVPCResourceController"
}


--- [FILE]: ./ap_south_1/iam/iam_user_velero.tf ---
resource "aws_iam_user" "velero" {
  name = "${var.project_name}-velero"
}

resource "aws_iam_policy" "velero_policy" {
  name        = "${var.project_name}_VeleroPolicy"
  description = "Policy for Velero to backup and restore EKS resources."
  policy      = jsonencode({
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "ec2:DescribeVolumes",
                "ec2:DescribeSnapshots",
                "ec2:CreateTags",
                "ec2:CreateVolume",
                "ec2:CreateSnapshot",
                "ec2:DeleteSnapshot"
            ],
            "Resource": "*"
        },
        {
            "Effect": "Allow",
            "Action": [
                "s3:GetObject",
                "s3:DeleteObject",
                "s3:PutObject",
                "s3:PutObjectTagging",
                "s3:AbortMultipartUpload",
                "s3:ListMultipartUploadParts"
            ],
            "Resource": [
                "${var.s3_list.velero_bucket.arn}/*"
            ]
        },
        {
            "Effect": "Allow",
            "Action": [
                "s3:ListBucket"
            ],
            "Resource": [
                "${var.s3_list.velero_bucket.arn}"
            ]
        }
    ]
})
}

resource "aws_iam_user_policy_attachment" "velero_policy_attach" {
  user       = aws_iam_user.velero.name
  policy_arn = aws_iam_policy.velero_policy.arn
}

resource "aws_iam_access_key" "velero" {
  user = aws_iam_user.velero.name
}

--- [FILE]: ./ap_south_1/eks/eks_variables.tf ---
variable "vpc_id" {
  description = "라우팅 테이블에 연결할 VPC ID"
  type        = string
}

variable "subnets" {
  description = "생성된 서브넷 목록"
  type        = any
}

variable "project_name" {
  description = "Project Name"
  type        = string
}
variable "iam_roles" {
  description = "IAM Role 목록"
  type        = any
}

variable "security_groups" {
  type = any
}


--- [FILE]: ./ap_south_1/eks/eks_main.tf ---
provider "helm" {
  kubernetes {
    host                   = module.eks.cluster_endpoint
    cluster_ca_certificate = base64decode(module.eks.cluster_certificate_authority_data)

    exec {
      api_version = "client.authentication.k8s.io/v1beta1"
      command     = "aws"
      # This requires the awscli to be installed locally where Terraform is executed
      args = ["eks", "get-token", "--cluster-name", module.eks.cluster_name]
    }
  }
}

# provider "kubectl" {
#   host                   = module.eks.cluster_endpoint
#   cluster_ca_certificate = base64decode(module.eks.cluster_certificate_authority_data)
#   load_config_file       = false

#   exec {
#     api_version = "client.authentication.k8s.io/v1beta1"
#     command     = "aws"
#     # This requires the awscli to be installed locally where Terraform is executed
#     args = ["eks", "get-token", "--cluster-name", module.eks.cluster_name]
#   }
# }

provider "aws" {
  region = "us-east-1"
  alias  = "us_east"
}

data "aws_ecrpublic_authorization_token" "token" {
  provider = aws.us_east
}

locals {
  oidc_provider = replace(module.eks.cluster_oidc_issuer_url, "https://", "")
  name          = "${var.project_name}-eks-cluster-01"
  tags = {
    Example    = "${var.project_name}-eks-karpenter"
    GithubRepo = "terraform-aws-eks"
    GithubOrg  = "terraform-aws-modules"
    Project    = "${var.project_name}-eks-cluster"
    # "karpenter.sh/discovery" = "${var.project_name}-eks-cluster-01"
  }
}

# EKS Cluster 생성
module "eks" {
  source  = "terraform-aws-modules/eks/aws"
  version = "~> 20.31"

  cluster_name    = local.name
  vpc_id          = var.vpc_id
  subnet_ids      = [for subnet in var.subnets.eks_subnets : subnet.id]
  cluster_version = "1.30"

  cluster_endpoint_public_access  = true
  cluster_endpoint_private_access = true

  create_iam_role = false
  iam_role_arn    = var.iam_roles.eks_cluster_role.arn

  # 자동으로 cluster creator(현재 Terraform을 실행하는 주체)를 admin으로 등록하지 않도록 하려면 false로 설정
  enable_cluster_creator_admin_permissions = true

  # Bastion에서 EKS API 서버(443)에 접근할 수 있도록 추가 규칙 추가
  cluster_security_group_additional_rules = {
    bastion_access = {
      description              = "Allow Bastion SG access to EKS API server"
      type                     = "ingress"
      protocol                 = "tcp"
      from_port                = 443
      to_port                  = 443
      source_security_group_id = var.security_groups.bastion_sg.id
    }
  }

  eks_managed_node_groups = {
    "${var.project_name}-nodegroup-1" = {
      # Starting on 1.30, AL2023 is the default AMI type for EKS managed node groups
      ami_type = "BOTTLEROCKET_x86_64"
      # ami_type = "AL2_x86_64"

      instance_types = ["t3a.large"]

      create_worker_iam_role = false
      worker_iam_role_arn    = var.iam_roles.eks_node_role.arn
      name                   = "${var.project_name}-ng-1"
      create_launch_template = true
      launch_template_name   = "${var.project_name}-ng-1-lt"

      subnet_ids = [for subnet in var.subnets.private_pri_subnets : subnet.id]
      # Amazon Linux 2 node groups do not require a specific settings block.
      # Any custom configuration (e.g. user data modifications) should be handled via a launch template if needed.
      min_size     = 3
      max_size     = 3
      desired_size = 3
      labels = {
        # Used to ensure Karpenter runs on nodes that it does not manage
        "karpenter.sh/controller" = "true"
      }
    }
  }

  access_entries = {
    # One access entry with a policy associated
    ssm_role_access = {
      principal_arn = var.iam_roles.ssm_role.arn

      policy_associations = {
        eks_admin_policy = {
          policy_arn = "arn:aws:eks::aws:cluster-access-policy/AmazonEKSAdminPolicy"
          access_scope = {
            type = "cluster"
          }
        }
        cluster_admin_policy = {
          policy_arn = "arn:aws:eks::aws:cluster-access-policy/AmazonEKSClusterAdminPolicy"
          access_scope = {
            type = "cluster"
          }
        }
      }
    }
  }

  node_security_group_tags = merge(local.tags, {
    # NOTE - if creating multiple security groups with this module, only tag the
    # security group that Karpenter should utilize with the following tag
    # (i.e. - at most, only one security group should have this tag in your account)
    "karpenter.sh/discovery" = local.name
  })
  tags = local.tags
}

module "karpenter" {
  source                 = "terraform-aws-modules/eks/aws//modules/karpenter"
  enable_irsa            = true
  irsa_oidc_provider_arn = module.eks.oidc_provider_arn
  cluster_name           = local.name
  enable_v1_permissions  = true

  # Name needs to match role name passed to the EC2NodeClass
  node_iam_role_use_name_prefix   = false
  node_iam_role_arn               = module.eks.eks_managed_node_groups["${var.project_name}-nodegroup-1"].iam_role_arn
  create_pod_identity_association = true

  # Used to attach additional IAM policies to the Karpenter node IAM role
  node_iam_role_additional_policies = {
    AmazonSSMManagedInstanceCore = "arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore"
  }

  tags = local.tags
}

module "karpenter_disabled" {
  source = "terraform-aws-modules/eks/aws//modules/karpenter"

  create = false
}

################################################################################
# Karpenter Helm chart & manifests
# Not required; just to demonstrate functionality of the sub-module
################################################################################

resource "helm_release" "karpenter" {
  namespace           = "kube-system"
  name                = "karpenter"
  repository          = "oci://public.ecr.aws/karpenter"
  repository_username = data.aws_ecrpublic_authorization_token.token.user_name
  repository_password = data.aws_ecrpublic_authorization_token.token.password
  chart               = "karpenter"
  version             = "1.3.3"
  wait                = false

  values = [
    <<-EOT
    nodeSelector:
      karpenter.sh/controller: 'true'
    dnsPolicy: Default
    settings:
      clusterName: ${module.eks.cluster_name}
      clusterEndpoint: ${module.eks.cluster_endpoint}
      interruptionQueue: ${module.karpenter.queue_name}
    webhook:
      enabled: false
    EOT
  ]

  set {
    name  = "serviceAccount.annotations.eks\\.amazonaws\\.com/role-arn"
    value = module.karpenter.iam_role_arn
  }

}

resource "aws_eks_addon" "ebs_csi" {
  cluster_name                = module.eks.cluster_name
  addon_name                  = "aws-ebs-csi-driver"
  addon_version               = "v1.41.0-eksbuild.1"
  resolve_conflicts_on_create = "OVERWRITE"
  depends_on                  = [module.eks]
}

resource "aws_eks_addon" "efs_csi" {
  cluster_name                = module.eks.cluster_name
  addon_name                  = "aws-efs-csi-driver"
  addon_version               = "v2.1.7-eksbuild.1"
  resolve_conflicts_on_create = "OVERWRITE"
  depends_on                  = [module.eks]
}

resource "aws_eks_addon" "coredns" {
  cluster_name                = module.eks.cluster_name
  addon_name                  = "coredns"
  addon_version               = "v1.11.1-eksbuild.9"
  resolve_conflicts_on_create = "OVERWRITE"
  depends_on                  = [module.eks]
}

resource "aws_eks_addon" "kube_proxy" {
  cluster_name                = module.eks.cluster_name
  addon_name                  = "kube-proxy"
  addon_version               = "v1.30.6-eksbuild.3"
  resolve_conflicts_on_create = "OVERWRITE"
  depends_on                  = [module.eks]
}

resource "aws_eks_addon" "vpc_cni" {
  cluster_name                = module.eks.cluster_name
  addon_name                  = "vpc-cni"
  addon_version               = "v1.19.0-eksbuild.1"
  resolve_conflicts_on_create = "OVERWRITE"
  depends_on                  = [module.eks]
  configuration_values = jsonencode({
    env = {
      ENABLE_PREFIX_DELEGATION = "true"
      WARM_PREFIX_TARGET       = "1"
    }
  })
}
resource "kubectl_manifest" "karpenter_node_template" {
  yaml_body = <<-YAML
    apiVersion: karpenter.k8s.aws/v1alpha1
    kind: AWSNodeTemplate
    metadata:
      name: default
    spec:
      blockDeviceMappings:
        - deviceName: /dev/xvda
          ebs:
            volumeSize: 10
            volumeType: gp3
      subnetSelector:
        karpenter.sh/discovery: ${module.eks.cluster_name}
      securityGroupSelector:
        karpenter.sh/discovery: ${module.eks.cluster_name}
      tags:
        karpenter.sh/discovery: ${module.eks.cluster_name}
  YAML

  depends_on = [
    helm_release.karpenter
  ]
}

resource "kubectl_manifest" "karpenter_provisioner" {
  yaml_body = <<-YAML
apiVersion: karpenter.sh/v1alpha5
kind: Provisioner
metadata:
  name: default
spec:
  labels:
    role: webapp
  requirements:
    - key: "node.kubernetes.io/instance-type"
      operator: In
      values: ["t3a.large"]
    - key: karpenter.sh/capacity-type
      operator: In
      values: ["on-demand", "spot"]
  limits:
    resources:
      cpu: 50
      memory: 192Gi
  providerRef:
    name: default
  ttlSecondsUntilExpired: 2592000 # 30d
  ttlSecondsAfterEmpty: 30
YAML

  depends_on = [helm_release.karpenter]
}
# #############################
# # Karpenter Controller IAM Role 생성
# #############################
# resource "aws_iam_role" "karpenter_controller_role" {
#   name = "${var.project_name}-karpenter-controller-role"
#   assume_role_policy = jsonencode({
#     "Version" : "2012-10-17",
#     "Statement" : [
#       {
#         "Effect" : "Allow",
#         "Principal" : {
#           "Federated" : module.eks.oidc_provider_arn
#         },
#         "Action" : "sts:AssumeRoleWithWebIdentity",
#         "Condition" : {
#           "StringEquals" : {
#             "${local.oidc_provider}:sub" : "system:serviceaccount:karpenter:karpenter",
#             "${local.oidc_provider}:aud" : "sts.amazonaws.com"
#           }
#         }
#       }
#     ]
#   })
# }
# #############################
# # Karpenter Controller IAM Role에 Policy Attach
# #############################
# resource "aws_iam_role_policy_attachment" "karpenter_controller_role_attach" {
#   role       = aws_iam_role.karpenter_controller_role.name
#   policy_arn = aws_iam_policy.karpenter_controller_policy.arn
# }

# resource "aws_iam_policy" "karpenter_controller_policy" {
#   name        = "${var.project_name}-karpenter-controller-policy"
#   description = "Policy for AWS Karpenter Controller to manage Karpenter Nodes."
#   policy = jsonencode({
#     "Statement" : [
#       {
#         "Action" : [
#           "ssm:GetParameter",
#           "ec2:DescribeImages",
#           "ec2:RunInstances",
#           "ec2:DescribeSubnets",
#           "ec2:DescribeSecurityGroups",
#           "ec2:DescribeLaunchTemplates",
#           "ec2:DescribeInstances",
#           "ec2:DescribeInstanceTypes",
#           "ec2:DescribeInstanceTypeOfferings",
#           "ec2:DescribeAvailabilityZones",
#           "ec2:DeleteLaunchTemplate",
#           "ec2:CreateTags",
#           "ec2:CreateLaunchTemplate",
#           "ec2:CreateFleet",
#           "ec2:DescribeSpotPriceHistory",
#           "pricing:GetProducts"
#         ],
#         "Effect" : "Allow",
#         "Resource" : "*",
#         "Sid" : "Karpenter"
#       },
#       {
#         "Action" : "ec2:TerminateInstances",
#         "Condition" : {
#           "StringLike" : {
#             "ec2:ResourceTag/karpenter.sh/provisioner-name" : "*"
#           }
#         },
#         "Effect" : "Allow",
#         "Resource" : "*",
#         "Sid" : "ConditionalEC2Termination"
#       },
#       {
#         "Effect" : "Allow",
#         "Action" : "iam:PassRole",
#         "Resource" : module.karpenter.node_iam_role_arn,
#         "Sid" : "PassNodeIAMRole"
#       },
#       {
#         "Effect" : "Allow",
#         "Action" : "eks:DescribeCluster",
#         "Resource" : "${module.eks.cluster_arn}",
#         "Sid" : "EKSClusterEndpointLookup"
#       }
#     ],
#     "Version" : "2012-10-17"
#   })
# }

#############################
# ALB Controller IAM Role 생성
#############################
resource "aws_iam_role" "alb_controller_role" {
  name = "${var.project_name}-alb-controller-role"
  assume_role_policy = jsonencode({
    "Version" : "2012-10-17",
    "Statement" : [
      {
        "Effect" : "Allow",
        "Principal" : {
          "Federated" : module.eks.oidc_provider_arn
        },
        "Action" : "sts:AssumeRoleWithWebIdentity",
        "Condition" : {
          "StringEquals" : {
            "${local.oidc_provider}:sub" : "system:serviceaccount:kube-system:aws-load-balancer-controller",
            "${local.oidc_provider}:aud" : "sts.amazonaws.com"
          }
        }
      }
    ]
  })
}

#############################
#Role에 Policy Attach
#############################

resource "aws_iam_role_policy_attachment" "alb_controller_role_attach" {
  role       = aws_iam_role.alb_controller_role.name
  policy_arn = aws_iam_policy.alb_controller_policy.arn
}

resource "aws_iam_policy" "alb_controller_policy" {
  name        = "${var.project_name}_ALBControllerPolicy"
  description = "Policy for AWS ALB Controller to manage ELB resources."
  policy = jsonencode({
    "Version" : "2012-10-17",
    "Statement" : [
      {
        "Effect" : "Allow",
        "Action" : [
          "iam:CreateServiceLinkedRole"
        ],
        "Resource" : "*",
        "Condition" : {
          "StringEquals" : {
            "iam:AWSServiceName" : "elasticloadbalancing.amazonaws.com"
          }
        }
      },
      {
        "Effect" : "Allow",
        "Action" : [
          "ec2:DescribeAccountAttributes",
          "ec2:DescribeAddresses",
          "ec2:DescribeAvailabilityZones",
          "ec2:DescribeInternetGateways",
          "ec2:DescribeVpcs",
          "ec2:DescribeVpcPeeringConnections",
          "ec2:DescribeSubnets",
          "ec2:DescribeSecurityGroups",
          "ec2:DescribeInstances",
          "ec2:DescribeNetworkInterfaces",
          "ec2:DescribeTags",
          "ec2:GetCoipPoolUsage",
          "ec2:DescribeCoipPools",
          "ec2:GetSecurityGroupsForVpc",
          "elasticloadbalancing:DescribeLoadBalancers",
          "elasticloadbalancing:DescribeLoadBalancerAttributes",
          "elasticloadbalancing:DescribeListeners",
          "elasticloadbalancing:DescribeListenerCertificates",
          "elasticloadbalancing:DescribeSSLPolicies",
          "elasticloadbalancing:DescribeRules",
          "elasticloadbalancing:DescribeTargetGroups",
          "elasticloadbalancing:DescribeTargetGroupAttributes",
          "elasticloadbalancing:DescribeTargetHealth",
          "elasticloadbalancing:DescribeTags",
          "elasticloadbalancing:DescribeTrustStores",
          "elasticloadbalancing:DescribeListenerAttributes",
          "elasticloadbalancing:DescribeCapacityReservation"
        ],
        "Resource" : "*"
      },
      {
        "Effect" : "Allow",
        "Action" : [
          "cognito-idp:DescribeUserPoolClient",
          "acm:ListCertificates",
          "acm:DescribeCertificate",
          "iam:ListServerCertificates",
          "iam:GetServerCertificate",
          "waf-regional:GetWebACL",
          "waf-regional:GetWebACLForResource",
          "waf-regional:AssociateWebACL",
          "waf-regional:DisassociateWebACL",
          "wafv2:GetWebACL",
          "wafv2:GetWebACLForResource",
          "wafv2:AssociateWebACL",
          "wafv2:DisassociateWebACL",
          "shield:GetSubscriptionState",
          "shield:DescribeProtection",
          "shield:CreateProtection",
          "shield:DeleteProtection"
        ],
        "Resource" : "*"
      },
      {
        "Effect" : "Allow",
        "Action" : [
          "ec2:AuthorizeSecurityGroupIngress",
          "ec2:RevokeSecurityGroupIngress"
        ],
        "Resource" : "*"
      },
      {
        "Effect" : "Allow",
        "Action" : [
          "ec2:CreateSecurityGroup"
        ],
        "Resource" : "*"
      },
      {
        "Effect" : "Allow",
        "Action" : [
          "ec2:CreateTags"
        ],
        "Resource" : "arn:aws:ec2:*:*:security-group/*",
        "Condition" : {
          "StringEquals" : {
            "ec2:CreateAction" : "CreateSecurityGroup"
          },
          "Null" : {
            "aws:RequestTag/elbv2.k8s.aws/cluster" : "false"
          }
        }
      },
      {
        "Effect" : "Allow",
        "Action" : [
          "ec2:CreateTags",
          "ec2:DeleteTags"
        ],
        "Resource" : "arn:aws:ec2:*:*:security-group/*",
        "Condition" : {
          "Null" : {
            "aws:RequestTag/elbv2.k8s.aws/cluster" : "true",
            "aws:ResourceTag/elbv2.k8s.aws/cluster" : "false"
          }
        }
      },
      {
        "Effect" : "Allow",
        "Action" : [
          "ec2:AuthorizeSecurityGroupIngress",
          "ec2:RevokeSecurityGroupIngress",
          "ec2:DeleteSecurityGroup"
        ],
        "Resource" : "*",
        "Condition" : {
          "Null" : {
            "aws:ResourceTag/elbv2.k8s.aws/cluster" : "false"
          }
        }
      },
      {
        "Effect" : "Allow",
        "Action" : [
          "elasticloadbalancing:CreateLoadBalancer",
          "elasticloadbalancing:CreateTargetGroup"
        ],
        "Resource" : "*",
        "Condition" : {
          "Null" : {
            "aws:RequestTag/elbv2.k8s.aws/cluster" : "false"
          }
        }
      },
      {
        "Effect" : "Allow",
        "Action" : [
          "elasticloadbalancing:CreateListener",
          "elasticloadbalancing:DeleteListener",
          "elasticloadbalancing:CreateRule",
          "elasticloadbalancing:DeleteRule"
        ],
        "Resource" : "*"
      },
      {
        "Effect" : "Allow",
        "Action" : [
          "elasticloadbalancing:AddTags",
          "elasticloadbalancing:RemoveTags"
        ],
        "Resource" : [
          "arn:aws:elasticloadbalancing:*:*:targetgroup/*/*",
          "arn:aws:elasticloadbalancing:*:*:loadbalancer/net/*/*",
          "arn:aws:elasticloadbalancing:*:*:loadbalancer/app/*/*"
        ],
        "Condition" : {
          "Null" : {
            "aws:RequestTag/elbv2.k8s.aws/cluster" : "true",
            "aws:ResourceTag/elbv2.k8s.aws/cluster" : "false"
          }
        }
      },
      {
        "Effect" : "Allow",
        "Action" : [
          "elasticloadbalancing:AddTags",
          "elasticloadbalancing:RemoveTags"
        ],
        "Resource" : [
          "arn:aws:elasticloadbalancing:*:*:listener/net/*/*/*",
          "arn:aws:elasticloadbalancing:*:*:listener/app/*/*/*",
          "arn:aws:elasticloadbalancing:*:*:listener-rule/net/*/*/*",
          "arn:aws:elasticloadbalancing:*:*:listener-rule/app/*/*/*"
        ]
      },
      {
        "Effect" : "Allow",
        "Action" : [
          "elasticloadbalancing:ModifyLoadBalancerAttributes",
          "elasticloadbalancing:SetIpAddressType",
          "elasticloadbalancing:SetSecurityGroups",
          "elasticloadbalancing:SetSubnets",
          "elasticloadbalancing:DeleteLoadBalancer",
          "elasticloadbalancing:ModifyTargetGroup",
          "elasticloadbalancing:ModifyTargetGroupAttributes",
          "elasticloadbalancing:DeleteTargetGroup",
          "elasticloadbalancing:ModifyListenerAttributes",
          "elasticloadbalancing:ModifyCapacityReservation"
        ],
        "Resource" : "*",
        "Condition" : {
          "Null" : {
            "aws:ResourceTag/elbv2.k8s.aws/cluster" : "false"
          }
        }
      },
      {
        "Effect" : "Allow",
        "Action" : [
          "elasticloadbalancing:AddTags"
        ],
        "Resource" : [
          "arn:aws:elasticloadbalancing:*:*:targetgroup/*/*",
          "arn:aws:elasticloadbalancing:*:*:loadbalancer/net/*/*",
          "arn:aws:elasticloadbalancing:*:*:loadbalancer/app/*/*"
        ],
        "Condition" : {
          "StringEquals" : {
            "elasticloadbalancing:CreateAction" : [
              "CreateTargetGroup",
              "CreateLoadBalancer"
            ]
          },
          "Null" : {
            "aws:RequestTag/elbv2.k8s.aws/cluster" : "false"
          }
        }
      },
      {
        "Effect" : "Allow",
        "Action" : [
          "elasticloadbalancing:RegisterTargets",
          "elasticloadbalancing:DeregisterTargets"
        ],
        "Resource" : "arn:aws:elasticloadbalancing:*:*:targetgroup/*/*"
      },
      {
        "Effect" : "Allow",
        "Action" : [
          "elasticloadbalancing:SetWebAcl",
          "elasticloadbalancing:ModifyListener",
          "elasticloadbalancing:AddListenerCertificates",
          "elasticloadbalancing:RemoveListenerCertificates",
          "elasticloadbalancing:ModifyRule"
        ],
        "Resource" : "*"
      }
    ]
  })
}


--- [FILE]: ./ap_south_1/eks/eks_output.tf ---
output "eks" {
  value = {
    cluster_name     = module.eks.cluster_name
    cluster_version  = module.eks.cluster_version
  }
}


--- [FILE]: ./ap_south_1/endpoint/endpoint_main.tf ---
module "endpoints" {
  source = "terraform-aws-modules/vpc/aws//modules/vpc-endpoints"
  vpc_id = var.vpc_id

  endpoints = {
    s3 = {
      service         = "s3"
      endpoints       = "gateway"
      route_table_ids = ["${var.route_table.id}"]
      tags            = { Name = "${var.project_name}-youngjin-s3-endpoint" }
    },
  }
}


--- [FILE]: ./ap_south_1/endpoint/endpoint_variables.tf ---
variable "vpc_id" {
  description = "VPC ID"
  type        = string
}

variable "project_name" {
  description = "Project Name"
  type        = string
}

variable "route_table" {
  description = "Route table list"
  type        = any
}


--- [FILE]: ./ap_south_1/rds/rds_variabels.tf ---
variable "vpc_id" {
  description = "VPC ID"
  type        = string
}
variable "project_name" {
  description = "Project Name"
  type        = string
}

variable "subnets" {
  description = "생성된 서브넷 목록"
  type        = any
}

variable "db_subnet" {
  description = "DB 서브넷 그룹"
  type        = any
}


--- [FILE]: ./ap_south_1/rds/rds_aurora_mysql.tf_ ---
module "cluster" {
  source = "terraform-aws-modules/rds-aurora/aws"

  name           = "${var.project_name}-aurora-mysql"
  engine         = "aurora-mysql"
  engine_version = "8.0.mysql_aurora.3.05.2"
  instance_class = "db.r7g.large"
  instances = {
    01 = {}
    02 = {}
  }
  autoscaling_enabled      = true
  autoscaling_min_capacity = 2
  autoscaling_max_capacity = 5
  skip_final_snapshot      = true
  database_name            = {put-your-db-name-here}
  master_username          = {put-your-db-username-here}
  master_password          = {put-your-db-password-here}

  vpc_id               = var.vpc_id
  db_subnet_group_name = var.db_subnet.db_subnet.name
  security_group_rules = {
    ex1_ingress = {
      cidr_blocks = ["150.0.40.0/24"]
    }
    ex2_ingress = {
      cidr_blocks = ["150.0.30.0/24"]
    }
  }

  storage_encrypted   = true
  apply_immediately   = true
  monitoring_interval = 10


  tags = {
    Environment = "prd"
    Terraform   = "true"
  }
}


--- [FILE]: ./ap_south_1/subnets/private_subnets.tf ---
resource "aws_subnet" "private_pri_subnet_a" {
  vpc_id            = var.vpc_id
  availability_zone = "${var.region_code}a"
  cidr_block        = "150.0.30.0/24"

  tags = {
    Name                              = "${var.project_name}-pri-subnet-${var.region_code}a"
    type                              = "private"
    "kubernetes.io/role/internal-elb" = "1"
    "karpenter.sh/discovery"          = "${var.project_name}-eks-cluster-01"
  }
}

resource "aws_subnet" "private_pri_subnet_c" {
  vpc_id            = var.vpc_id
  availability_zone = "${var.region_code}c"
  cidr_block        = "150.0.40.0/24"

  tags = {
    Name                              = "${var.project_name}-pri-subnet-${var.region_code}c"
    type                              = "private"
    "kubernetes.io/role/internal-elb" = "1"
    "karpenter.sh/discovery"          = "${var.project_name}-eks-cluster-01"
  }
}


--- [FILE]: ./ap_south_1/subnets/subnets_variables.tf ---
variable "project_name" {
  description = "Project Name"
  type        = string
}

variable "vpc_id" {
  description = "라우팅 테이블에 연결할 VPC ID"
  type        = string
}

variable "region_code" {
  description = "Region code"
  type        = string
}

variable "subnets" {
  description = "subnet_list"
  type        = any
}

variable "eks_output"{
  description = "eks_list"
  type        = any
}


--- [FILE]: ./ap_south_1/subnets/subnets_outputs.tf ---
output "subnet_list" {
  value = {
    public_pub_subnets    = [aws_subnet.public_subnet_a, aws_subnet.public_subnet_c]
    private_pri_subnets   = [aws_subnet.private_pri_subnet_a, aws_subnet.private_pri_subnet_c]
    private_pri_a_subnets = aws_subnet.private_pri_subnet_a
    private_pri_c_subnets = aws_subnet.private_pri_subnet_c
    private_nat_subnets   = [aws_subnet.public_subnet_a]
    db_subnets            = [aws_subnet.db_subnet_a, aws_subnet.db_subnet_c]
    eks_subnets           = [aws_subnet.public_subnet_a, aws_subnet.public_subnet_c, aws_subnet.private_pri_subnet_a, aws_subnet.private_pri_subnet_c]
  }
}

output "db_subnet" {
  value = {
    db_subnet    = aws_db_subnet_group.db_subnet
    redis_subnet = aws_elasticache_subnet_group.cache_subnet
  }
}


--- [FILE]: ./ap_south_1/subnets/public_subnets.tf ---
resource "aws_subnet" "public_subnet_a" {
  vpc_id                  = var.vpc_id
  availability_zone       = "${var.region_code}a"
  cidr_block              = "150.0.10.0/24"
  map_public_ip_on_launch = true
  # depends_on              = [ module.igw ]

  tags = {
    Name                     = "${var.project_name}-pub-subnet-${var.region_code}a"
    nat                      = "true"
    type                     = "public"
    "kubernetes.io/role/elb" = "1"
  }
}

resource "aws_subnet" "public_subnet_c" {
  vpc_id                  = var.vpc_id
  availability_zone       = "${var.region_code}c"
  cidr_block              = "150.0.20.0/24"
  map_public_ip_on_launch = true
  # depends_on              = [ module.igw ]

  tags = {
    Name                     = "${var.project_name}-pub-subnet-${var.region_code}c"
    nat                      = "false"
    type                     = "public"
    "kubernetes.io/role/elb" = "1"
  }
}


--- [FILE]: ./ap_south_1/subnets/db_subnets.tf ---
resource "aws_subnet" "db_subnet_a" {
  vpc_id                  = var.vpc_id
  availability_zone       = "${var.region_code}a"
  cidr_block              = "150.0.50.0/24"
  map_public_ip_on_launch = true
  //depends_on              = [ module.igw ]

  tags = {
    Name = "${var.project_name}-db-subnet-${var.region_code}a"
    nat  = "true"
    type = "private"
  }
}

resource "aws_subnet" "db_subnet_c" {
  vpc_id                  = var.vpc_id
  availability_zone       = "${var.region_code}c"
  cidr_block              = "150.0.60.0/24"
  map_public_ip_on_launch = true
  //depends_on              = [ module.igw ]

  tags = {
    Name = "${var.project_name}-db-subnet-${var.region_code}c"
    nat  = "false"
    type = "private"
  }
}

resource "aws_db_subnet_group" "db_subnet" {
  name       = "${var.project_name}-db-subnet"
  subnet_ids = [for subnet in var.subnets.db_subnets : subnet.id]
}


--- [FILE]: ./ap_south_1/subnets/redis_subnets.tf ---
resource "aws_elasticache_subnet_group" "cache_subnet" {
  name       = "${var.project_name}-cache-subnet"
  subnet_ids = [for subnet in var.subnets.db_subnets : subnet.id]
}


--- [FILE]: ./ap_south_1/eip/eip_variables.tf ---
variable "vpc_id" {
  description = "인터넷 게이트웨이에 연결할 VPC ID"
  type        = string
}

variable "nat_subnets" {
  description = "NAT가 적용될 서브넷 목록"
  type        = any
}
variable "project_name" {
  description = "Project Name"
  type        = string
}


--- [FILE]: ./ap_south_1/eip/eip_main.tf ---
resource "aws_eip" "nat_eips" {
  count  = length(var.nat_subnets)
  domain = "vpc"

  tags = {
    Name = "${var.project_name}-nat-${substr(element(var.nat_subnets.*.availability_zone, count.index), 14, 1)}-eip"
    nat  = "true"
  }
}


--- [FILE]: ./ap_south_1/eip/eip_outputs.tf ---
output "nat_eip" {
  value = aws_eip.nat_eips
}


--- [FILE]: ./ap_south_1/efs/efs_main.tf ---
module "efs" {
  source = "terraform-aws-modules/efs/aws"

  name      = "${var.project_name}-efs-01"
  encrypted = true

  performance_mode = "generalPurpose"
  throughput_mode  = "bursting"

  lifecycle_policy = {
    transition_to_ia = "AFTER_30_DAYS"
  }

  attach_policy                      = true
  bypass_policy_lockout_safety_check = false

  mount_targets = {
    "${var.region_code}a" = {
      subnet_id = var.subnets.private_pri_a_subnets.id
    }
    "${var.region_code}c" = {
      subnet_id = var.subnets.private_pri_c_subnets.id
    }
  }
  security_group_description = "Youngjin EFS SG"
  security_group_vpc_id      = var.vpc_id
  security_group_rules = {
    vpc = {
      description = "EFS Inbound SG Rule"
      cidr_blocks = ["150.0.30.0/24", "150.0.40.0/24"]
    }
  }

  access_points = {
    root_example = {
      root_directory = {
        path = "/example"
        creation_info = {
          owner_gid   = 1001
          owner_uid   = 1001
          permissions = "755"
        }
      }
    }
  }

  enable_backup_policy = true

  tags = {
    Terraform = "true"
  }
}


--- [FILE]: ./ap_south_1/efs/efs_variables.tf ---
variable "vpc_id" {
  description = "VPC ID"
  type        = string
}
variable "project_name" {
  description = "Project Name"
  type        = string
}

variable "subnets" {
  description = "생성된 서브넷 목록"
  type        = any
}

variable "region_code" {
  description = "Region Code"
  type        = string
}

variable "security_groups" {
  type = any
}  


--- [FILE]: ./ap_south_1/ec2/ec2_bastion.tf ---
# 1. Amazon Linux 2023 AMI를 조회하는 Data Source
data "aws_ami" "amazon_linux_2023" {
  most_recent = true
  owners      = ["amazon"] # AWS 공식 AMI
  filter {
    name   = "name"
    values = ["al2023-ami-2023.*"] # Amazon Linux 2023 이미지 이름 패턴
  }
  filter {
    name   = "state"
    values = ["available"]
  }
  filter {
    name   = "architecture"
    values = ["x86_64"]
  }
}

# 2. EC2 인스턴스 생성 모듈
module "ec2_instance" {
  source = "terraform-aws-modules/ec2-instance/aws"

  name = "${var.project_name}-ec2-bastion"

  instance_type          = "t3a.nano"
  monitoring             = true
  vpc_security_group_ids = [var.security_groups.bastion_sg.id]
  subnet_id              = var.subnets.private_pri_a_subnets.id
  iam_instance_profile   = var.iam_ssm_profile.name

  # 여기서 ami 파라미터에 Data Source의 id를 전달
  ami = data.aws_ami.amazon_linux_2023.id

  # IMDSv2만 사용하도록 metadata_options 설정 (IMDSv1 비활성화)
  metadata_options = {
    http_tokens                 = "required" # 토큰 없이 접근 불가 → IMDSv2 강제
    http_put_response_hop_limit = 2          # 응답 hop 제한 (필요에 따라 조정)
    http_endpoint               = "enabled"  # 메타데이터 엔드포인트 활성화
  }

  # 사용자 데이터 (user_data) 스크립트: kubectl 설치 및 kubeconfig 업데이트  
  user_data = <<-EOF
  #!/bin/bash
  # set -e 제거, 대신 -u와 -o pipefail만 사용
  set -uo pipefail

  LOG_FILE="/root/user_data.log"
  success_list=()
  fail_list=()

  # 명령어를 실행하면서 로그를 남기고 성공/실패를 배열에 저장
  run_command() {
    local cmd="$*"
    echo "---------------------------------------------------" | tee -a $LOG_FILE
    echo "[INFO] Running: $cmd" | tee -a $LOG_FILE
    eval "$cmd" >> $LOG_FILE 2>&1
    if [ $? -eq 0 ]; then
      echo "[SUCCESS] $cmd" | tee -a $LOG_FILE
      success_list+=("$cmd")
    else
      echo "[FAILURE] $cmd" | tee -a $LOG_FILE
      fail_list+=("$cmd")
    fi
  }

  # 예시: yum 업데이트
  run_command "yum update -y"
  run_command "yum install -y awscli vim git"

  # kubectl 설치
  run_command "curl -O https://s3.us-west-2.amazonaws.com/amazon-eks/1.31.3/2024-12-12/bin/linux/amd64/kubectl"
  run_command "chmod +x ./kubectl"
  run_command "mkdir -p /root/bin && cp ./kubectl /root/bin/kubectl && export PATH=/root/bin:\$PATH"
  run_command "echo 'export PATH=/root/bin:\$PATH' >> /root/.bashrc"

  # kubeconfig 업데이트
  run_command "aws eks update-kubeconfig --region ${var.region_code} --name ${var.eks_cluster.cluster_name}"

  # k9s 설치
  run_command "curl -LO https://github.com/derailed/k9s/releases/download/v0.40.7/k9s_Linux_amd64.tar.gz"
  run_command "tar -xvf k9s_Linux_amd64.tar.gz"
  run_command "mkdir -p /root/.local/bin && mv ./k9s /root/.local/bin && chmod +x /root/.local/bin/k9s"
  run_command "rm -f k9s_Linux_amd64.tar.gz"
  run_command "echo 'export PATH=\$PATH:/root/.local/bin' >> /root/.bashrc"

  # helm 설치
  run_command "curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3"
  run_command "chmod 700 get_helm.sh"
  run_command "./get_helm.sh"

  # eksctl 설치
  run_command "ARCH=amd64 && PLATFORM=\$(uname -s)_\$ARCH"
  run_command "curl -sLO https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_\$PLATFORM.tar.gz"
  run_command "curl -sL https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_checksums.txt | grep \$PLATFORM | sha256sum --check"
  run_command "tar -xzf eksctl_\$PLATFORM.tar.gz -C /tmp && rm -f eksctl_\$PLATFORM.tar.gz"
  run_command "mv /tmp/eksctl /usr/local/bin"

  # git clone
  run_command "mkdir /root/git && cd /root/git && git clone https://github.com/PYJ-zero/youngjin_oss.git"

  # velero 설치
  run_command "curl -L https://github.com/vmware-tanzu/velero/releases/download/v1.15.2/velero-v1.15.2-linux-amd64.tar.gz -o /tmp/velero.tar.gz"
  run_command "tar -xzvf /tmp/velero.tar.gz -C /tmp && rm -f /tmp/velero.tar.gz"
  run_command "mv /tmp/velero-v1.15.2-linux-amd64/velero /usr/local/bin/"
  run_command "chmod +x /usr/local/bin/velero"

  # credentials-velero 파일 생성
  run_command "cat <<CRED > /root/credentials-velero
  [default]
  aws_access_key_id=${var.iam_users.velero_user.id}
  aws_secret_access_key=${var.iam_users.velero_user.secret}
  CRED
  "

  # velero instal
  run_command "velero install \
    --provider aws \
    --plugins velero/velero-plugin-for-aws:v1.11.1 \
    --bucket ${var.s3_list.velero_bucket.id} \
    --backup-location-config region=${var.region_code} \
    --secret-file /root/credentials-velero \
    --snapshot-location-config region=${var.region_code}"

  # 스크립트 마지막에 요약 로그 출력
  echo "---------------------------------------------------" | tee -a $LOG_FILE
  echo "[INFO] Summary of commands" | tee -a $LOG_FILE

  if [ $${#success_list[@]} -gt 0 ]; then
    echo "[INFO] Succeeded: $${success_list[*]}" | tee -a $LOG_FILE
  fi

  if [ $${#fail_list[@]} -gt 0 ]; then
    echo "[INFO] Failed: $${fail_list[*]}" | tee -a $LOG_FILE
  fi
  source /root/.bashrc
  EOF

  tags = {
    Terraform   = "true"
    Environment = "dev"
  }
}


--- [FILE]: ./ap_south_1/ec2/ec2_variables.tf ---
variable "vpc_id" {
  description = "VPC ID"
  type        = string
}
variable "project_name" {
  description = "Project Name"
  type        = string
}

variable "subnets" {
  description = "생성된 서브넷 목록"
  type        = any
}

variable "region_code" {
  description = "Region Code"
  type        = string
}

variable "security_groups" {
  type = any
}

variable "iam_ssm_profile" {
  type = any
}

variable "eks_cluster" {
  type = any
}

variable "iam_users" {
  type = any
}

variable "s3_list" {
  type = any
  
}


--- [FILE]: ./.terraform.lock.hcl ---
# This file is maintained automatically by "terraform init".
# Manual edits may be lost in future updates.

provider "registry.terraform.io/gavinbunney/kubectl" {
  version     = "1.19.0"
  constraints = ">= 1.7.0"
  hashes = [
    "h1:quymfa/OKEfWI5JXFEwGbUY2aAy0vet3rA9JWJam+3k=",
    "zh:1dec8766336ac5b00b3d8f62e3fff6390f5f60699c9299920fc9861a76f00c71",
    "zh:43f101b56b58d7fead6a511728b4e09f7c41dc2e3963f59cf1c146c4767c6cb7",
    "zh:4c4fbaa44f60e722f25cc05ee11dfaec282893c5c0ffa27bc88c382dbfbaa35c",
    "zh:51dd23238b7b677b8a1abbfcc7deec53ffa5ec79e58e3b54d6be334d3d01bc0e",
    "zh:5afc2ebc75b9d708730dbabdc8f94dd559d7f2fc5a31c5101358bd8d016916ba",
    "zh:6be6e72d4663776390a82a37e34f7359f726d0120df622f4a2b46619338a168e",
    "zh:72642d5fcf1e3febb6e5d4ae7b592bb9ff3cb220af041dbda893588e4bf30c0c",
    "zh:9b12af85486a96aedd8d7984b0ff811a4b42e3d88dad1a3fb4c0b580d04fa425",
    "zh:a1da03e3239867b35812ee031a1060fed6e8d8e458e2eaca48b5dd51b35f56f7",
    "zh:b98b6a6728fe277fcd133bdfa7237bd733eae233f09653523f14460f608f8ba2",
    "zh:bb8b071d0437f4767695c6158a3cb70df9f52e377c67019971d888b99147511f",
    "zh:dc89ce4b63bfef708ec29c17e85ad0232a1794336dc54dd88c3ba0b77e764f71",
    "zh:dd7dd18f1f8218c6cd19592288fde32dccc743cde05b9feeb2883f37c2ff4b4e",
    "zh:ec4bd5ab3872dedb39fe528319b4bba609306e12ee90971495f109e142d66310",
    "zh:f610ead42f724c82f5463e0e71fa735a11ffb6101880665d93f48b4a67b9ad82",
  ]
}

provider "registry.terraform.io/hashicorp/aws" {
  version     = "5.89.0"
  constraints = ">= 4.33.0, >= 4.66.0, >= 5.83.0, 5.89.0"
  hashes = [
    "h1:rFvk42jJEKiSUhK1cbERfNgYm4mD+8tq0ZcxCwpXSJs=",
    "zh:0e55784d6effc33b9098ffab7fb77a242e0223a59cdcf964caa0be94d14684af",
    "zh:23c64f3eaeffcafb007c89db3dfca94c8adf06b120af55abddaca55a6c6c924c",
    "zh:338f620133cb607ce980f1725a0a78f61cbd42f4c601808ec1ee01a6c16c9811",
    "zh:6ab0499172f17484d7b39924cf06782789df1473d31ebae0c7f3294f6e7a1227",
    "zh:6dcde3e29e538cdf80971cbdce3b285056fd0e31dd64b02d2dcdf4c02f21d0a9",
    "zh:75c9b594d77c9125bfb1aaf3fbd77a49e392841d53029b5726eb71d64de1233e",
    "zh:7b334c23091e7b4c142e378416586292197c40a31a5bdb3b29c4f9afddd286f0",
    "zh:991bbba72e5eb6eb351f466d68080992f5b0495f862a6723f386d1b4c965aa7d",
    "zh:9b12af85486a96aedd8d7984b0ff811a4b42e3d88dad1a3fb4c0b580d04fa425",
    "zh:9bd2f12eef4a5dceafc211ab3b9a63f0e3e224007a60c1bbb842f76e0377033d",
    "zh:b1ac1eb3b3e1a79fa5e5ad3364615f23b9ee0b093ceeb809fd386a4d40e7abb4",
    "zh:cea91f43151b30c428c441b97c3b98bf1e5fb72ef72f6971308e3895e23437f4",
    "zh:d3f000a1696a43d8186a516aace7d476d1fd76443627980504133477e19c8ecb",
    "zh:d6f526fbbb3e51b3acc3b9640a158f7acc4a089632fca8ec6db430b450673f25",
    "zh:e0c542950f96c93e761d50602e449fef8447f1389a6d5242a0a7dc9b06826d0b",
  ]
}

provider "registry.terraform.io/hashicorp/cloudinit" {
  version     = "2.3.6"
  constraints = ">= 2.0.0, 2.3.6"
  hashes = [
    "h1:afnqn3XPnO40laFt+SVHPPKsg1j3HXT0VAO0xBVvmrY=",
    "zh:1321b5ddede56be3f9b35bf75d7cda79adcb357fad62eb8677b6595e0baaa6cd",
    "zh:265d66e61b9cd16ca1182ebf094cc0a08fb3687e8193a1dbac6899b16c237151",
    "zh:3875c3a20e082ac55d5ff24bcaf7133ebc90c7f999fd0fb37cf0f0003474c94c",
    "zh:68ce41ccd07757c451682703840cae1ec270ed5275cd491bbf8279782dfcbb73",
    "zh:78d5eefdd9e494defcb3c68d282b8f96630502cac21d1ea161f53cfe9bb483b3",
    "zh:8dca3bb3f85ff8ac4d1b3f93975dcb751ed788396c56ebf0c3737ce1a4c60492",
    "zh:9339bdaa99939291cedf543861353c8e7171ec5231c0dfacaa9bdb3338978dab",
    "zh:a8510c2256e9a78697910bb5542aeca457c81225ea88130335f6d14a36a36c74",
    "zh:af7ed71b8fceb60a5e3b7fa663be171e0bd41bb0af30e0e1f06a004c7b584e4a",
    "zh:bc9de0f921b69d07f5fc1ea65f8af71d8d1a7053aafb500788b30bfce64b8fbe",
    "zh:bccd0a49f161a91660d7d30dd6b389e6820f29752ccf351f10a3297c96973823",
    "zh:c69321caca20009abead617f888a67aca990276cb7388b738b19157b88749190",
  ]
}

provider "registry.terraform.io/hashicorp/helm" {
  version = "2.17.0"
  hashes = [
    "h1:kQMkcPVvHOguOqnxoEU2sm1ND9vCHiT8TvZ2x6v/Rsw=",
    "zh:06fb4e9932f0afc1904d2279e6e99353c2ddac0d765305ce90519af410706bd4",
    "zh:104eccfc781fc868da3c7fec4385ad14ed183eb985c96331a1a937ac79c2d1a7",
    "zh:129345c82359837bb3f0070ce4891ec232697052f7d5ccf61d43d818912cf5f3",
    "zh:3956187ec239f4045975b35e8c30741f701aa494c386aaa04ebabffe7749f81c",
    "zh:66a9686d92a6b3ec43de3ca3fde60ef3d89fb76259ed3313ca4eb9bb8c13b7dd",
    "zh:88644260090aa621e7e8083585c468c8dd5e09a3c01a432fb05da5c4623af940",
    "zh:a248f650d174a883b32c5b94f9e725f4057e623b00f171936dcdcc840fad0b3e",
    "zh:aa498c1f1ab93be5c8fbf6d48af51dc6ef0f10b2ea88d67bcb9f02d1d80d3930",
    "zh:bf01e0f2ec2468c53596e027d376532a2d30feb72b0b5b810334d043109ae32f",
    "zh:c46fa84cc8388e5ca87eb575a534ebcf68819c5a5724142998b487cb11246654",
    "zh:d0c0f15ffc115c0965cbfe5c81f18c2e114113e7a1e6829f6bfd879ce5744fbb",
    "zh:f569b65999264a9416862bca5cd2a6177d94ccb0424f3a4ef424428912b9cb3c",
  ]
}

provider "registry.terraform.io/hashicorp/null" {
  version     = "3.2.3"
  constraints = ">= 3.0.0, 3.2.3"
  hashes = [
    "h1:I0Um8UkrMUb81Fxq/dxbr3HLP2cecTH2WMJiwKSrwQY=",
    "zh:22d062e5278d872fe7aed834f5577ba0a5afe34a3bdac2b81f828d8d3e6706d2",
    "zh:23dead00493ad863729495dc212fd6c29b8293e707b055ce5ba21ee453ce552d",
    "zh:28299accf21763ca1ca144d8f660688d7c2ad0b105b7202554ca60b02a3856d3",
    "zh:55c9e8a9ac25a7652df8c51a8a9a422bd67d784061b1de2dc9fe6c3cb4e77f2f",
    "zh:756586535d11698a216291c06b9ed8a5cc6a4ec43eee1ee09ecd5c6a9e297ac1",
    "zh:78d5eefdd9e494defcb3c68d282b8f96630502cac21d1ea161f53cfe9bb483b3",
    "zh:9d5eea62fdb587eeb96a8c4d782459f4e6b73baeece4d04b4a40e44faaee9301",
    "zh:a6355f596a3fb8fc85c2fb054ab14e722991533f87f928e7169a486462c74670",
    "zh:b5a65a789cff4ada58a5baffc76cb9767dc26ec6b45c00d2ec8b1b027f6db4ed",
    "zh:db5ab669cf11d0e9f81dc380a6fdfcac437aea3d69109c7aef1a5426639d2d65",
    "zh:de655d251c470197bcbb5ac45d289595295acb8f829f6c781d4a75c8c8b7c7dd",
    "zh:f5c68199f2e6076bce92a12230434782bf768103a427e9bb9abee99b116af7b5",
  ]
}

provider "registry.terraform.io/hashicorp/time" {
  version     = "0.13.0"
  constraints = ">= 0.9.0, 0.13.0"
  hashes = [
    "h1:iwR4JouIoeVPDabb8XCqsiaZlZ28IcB3tDD9MuPeSXE=",
    "zh:3776dd78ef3053562ccb2f8916d5d3f21a28f05e78859f0f1e4510525f891ecb",
    "zh:541ca0b56f808c15d208b9396f149563b133223c4b66cdefbcfe2d8f1c23497e",
    "zh:67ed315f3572eb20ce6778423b14fbb6faba3090f454bc20ec4146489b4738c0",
    "zh:69dc375845bcfc451426480119f2941ee28b9ef01273d228bb66918180863b3a",
    "zh:78d5eefdd9e494defcb3c68d282b8f96630502cac21d1ea161f53cfe9bb483b3",
    "zh:93c24b7c87b5db9721f60782ac784152599aa78b30fdea2fc9c594d46d92767c",
    "zh:95441cf14312041ae0b34640ff33975c09540125b01f9131358fca50e7be239d",
    "zh:a294103aeed868c58987e131357a3ec259316c937c909e8a726b862d5a227b82",
    "zh:adf6ded3f2e2f318e8aebf1040bc2791b448d006af7d12f7ddc3e8d40b22047a",
    "zh:b2d9c16b7acd20d3813060c4d3647dc5f40598ebbdf59f642d53d189e4e3870a",
    "zh:bc76a5161e9bcf74cadd76b3d4a51de508aa0c62e7f7ae536a87cd7595d81ebf",
    "zh:ce6df2c1052c60b4432cb5c0ead471d7cdb4b285b807c265328a358631fc3610",
  ]
}

provider "registry.terraform.io/hashicorp/tls" {
  version     = "4.0.6"
  constraints = ">= 3.0.0, 4.0.6"
  hashes = [
    "h1:n3M50qfWfRSpQV9Pwcvuse03pEizqrmYEryxKky4so4=",
    "zh:10de0d8af02f2e578101688fd334da3849f56ea91b0d9bd5b1f7a243417fdda8",
    "zh:37fc01f8b2bc9d5b055dc3e78bfd1beb7c42cfb776a4c81106e19c8911366297",
    "zh:4578ca03d1dd0b7f572d96bd03f744be24c726bfd282173d54b100fd221608bb",
    "zh:6c475491d1250050765a91a493ef330adc24689e8837a0f07da5a0e1269e11c1",
    "zh:81bde94d53cdababa5b376bbc6947668be4c45ab655de7aa2e8e4736dfd52509",
    "zh:abdce260840b7b050c4e401d4f75c7a199fafe58a8b213947a258f75ac18b3e8",
    "zh:b754cebfc5184873840f16a642a7c9ef78c34dc246a8ae29e056c79939963c7a",
    "zh:c928b66086078f9917aef0eec15982f2e337914c5c4dbc31dd4741403db7eb18",
    "zh:cded27bee5f24de6f2ee0cfd1df46a7f88e84aaffc2ecbf3ff7094160f193d50",
    "zh:d65eb3867e8f69aaf1b8bb53bd637c99c6b649ba3db16ded50fa9a01076d1a27",
    "zh:ecb0c8b528c7a619fa71852bb3fb5c151d47576c5aab2bf3af4db52588722eeb",
    "zh:f569b65999264a9416862bca5cd2a6177d94ccb0424f3a4ef424428912b9cb3c",
  ]
}


--- [FILE]: ./ap_south_1/ap_south_1_variables.tf ---
variable "project_name" {
  description = "Project Name"
  type        = string
}

variable "region_code" {
  description = "Region code"
  type        = string
}


--- [FILE]: ./ap_south_1/ap_south_1_main.tf ---
### 기초가 되는 AWS 리소스 제외하고 Terraform에서 제공하는 module 활용###
#########################################################################

resource "aws_vpc" "vpc" {
  cidr_block           = "150.0.0.0/16"
  enable_dns_hostnames = true
  enable_dns_support   = true
  tags = {
    Name = "${var.project_name}-EKS-CICD-TEST-VPC"
  }
}

module "subnets" {
  source       = "./subnets"
  project_name = var.project_name
  vpc_id       = aws_vpc.vpc.id
  region_code  = var.region_code
  subnets      = module.subnets.subnet_list
  eks_output   = module.eks.eks
}

module "security_group" {
  source       = "./security_groups"
  project_name = var.project_name
  vpc_id       = aws_vpc.vpc.id
}

module "igw" {
  source       = "./igw"
  depends_on   = [module.subnets]
  vpc_id       = aws_vpc.vpc.id
  project_name = var.project_name
}

module "eip" {
  source       = "./eip"
  project_name = var.project_name
  depends_on   = [module.subnets]
  vpc_id       = aws_vpc.vpc.id
  nat_subnets  = module.subnets.subnet_list.private_nat_subnets
}

module "nat" {
  source       = "./nat"
  project_name = var.project_name
  depends_on   = [module.eip]
  nat_subnets  = module.subnets.subnet_list.private_nat_subnets
  nat_eip      = module.eip.nat_eip
  vpc_id       = aws_vpc.vpc.id
}

module "route_table" {
  source       = "./route_table"
  project_name = var.project_name
  depends_on   = [module.subnets]
  vpc_id       = aws_vpc.vpc.id
  igw_id       = module.igw.igw_id
  nat          = module.nat.nat
  subnets      = module.subnets.subnet_list
}

module "s3"{
  source = "./s3"
  project_name  = var.project_name
  s3_list       = module.s3.s3_list
}

module "eks" {
  source          = "./eks"
  project_name    = var.project_name
  subnets         = module.subnets.subnet_list
  vpc_id          = aws_vpc.vpc.id
  iam_roles       = module.iam.iam_roles
  security_groups = module.security_group.security_groups

}

# module "rds" {
#   source         = "./rds"
#   project_name   = var.project_name
#   subnets        = module.subnets.subnet_list
#   vpc_id         = aws_vpc.vpc.id
#   db_subnet      = module.subnets.db_subnet
# }

# module "efs" {
#   source         = "./efs"
#   project_name   = var.project_name
#   subnets        = module.subnets.subnet_list
#   region_code    = var.region_code
#   vpc_id         = aws_vpc.vpc.id
#   security_groups = module.security_group.security_groups
# } 

# module "redis" {
#   source         = "./redis"
#   project_name   = var.project_name
#   db_subnet      = module.subnets.subnet_list
#   vpc_id         = aws_vpc.vpc.id
#   region_code    = var.region_code
#   security_groups   = module.security_group.security_groups
# }

# module "endpoint" {
#   source         = "./endpoint"
#   project_name   = var.project_name
#   vpc_id         = aws_vpc.vpc.id
#   route_table    = module.route_table.private_rtb
# }

module "ec2" {
  source          = "./ec2"
  project_name    = var.project_name
  subnets         = module.subnets.subnet_list
  region_code     = var.region_code
  vpc_id          = aws_vpc.vpc.id
  eks_cluster     = module.eks.eks
  security_groups = module.security_group.security_groups
  iam_ssm_profile = module.iam.iam_roles.ssm_instance_profile
  iam_users       = module.iam.iam_users
  s3_list         = module.s3.s3_list
}

module "iam" {
  source       = "./iam"
  project_name = var.project_name
  s3_list      = module.s3.s3_list
}

# module "elb" {
#   source         = "./elb"
#   project_name   = var.project_name
#   subnets        = module.subnets.subnet_list
#   vpc_id         = aws_vpc.vpc.id
#   security_groups = module.security_group.security_groups
# }


--- [FILE]: ./ap_south_1/ap_south_1_outputs.tf ---
# VPC
output "vpc_id" {
  description = "The ID of the VPC"
  value       = aws_vpc.vpc.id
}


--- [FILE]: ./ap_south_1/s3/s3_outputs.tf ---
output "s3_list" {
  value = {
    # youngjin_s3 = aws_s3_bucket.youngjin_s3
    velero_bucket = aws_s3_bucket.velero_bucket
  }
}


--- [FILE]: ./ap_south_1/s3/s3_bucket_velero.tf ---
resource "aws_s3_bucket" "velero_bucket" {
  bucket = "${var.project_name}-velero-s3"
}


--- [FILE]: ./ap_south_1/s3/s3_bucket_main.tf_ ---
resource "aws_s3_bucket" "youngjin_s3" {
  bucket = "${var.project_name}-s3"
}


--- [FILE]: ./ap_south_1/s3/s3_variables.tf ---
variable "project_name" {
  description = "Project Name"
  type        = string
}

variable "s3_list" {
  description = "s3 list"
  type        = any
}


--- [FILE]: ./ap_south_1/igw/igw_outputs.tf ---
output "igw_id" {
  value = aws_internet_gateway.igw.id
}


--- [FILE]: ./ap_south_1/igw/igw_variables.tf ---
variable "vpc_id" {
  description = "인터넷 게이트웨이에 연결할 VPC ID"
  type        = string
}

variable "project_name" {
  description = "Project Name"
  type        = string
}


--- [FILE]: ./ap_south_1/igw/igw_main.tf ---
//  인터넷 게이트웨이 생성
resource "aws_internet_gateway" "igw" {
  vpc_id = var.vpc_id
  tags = {
    Name = "${var.project_name}-igw"
  }
}


--- [FILE]: ./ap_south_1/redis/redis_main.tf ---
module "this" {
  source    = "cloudposse/label/null"
  namespace = "prd"
  stage     = "youngjin"
  name      = "redis-cluster-01"
}

module "redis" {
  source = "cloudposse/elasticache-redis/aws"

  availability_zones         = ["${var.region_code}a", "${var.region_code}c"]
  vpc_id                     = var.vpc_id
  allowed_security_group_ids = [var.security_groups.redis_sg.id]
  subnets                    = [for subnet in var.db_subnet.private_pri_subnets : subnet.id]
  cluster_size               = "2"
  instance_type              = "cache.m6g.large"
  apply_immediately          = true
  automatic_failover_enabled = true
  engine_version             = "6.2"
  family                     = "redis6.x"

  context = module.this.context
}


--- [FILE]: ./ap_south_1/redis/redis_variables.tf ---
variable "vpc_id" {
  description = "VPC ID"
  type        = string
}

variable "project_name" {
  description = "Project Name"
  type        = string
}

variable "db_subnet" {
  description = "DB 서브넷 그룹"
  type        = any
}

variable "region_code" {
  description = "Region code"
  type        = string
}

variable "security_groups" {
  type = any
}


--- [FILE]: ./ap_south_1/nat/nat_variables.tf ---
variable "nat_subnets" {
  description = "NAT가 적용될 서브넷 목록"
  type        = any
}

variable "nat_eip" {
  description = "NAT가 적용될 eip 목록"
  type        = any
}

variable "vpc_id" {
  description = "VPC ID"
  type        = string
}
variable "project_name" {
  description = "Project Name"
  type        = string
}


--- [FILE]: ./ap_south_1/nat/nat_outputs.tf ---
output "nat" {
  value = aws_nat_gateway.nat
}


--- [FILE]: ./ap_south_1/nat/nat_main.tf ---
resource "aws_nat_gateway" "nat" {
  count = length(var.nat_eip)

  allocation_id = element(var.nat_eip.*.id, count.index)
  subnet_id     = element(var.nat_subnets.*.id, count.index)

  tags = {
    Name = "${var.project_name}-nat"
  }
}


--- [FILE]: ./ap_south_1/route_table/rt_db_route.tf ---
resource "aws_route_table" "db_rtb" {
  vpc_id = var.vpc_id
  tags = {
    Name = "${var.project_name}-rtb-db"
  }
}

resource "aws_route_table_association" "db_subnet" {
  count          = length(var.subnets.db_subnets)
  subnet_id      = element(concat(var.subnets.db_subnets.*.id), count.index)
  route_table_id = aws_route_table.db_rtb.id
}


--- [FILE]: ./ap_south_1/route_table/rt_public_route.tf ---
resource "aws_route_table" "public_rtb" {
  vpc_id = var.vpc_id
  tags = {
    Name = "${var.project_name}-rtb-pub"
  }
}

resource "aws_route" "public_rtb_igw_route" {
  route_table_id         = aws_route_table.public_rtb.id
  destination_cidr_block = "0.0.0.0/0"
  gateway_id             = var.igw_id
}

resource "aws_route_table_association" "public_subnet" {
  count          = length(var.subnets.public_pub_subnets)
  subnet_id      = element(var.subnets.public_pub_subnets.*.id, count.index)
  route_table_id = aws_route_table.public_rtb.id
}


--- [FILE]: ./ap_south_1/route_table/rt_private_route.tf ---
resource "aws_route_table" "private_rtb" {
  vpc_id = var.vpc_id
  tags = {
    Name = "${var.project_name}-rtb-pri"
  }
}

resource "aws_route" "private_rtb_nat_route" {
  route_table_id         = aws_route_table.private_rtb.id
  destination_cidr_block = "0.0.0.0/0"
  nat_gateway_id         = element(var.nat.*.id, 0)
}

resource "aws_route_table_association" "private_subnet" {
  count          = length(var.subnets.private_pri_subnets)
  subnet_id      = element(concat(var.subnets.private_pri_subnets.*.id), count.index)
  route_table_id = aws_route_table.private_rtb.id
}


--- [FILE]: ./ap_south_1/route_table/rt_variables.tf ---
variable "vpc_id" {
  description = "라우팅 테이블에 연결할 VPC ID"
  type        = string
}

variable "igw_id" {
  description = "생성된 Internet Gateway ID"
  type        = any
}

variable "nat" {
  description = "생성된 NAT 게이트웨이 목록"
  type        = list(any)
}

variable "subnets" {
  description = "생성된 서브넷 목록"
  type        = any
}

variable "project_name" {
  description = "Project Name"
  type        = string
}


--- [FILE]: ./ap_south_1/route_table/rt_outputs.tf ---
output "rtb_list" {
  value = {
    public_rtb      = [aws_route_table.public_rtb]
    private_pri_rtb = [aws_route_table.private_rtb]
    db_rtb          = [aws_route_table.db_rtb]
  }
}

output "public_rtb" {
  value = aws_route_table.public_rtb
}

output "private_rtb" {
  value = aws_route_table.private_rtb
}

output "db_rtb" {
  value = aws_route_table.db_rtb
}


--- [FILE]: ./ap_south_1/elb/elb_variables.tf ---
variable "vpc_id" {
  description = "VPC ID"
  type        = string
}
variable "project_name" {
  description = "Project Name"
  type        = string
}

variable "subnets" {
  description = "생성된 서브넷 목록"
  type        = any
}

variable "security_groups" {
  type = any
}


--- [FILE]: ./ap_south_1/elb/elb_main.tf ---
# ALB 생성
module "alb" {
  source  = "terraform-aws-modules/alb/aws"
  version = "~> 8.0" # 사용 중인 모듈 버전에 맞게 수정

  name               = "${var.project_name}-alb"
  load_balancer_type = "application"
  internal           = false
  subnets            = [for subnet in var.subnets.public_pub_subnets.id : subnet.id] # 퍼블릭 서브넷 ID 목록
  security_groups    = [var.security_groups.alb_sg.id]                               # ALB에 적용할 보안 그룹 ID

}

# # ALB 리스너 생성 (HTTP:80)
# module "alb_listener" {
#   source  = "terraform-aws-modules/alb/aws//modules/listener"
#   version = "~> 8.0"

#   load_balancer_arn = module.alb.this_lb_arn
#   port              = 80
#   protocol          = "HTTP"

#   default_action = {
#     type = "fixed-response"
#     fixed_response = {
#       content_type = "text/plain"
#       message_body = "OK"
#       status_code  = "200"
#     }
#   }
# }

# # (옵션) ALB 타겟 그룹 생성 예시
# module "alb_target_group" {
#   source  = "terraform-aws-modules/alb/aws//modules/target-group"
#   version = "~> 8.0"

#   name     = "${var.project_name}-tg"
#   port     = 80
#   protocol = "HTTP"
#   vpc_id   = var.vpc_id

#   health_check = {
#     healthy_threshold   = 3
#     unhealthy_threshold = 3
#     timeout             = 5
#     interval            = 30
#     path                = "/"
#     matcher             = "200-299"
#   }

#   tags = {
#     Environment = var.environment
#     Project     = var.project_name
#   }
# }

# # (옵션) ALB 리스너 규칙을 통해 타겟 그룹으로 트래픽 전달
# module "alb_listener_rule" {
#   source  = "terraform-aws-modules/alb/aws//modules/listener-rule"
#   version = "~> 8.0"

#   listener_arn = module.alb_listener.this_listener_arn
#   priority     = 100

#   actions = [
#     {
#       type             = "forward"
#       target_group_arn = module.alb_target_group.this_target_group_arn
#     }
#   ]

#   conditions = [
#     {
#       field  = "path-pattern"
#       values = ["/app/*"]
#     }
#   ]
# }


--- [FILE]: ./ap_south_1/security_groups/locals.tf ---
locals {
  bastion_sg_name    = "${var.project_name}-bastion-sg"
  bastion_sg_desc    = local.bastion_sg_name
  redis_sg_name      = "${var.project_name}-redis-sg"
  redis_sg_desc      = local.redis_sg_name
  alb_sg_name        = "${var.project_name}-alb-sg"
  alb_sg_desc        = local.redis_sg_name
  description_suffix = "by terraform"
}


--- [FILE]: ./ap_south_1/security_groups/sg_variables.tf ---
variable "project_name" {
  description = "Project Name"
  type        = string
}

variable "vpc_id" {
  description = "The ID of the VPC"
  type        = string
  default     = ""
}


--- [FILE]: ./ap_south_1/security_groups/sg_outputs.tf ---
output "security_groups" {
  value = {
    redis_sg   = aws_security_group.redis_sg
    bastion_sg = aws_security_group.bastion_sg
    alb_sg     = aws_security_group.alb_sg
  }
}


--- [FILE]: ./ap_south_1/security_groups/redis_sg.tf ---
// 보안 그룹 생성
resource "aws_security_group" "redis_sg" {
  vpc_id      = var.vpc_id
  name        = local.redis_sg_name
  description = local.redis_sg_desc

  tags = {
    Name   = local.redis_sg_name
    t_addr = "${path.module}/redis_sg.tf"
  }
  lifecycle {
    ignore_changes = [
      tags["CreateDate"],
    ]
  }
}

### 인바운드
############

resource "aws_security_group_rule" "redis_sg_rule_ingress_0" {
  security_group_id = aws_security_group.redis_sg.id
  type              = "ingress"
  protocol          = "tcp"
  from_port         = 6379
  to_port           = 6379
  cidr_blocks       = ["150.0.0.0/16"]
  description       = "redis sg ${local.description_suffix}"
  lifecycle {
    ignore_changes = [
      # description,
    ]
  }
}

resource "aws_security_group_rule" "redis_sg_rule_egress_0" {
  security_group_id = aws_security_group.redis_sg.id
  type              = "egress"
  protocol          = "tcp"
  from_port         = 3306
  to_port           = 3306
  cidr_blocks       = ["150.0.0.0/16"]
  description       = "redis sg to rds ${local.description_suffix}"
  lifecycle {
    ignore_changes = [
      # description,
    ]
  }
}


--- [FILE]: ./ap_south_1/security_groups/alb_sg.tf ---
// 보안 그룹 생성
resource "aws_security_group" "alb_sg" {
  vpc_id      = var.vpc_id
  name        = local.alb_sg_name
  description = local.alb_sg_desc

  tags = {
    Name   = local.alb_sg_name
    t_addr = "${path.module}/alb.sg.tf"
  }
  lifecycle {
    ignore_changes = [
      tags["CreateDate"],
    ]
  }
}

### 인바운드
############
resource "aws_security_group_rule" "alb_sg_rule_ingress_0" {
  security_group_id = aws_security_group.alb_sg.id
  type              = "ingress"
  protocol          = "tcp"
  from_port         = 443
  to_port           = 443
  cidr_blocks       = ["0.0.0.0/0"]
  description       = "sg for alb ${local.description_suffix}"
  lifecycle {
    ignore_changes = [
      # description,
    ]
  }
}
resource "aws_security_group_rule" "alb_sg_rule_ingress_1" {
  security_group_id = aws_security_group.alb_sg.id
  type              = "ingress"
  protocol          = "tcp"
  from_port         = 80
  to_port           = 80
  cidr_blocks       = ["0.0.0.0/0"]
  description       = "sg for alb ${local.description_suffix}"
  lifecycle {
    ignore_changes = [
      # description,
    ]
  }
}
### 아웃바운드
############
resource "aws_security_group_rule" "alb_sg_rule_egress_0" {
  security_group_id = aws_security_group.alb_sg.id
  type              = "egress"
  protocol          = "tcp"
  from_port         = 443
  to_port           = 443
  cidr_blocks       = ["150.0.0.0/8"]
  description       = "sg for alb ${local.description_suffix}"
  lifecycle {
    ignore_changes = [
      # description,
    ]
  }
}

resource "aws_security_group_rule" "alb_sg_rule_egress_1" {
  security_group_id = aws_security_group.alb_sg.id
  type              = "egress"
  protocol          = "tcp"
  from_port         = 80
  to_port           = 80
  cidr_blocks       = ["150.0.0.0/8"]
  description       = "sg for alb ${local.description_suffix}"
  lifecycle {
    ignore_changes = [
      # description,
    ]
  }
}


--- [FILE]: ./ap_south_1/security_groups/bastion_sg.tf ---
// 보안 그룹 생성
resource "aws_security_group" "bastion_sg" {
  vpc_id      = var.vpc_id
  name        = local.bastion_sg_name
  description = local.bastion_sg_desc

  tags = {
    Name   = local.bastion_sg_name
    t_addr = "${path.module}/bastion.sg.tf"
  }
  lifecycle {
    ignore_changes = [
      tags["CreateDate"],
    ]
  }
}

### 아웃바운드
############
resource "aws_security_group_rule" "bastion_sg_rule_egress_0" {
  security_group_id = aws_security_group.bastion_sg.id
  type              = "egress"
  protocol          = "tcp"
  from_port         = 443
  to_port           = 443
  cidr_blocks       = ["0.0.0.0/0"]
  description       = "sg for bastion ${local.description_suffix}"
  lifecycle {
    ignore_changes = [
      # description,
    ]
  }
}


--- [FILE]: ./ap_south_1/iam/iam_role_eksNodegroup.tf ---
resource "aws_iam_role" "eks_node_role" {
  name               = "${var.project_name}-eks-node-role"
  assume_role_policy = data.aws_iam_policy_document.eks_node_trust.json
}

data "aws_iam_policy_document" "eks_node_trust" {
  statement {
    actions = ["sts:AssumeRole"]
    principals {
      type        = "Service"
      identifiers = ["ec2.amazonaws.com"]
    }
  }
}

# 노드 관련 정책 연결
resource "aws_iam_role_policy_attachment" "eks_workernode_policy" {
  role       = aws_iam_role.eks_node_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy"
}

resource "aws_iam_role_policy_attachment" "eks_cni_policy" {
  role       = aws_iam_role.eks_node_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy"
}

resource "aws_iam_role_policy_attachment" "ecr_read_policy" {
  role       = aws_iam_role.eks_node_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"
}


--- [FILE]: ./ap_south_1/iam/iam_variables.tf ---
variable "project_name" {
  description = "Project Name"
  type        = string
}

variable "s3_list" {
  description = "List of S3 Buckets"
  type        = any
}


--- [FILE]: ./ap_south_1/iam/iam_role_ssm.tf ---
# (1) IAM 역할 생성
resource "aws_iam_role" "ssm_role" {
  name = "${var.project_name}-role-ssm"
  assume_role_policy = jsonencode({
    Version = "2012-10-17",
    Statement = [
      {
        Effect = "Allow",
        Principal = {
          Service = "ec2.amazonaws.com"
        },
        Action = "sts:AssumeRole"
      }
    ]
  })
}

# (2) AmazonSSMManagedInstanceCore 정책을 역할에 연결
resource "aws_iam_role_policy_attachment" "bastion_attach_admin_policy" {
  role       = aws_iam_role.ssm_role.name
  policy_arn = "arn:aws:iam::aws:policy/AdministratorAccess"
}

# (3) 인스턴스 프로파일 생성
resource "aws_iam_instance_profile" "ssm_instance_profile" {
  name = "${var.project_name}-ssm-instance-profile"
  role = aws_iam_role.ssm_role.name
}


--- [FILE]: ./ap_south_1/iam/iam_role_karpenterNode.tf_ ---
#############################
#Karpenter IAM Role 생성
#############################
resource "aws_iam_role" "karpenter_node_role" {
  name = "${var.project_name}-karpenter-node-role"
  assume_role_policy = jsonencode({
    "Version" : "2012-10-17",
    "Statement" : [
      {
        "Effect" : "Allow",
        "Principal" : {
          "Service" : "ec2.amazonaws.com"
        },
        "Action" : "sts:AssumeRole"
      }
    ]
  })
}
#############################
#Role에 Policy Attach
#############################
resource "aws_iam_role_policy_attachment" "karpenter_worker_node_policy" {
  role       = aws_iam_role.karpenter_node_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy"
}

resource "aws_iam_role_policy_attachment" "karpenter_cni_policy" {
  role       = aws_iam_role.karpenter_node_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy"
}

resource "aws_iam_role_policy_attachment" "karpenter_ecr_policy" {
  role       = aws_iam_role.karpenter_node_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"
}

resource "aws_iam_role_policy_attachment" "karpenter_ssm_policy" {
  role       = aws_iam_role.karpenter_node_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore"
}

#############################
#Karpenter Instance Profile 생성
#############################
resource "aws_iam_instance_profile" "karpenter_instance_profile" {
  name = "${var.project_name}-karpenter-instance-profile"
  role = aws_iam_role.karpenter_node_role.name
}


--- [FILE]: ./ap_south_1/iam/iam_outputs.tf ---
output "iam_roles" {
  value = {
    ssm_role             = aws_iam_role.ssm_role
    eks_node_role        = aws_iam_role.eks_node_role
    eks_cluster_role     = aws_iam_role.eks_cluster_role
    ssm_instance_profile = aws_iam_instance_profile.ssm_instance_profile
    # karpenter_node_role = aws_iam_role.karpenter_node_role
  }
}

output "iam_users" {
  value = {
    velero_user = aws_iam_access_key.velero
  }
}


--- [FILE]: ./ap_south_1/iam/iam_role_eksCluster.tf ---
resource "aws_iam_role" "eks_cluster_role" {
  name               = "${var.project_name}-cluster-role"
  assume_role_policy = data.aws_iam_policy_document.eks_cluster_trust.json
}

data "aws_iam_policy_document" "eks_cluster_trust" {
  statement {
    actions = ["sts:AssumeRole"]
    principals {
      type        = "Service"
      identifiers = ["eks.amazonaws.com"]
    }
  }
}

# EKS 기본 정책 연결
resource "aws_iam_role_policy_attachment" "eks_cluster_policy_attach" {
  role       = aws_iam_role.eks_cluster_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSClusterPolicy"
}

resource "aws_iam_role_policy_attachment" "eks_service_policy_attach" {
  role       = aws_iam_role.eks_cluster_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSServicePolicy"
}

resource "aws_iam_role_policy_attachment" "eks_vpc_policy_attach" {
  role       = aws_iam_role.eks_cluster_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSVPCResourceController"
}


--- [FILE]: ./ap_south_1/iam/iam_user_velero.tf ---
resource "aws_iam_user" "velero" {
  name = "${var.project_name}-velero"
}

resource "aws_iam_policy" "velero_policy" {
  name        = "${var.project_name}_VeleroPolicy"
  description = "Policy for Velero to backup and restore EKS resources."
  policy      = jsonencode({
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "ec2:DescribeVolumes",
                "ec2:DescribeSnapshots",
                "ec2:CreateTags",
                "ec2:CreateVolume",
                "ec2:CreateSnapshot",
                "ec2:DeleteSnapshot"
            ],
            "Resource": "*"
        },
        {
            "Effect": "Allow",
            "Action": [
                "s3:GetObject",
                "s3:DeleteObject",
                "s3:PutObject",
                "s3:PutObjectTagging",
                "s3:AbortMultipartUpload",
                "s3:ListMultipartUploadParts"
            ],
            "Resource": [
                "${var.s3_list.velero_bucket.arn}/*"
            ]
        },
        {
            "Effect": "Allow",
            "Action": [
                "s3:ListBucket"
            ],
            "Resource": [
                "${var.s3_list.velero_bucket.arn}"
            ]
        }
    ]
})
}

resource "aws_iam_user_policy_attachment" "velero_policy_attach" {
  user       = aws_iam_user.velero.name
  policy_arn = aws_iam_policy.velero_policy.arn
}

resource "aws_iam_access_key" "velero" {
  user = aws_iam_user.velero.name
}

--- [FILE]: ./ap_south_1/eks/test.yaml ---
  apiVersion: karpenter.sh/v1
  kind: NodePool
  metadata:
    namespace : karpenter
    name: default
  spec:
    template:
      spec:
        requirements:
          - key: "node.kubernetes.io/instance-type"
            operator: In
            values: ["t3a.large"]
        nodeClassRef:
          group: karpenter.k8s.aws
          kind: EC2NodeClass
          name: default
  ---
  apiVersion: karpenter.k8s.aws/v1
  kind: EC2NodeClass
  metadata:
    namespace : karpenter
    name: default
  spec:
    kubelet:
      podsPerCore: 2
      maxPods: 20
      systemReserved:
        cpu: 100m
        memory: 100Mi
        ephemeral-storage: 1Gi
      kubeReserved:
        cpu: 200m
        memory: 100Mi
        ephemeral-storage: 3Gi
      evictionHard:
        memory.available: 5%
        nodefs.available: 10%
        nodefs.inodesFree: 10%
      evictionSoft:
        memory.available: 500Mi
        nodefs.available: 15%
        nodefs.inodesFree: 15%
      evictionSoftGracePeriod:
        memory.available: 1m
        nodefs.available: 1m30s
        nodefs.inodesFree: 2m
      evictionMaxPodGracePeriod: 60
      imageGCHighThresholdPercent: 85
      imageGCLowThresholdPercent: 80

    # Optional, dictates UserData generation and default block device mappings.
    # May be ommited when using an `alias` amiSelectorTerm, otherwise required.
    amiFamily: BottleRocket

    # Required, discovers subnets to attach to instances
    # Each term in the array of subnetSelectorTerms is ORed together
    # Within a single term, all conditions are ANDed
    subnetSelectorTerms:
      # Select on any subnet that has the "karpenter.sh/discovery:
      # AND the "environment: test" tag OR any subnet with ID "subnet-09fa4a0a8f233a921"
      - tags:
          karpenter.sh/discovery: "${module.eks.cluster_name}"
          environment: test

    # Required, discovers security groups to attach to instances
    # Each term in the array of securityGroupSelectorTerms is ORed together
    # Within a single term, all conditions are ANDed
    securityGroupSelectorTerms:
      # Select on any security group that has both the "karpenter.sh/discovery:" tag
      # AND the "environment: test" tag OR any security group with the "my-security-group" name
      # OR any security group with ID "sg-063d7acfb4b06c82c"
      - tags:
          karpenter.sh/discovery: "${module.eks.cluster_name}"

    # Optional, IAM role to use for the node identity.
    # The "role" field is immutable after EC2NodeClass creation. This may change in the
    # future, but this restriction is currently in place today to ensure that Karpenter
    # avoids leaking managed instance profiles in your account.
    # Must specify one of "role" or "instanceProfile" for Karpenter to launch nodes
    role: "${var.iam_roles.eks_node_role.arn}"

    # Optional, configures IMDS for the instance
    metadataOptions:
      httpEndpoint: enabled
      httpProtocolIPv6: disabled
      httpPutResponseHopLimit: 2 # This is changed to disable IMDS access from containers not on the host network
      httpTokens: required

    # Optional, configures storage devices for the instance
    blockDeviceMappings:
      - deviceName: /dev/xvda
        ebs:
          volumeSize: 20Gi
          volumeType: gp3
          encrypted: true
          deleteOnTermination: true
          throughput: 125

    # Optional, configures detailed monitoring for the instance
    detailedMonitoring: false

  status:
    # Capacity Reservations
    capacityReservations:
      - availabilityZone: ap-south-1a
        instanceType: t3a.large
      - availabilityZone: ap-south-1c
        instanceType: t3a.large

--- [FILE]: ./ap_south_1/eks/eks_variables.tf ---
variable "vpc_id" {
  description = "라우팅 테이블에 연결할 VPC ID"
  type        = string
}

variable "subnets" {
  description = "생성된 서브넷 목록"
  type        = any
}

variable "project_name" {
  description = "Project Name"
  type        = string
}
variable "iam_roles" {
  description = "IAM Role 목록"
  type        = any
}

variable "security_groups" {
  type = any
}


--- [FILE]: ./ap_south_1/eks/eks_main.tf ---
terraform {
  required_providers {
    kubectl = {
      source  = "gavinbunney/kubectl"
      version = ">= 1.7.0"
    }
  }
}

provider "helm" {
  kubernetes {
    host                   = module.eks.cluster_endpoint
    cluster_ca_certificate = base64decode(module.eks.cluster_certificate_authority_data)

    exec {
      api_version = "client.authentication.k8s.io/v1beta1"
      command     = "aws"
      # This requires the awscli to be installed locally where Terraform is executed
      args = ["eks", "get-token", "--cluster-name", module.eks.cluster_name]
    }
  }
}

data "aws_eks_cluster_auth" "main" {
  name = module.eks.cluster_name
}

provider "kubectl" {
  host                   = module.eks.cluster_endpoint
  cluster_ca_certificate = base64decode(module.eks.cluster_certificate_authority_data)
  load_config_file       = false
  token                  = data.aws_eks_cluster_auth.main.token
  exec {
    api_version = "client.authentication.k8s.io/v1beta1"
    command     = "aws"
    # This requires the awscli to be installed locally where Terraform is executed
    args = ["eks", "get-token", "--cluster-name", module.eks.cluster_name]
  }
}

provider "aws" {
  region = "us-east-1"
  alias  = "us_east"
}

data "aws_ecrpublic_authorization_token" "token" {
  provider = aws.us_east
}

locals {
  oidc_provider = replace(module.eks.cluster_oidc_issuer_url, "https://", "")
  name          = "${var.project_name}-eks-cluster-01"
  tags = {
    Project                  = "${var.project_name}"
    "karpenter.sh/discovery" = "${var.project_name}-eks-cluster-01"
  }
}

# EKS Cluster 생성
module "eks" {
  source  = "terraform-aws-modules/eks/aws"
  version = "~> 20.31"

  cluster_name    = local.name
  vpc_id          = var.vpc_id
  subnet_ids      = [for subnet in var.subnets.eks_subnets : subnet.id]
  cluster_version = "1.30"

  cluster_endpoint_public_access  = true
  cluster_endpoint_private_access = true

  create_iam_role = false
  iam_role_arn    = var.iam_roles.eks_cluster_role.arn

  # 자동으로 cluster creator(현재 Terraform을 실행하는 주체)를 admin으로 등록하지 않도록 하려면 false로 설정
  enable_cluster_creator_admin_permissions = true

  # Bastion에서 EKS API 서버(443)에 접근할 수 있도록 추가 규칙 추가
  cluster_security_group_additional_rules = {
    bastion_access = {
      description              = "Allow Bastion SG access to EKS API server"
      type                     = "ingress"
      protocol                 = "tcp"
      from_port                = 443
      to_port                  = 443
      source_security_group_id = var.security_groups.bastion_sg.id
    }
  }

  eks_managed_node_groups = {
    "${var.project_name}-ng-01" = {
      # Starting on 1.30, AL2023 is the default AMI type for EKS managed node groups
      ami_type               = "BOTTLEROCKET_x86_64"
      instance_types         = ["t3a.large"]
      name                   = "${var.project_name}-ng-01"
      create_launch_template = true
      launch_template_name   = "${var.project_name}-ng-01-lt"
      create_iam_role        = false
      iam_role_arn           = var.iam_roles.eks_node_role.arn

      subnet_ids = [for subnet in var.subnets.private_pri_subnets : subnet.id]
      # Amazon Linux 2 node groups do not require a specific settings block.
      # Any custom configuration (e.g. user data modifications) should be handled via a launch template if needed.
      min_size     = 3
      max_size     = 3
      desired_size = 3
      # labels = {
      #   # Used to ensure Karpenter runs on nodes that it does not manage
      #   "karpenter.sh/controller" = "true"
      # }
    }
  }

  access_entries = {
    # One access entry with a policy associated
    ssm_role_access = {
      principal_arn = var.iam_roles.ssm_role.arn

      policy_associations = {
        eks_admin_policy = {
          policy_arn = "arn:aws:eks::aws:cluster-access-policy/AmazonEKSAdminPolicy"
          access_scope = {
            type = "cluster"
          }
        }
        cluster_admin_policy = {
          policy_arn = "arn:aws:eks::aws:cluster-access-policy/AmazonEKSClusterAdminPolicy"
          access_scope = {
            type = "cluster"
          }
        }
      }
    }
  }

  node_security_group_tags = merge(local.tags, {
    "karpenter.sh/discovery" = local.name
  })
  tags = local.tags
}

resource "aws_eks_addon" "ebs_csi" {
  cluster_name                = module.eks.cluster_name
  addon_name                  = "aws-ebs-csi-driver"
  addon_version               = "v1.41.0-eksbuild.1"
  resolve_conflicts_on_create = "OVERWRITE"
  depends_on                  = [module.eks]
}

resource "aws_eks_addon" "efs_csi" {
  cluster_name                = module.eks.cluster_name
  addon_name                  = "aws-efs-csi-driver"
  addon_version               = "v2.1.7-eksbuild.1"
  resolve_conflicts_on_create = "OVERWRITE"
  depends_on                  = [module.eks]
}

resource "aws_eks_addon" "coredns" {
  cluster_name                = module.eks.cluster_name
  addon_name                  = "coredns"
  addon_version               = "v1.11.1-eksbuild.9"
  resolve_conflicts_on_create = "OVERWRITE"
  depends_on                  = [module.eks]
}

resource "aws_eks_addon" "kube_proxy" {
  cluster_name                = module.eks.cluster_name
  addon_name                  = "kube-proxy"
  addon_version               = "v1.30.6-eksbuild.3"
  resolve_conflicts_on_create = "OVERWRITE"
  depends_on                  = [module.eks]
}

resource "aws_eks_addon" "vpc_cni" {
  cluster_name                = module.eks.cluster_name
  addon_name                  = "vpc-cni"
  addon_version               = "v1.19.0-eksbuild.1"
  resolve_conflicts_on_create = "OVERWRITE"
  depends_on                  = [module.eks]
  configuration_values = jsonencode({
    env = {
      ENABLE_PREFIX_DELEGATION = "true"
      WARM_PREFIX_TARGET       = "1"
    }
  })
}

resource "aws_eks_addon" "pod_identity" {
  cluster_name                = module.eks.cluster_name
  addon_name                  = "eks-pod-identity-agent"
  addon_version               = "v1.3.4-eksbuild.1"
  resolve_conflicts_on_create = "OVERWRITE"
  depends_on                  = [module.eks]
}

module "karpenter" {
  source                 = "terraform-aws-modules/eks/aws//modules/karpenter"
  enable_irsa            = true
  irsa_oidc_provider_arn = module.eks.oidc_provider_arn
  cluster_name           = local.name
  namespace              = "karpenter"
  enable_v1_permissions  = true

  # Name needs to match role name passed to the EC2NodeClass

  iam_role_name        = "${var.project_name}-karpenter-controller-role"
  create_node_iam_role = true
  node_iam_role_name   = "${var.project_name}-karpenter-node-role"

  create_access_entry             = true
  create_pod_identity_association = true

  tags = local.tags
}

resource "helm_release" "karpenter" {
  create_namespace = true
  namespace        = "karpenter"
  name             = "karpenter"

  repository          = "oci://public.ecr.aws/karpenter"
  repository_username = data.aws_ecrpublic_authorization_token.token.user_name
  repository_password = data.aws_ecrpublic_authorization_token.token.password
  chart               = "karpenter"
  version             = "1.3.2"
  wait                = false

  values = [
    <<-EOF
    serviceAccount:
      annotations:
        eks.amazonaws.com/role-arn: "${module.karpenter.iam_role_arn}"
    logLevel: debug
    settings:
      clusterName: "${module.eks.cluster_name}"
      clusterEndpoint: "${module.eks.cluster_endpoint}"
      interruptionQueue: "${module.karpenter.queue_name}"
    controller:
      resources:
        requests:
          cpu: 1
          memory: 1Gi
        limits:
          cpu: 1
          memory: 1Gi
    EOF
  ]
}

resource "kubectl_manifest" "karpenter_nodepool" {
  yaml_body  = <<-YAML
    apiVersion: karpenter.sh/v1
    kind: NodePool
    metadata:
      name: "${var.project_name}-karpenter-nodepool"
      namespace: karpenter
    spec:
      template:
        spec:
          requirements:
            - key: "node.kubernetes.io/instance-type"
              operator: In
              values: ["t3a.large"]      
          nodeClassRef:
            group: karpenter.k8s.aws
            kind: EC2NodeClass
            name: "${var.project_name}-karpenter-nodeclass"
            namespace: karpenter
  YAML
  depends_on = [helm_release.karpenter]
}

resource "kubectl_manifest" "karpenter_ec2nodeclass" {
  yaml_body  = <<-YAML
    apiVersion: karpenter.k8s.aws/v1
    kind: EC2NodeClass
    metadata:
      name: "${var.project_name}-karpenter-nodeclass"
      namespace: karpenter
    spec:
      kubelet:
        podsPerCore: 4
        maxPods: 40
        systemReserved:
          cpu: 100m
          memory: 100Mi
          ephemeral-storage: 1Gi
        kubeReserved:
          cpu: 200m
          memory: 300Mi
          ephemeral-storage: 3Gi
        evictionHard:
          memory.available: 5%
          nodefs.available: 10%
          nodefs.inodesFree: 10%
        evictionSoft:
          memory.available: 500Mi
          nodefs.available: 15%
          nodefs.inodesFree: 15%
        evictionSoftGracePeriod:
          memory.available: 1m
          nodefs.available: 1m30s
          nodefs.inodesFree: 2m
        evictionMaxPodGracePeriod: 60
        imageGCHighThresholdPercent: 85
        imageGCLowThresholdPercent: 80
        cpuCFSQuota: true
      subnetSelectorTerms:
        - tags:
            karpenter.sh/discovery: ${module.eks.cluster_name}
      securityGroupSelectorTerms:
        - tags:
            karpenter.sh/discovery: "${module.eks.cluster_name}"
      role: "${module.karpenter.node_iam_role_arn}"
      amiSelectorTerms:
        - alias: bottlerocket@latest

  YAML
  depends_on = [helm_release.karpenter]
}

#############################
# ALB Controller IAM Role 생성
#############################
resource "aws_iam_role" "alb_controller_role" {
  name = "${var.project_name}-alb-controller-role"
  assume_role_policy = jsonencode({
    "Version" : "2012-10-17",
    "Statement" : [
      {
        "Effect" : "Allow",
        "Principal" : {
          "Federated" : module.eks.oidc_provider_arn
        },
        "Action" : "sts:AssumeRoleWithWebIdentity",
        "Condition" : {
          "StringEquals" : {
            "${local.oidc_provider}:sub" : "system:serviceaccount:kube-system:aws-load-balancer-controller",
            "${local.oidc_provider}:aud" : "sts.amazonaws.com"
          }
        }
      }
    ]
  })
}

#############################
#Role에 Policy Attach
#############################

resource "aws_iam_role_policy_attachment" "alb_controller_role_attach" {
  role       = aws_iam_role.alb_controller_role.name
  policy_arn = aws_iam_policy.alb_controller_policy.arn
}

resource "aws_iam_policy" "alb_controller_policy" {
  name        = "${var.project_name}_ALBControllerPolicy"
  description = "Policy for AWS ALB Controller to manage ELB resources."
  policy = jsonencode({
    "Version" : "2012-10-17",
    "Statement" : [
      {
        "Effect" : "Allow",
        "Action" : [
          "iam:CreateServiceLinkedRole"
        ],
        "Resource" : "*",
        "Condition" : {
          "StringEquals" : {
            "iam:AWSServiceName" : "elasticloadbalancing.amazonaws.com"
          }
        }
      },
      {
        "Effect" : "Allow",
        "Action" : [
          "ec2:DescribeAccountAttributes",
          "ec2:DescribeAddresses",
          "ec2:DescribeAvailabilityZones",
          "ec2:DescribeInternetGateways",
          "ec2:DescribeVpcs",
          "ec2:DescribeVpcPeeringConnections",
          "ec2:DescribeSubnets",
          "ec2:DescribeSecurityGroups",
          "ec2:DescribeInstances",
          "ec2:DescribeNetworkInterfaces",
          "ec2:DescribeTags",
          "ec2:GetCoipPoolUsage",
          "ec2:DescribeCoipPools",
          "ec2:GetSecurityGroupsForVpc",
          "elasticloadbalancing:DescribeLoadBalancers",
          "elasticloadbalancing:DescribeLoadBalancerAttributes",
          "elasticloadbalancing:DescribeListeners",
          "elasticloadbalancing:DescribeListenerCertificates",
          "elasticloadbalancing:DescribeSSLPolicies",
          "elasticloadbalancing:DescribeRules",
          "elasticloadbalancing:DescribeTargetGroups",
          "elasticloadbalancing:DescribeTargetGroupAttributes",
          "elasticloadbalancing:DescribeTargetHealth",
          "elasticloadbalancing:DescribeTags",
          "elasticloadbalancing:DescribeTrustStores",
          "elasticloadbalancing:DescribeListenerAttributes",
          "elasticloadbalancing:DescribeCapacityReservation"
        ],
        "Resource" : "*"
      },
      {
        "Effect" : "Allow",
        "Action" : [
          "cognito-idp:DescribeUserPoolClient",
          "acm:ListCertificates",
          "acm:DescribeCertificate",
          "iam:ListServerCertificates",
          "iam:GetServerCertificate",
          "waf-regional:GetWebACL",
          "waf-regional:GetWebACLForResource",
          "waf-regional:AssociateWebACL",
          "waf-regional:DisassociateWebACL",
          "wafv2:GetWebACL",
          "wafv2:GetWebACLForResource",
          "wafv2:AssociateWebACL",
          "wafv2:DisassociateWebACL",
          "shield:GetSubscriptionState",
          "shield:DescribeProtection",
          "shield:CreateProtection",
          "shield:DeleteProtection"
        ],
        "Resource" : "*"
      },
      {
        "Effect" : "Allow",
        "Action" : [
          "ec2:AuthorizeSecurityGroupIngress",
          "ec2:RevokeSecurityGroupIngress"
        ],
        "Resource" : "*"
      },
      {
        "Effect" : "Allow",
        "Action" : [
          "ec2:CreateSecurityGroup"
        ],
        "Resource" : "*"
      },
      {
        "Effect" : "Allow",
        "Action" : [
          "ec2:CreateTags"
        ],
        "Resource" : "arn:aws:ec2:*:*:security-group/*",
        "Condition" : {
          "StringEquals" : {
            "ec2:CreateAction" : "CreateSecurityGroup"
          },
          "Null" : {
            "aws:RequestTag/elbv2.k8s.aws/cluster" : "false"
          }
        }
      },
      {
        "Effect" : "Allow",
        "Action" : [
          "ec2:CreateTags",
          "ec2:DeleteTags"
        ],
        "Resource" : "arn:aws:ec2:*:*:security-group/*",
        "Condition" : {
          "Null" : {
            "aws:RequestTag/elbv2.k8s.aws/cluster" : "true",
            "aws:ResourceTag/elbv2.k8s.aws/cluster" : "false"
          }
        }
      },
      {
        "Effect" : "Allow",
        "Action" : [
          "ec2:AuthorizeSecurityGroupIngress",
          "ec2:RevokeSecurityGroupIngress",
          "ec2:DeleteSecurityGroup"
        ],
        "Resource" : "*",
        "Condition" : {
          "Null" : {
            "aws:ResourceTag/elbv2.k8s.aws/cluster" : "false"
          }
        }
      },
      {
        "Effect" : "Allow",
        "Action" : [
          "elasticloadbalancing:CreateLoadBalancer",
          "elasticloadbalancing:CreateTargetGroup"
        ],
        "Resource" : "*",
        "Condition" : {
          "Null" : {
            "aws:RequestTag/elbv2.k8s.aws/cluster" : "false"
          }
        }
      },
      {
        "Effect" : "Allow",
        "Action" : [
          "elasticloadbalancing:CreateListener",
          "elasticloadbalancing:DeleteListener",
          "elasticloadbalancing:CreateRule",
          "elasticloadbalancing:DeleteRule"
        ],
        "Resource" : "*"
      },
      {
        "Effect" : "Allow",
        "Action" : [
          "elasticloadbalancing:AddTags",
          "elasticloadbalancing:RemoveTags"
        ],
        "Resource" : [
          "arn:aws:elasticloadbalancing:*:*:targetgroup/*/*",
          "arn:aws:elasticloadbalancing:*:*:loadbalancer/net/*/*",
          "arn:aws:elasticloadbalancing:*:*:loadbalancer/app/*/*"
        ],
        "Condition" : {
          "Null" : {
            "aws:RequestTag/elbv2.k8s.aws/cluster" : "true",
            "aws:ResourceTag/elbv2.k8s.aws/cluster" : "false"
          }
        }
      },
      {
        "Effect" : "Allow",
        "Action" : [
          "elasticloadbalancing:AddTags",
          "elasticloadbalancing:RemoveTags"
        ],
        "Resource" : [
          "arn:aws:elasticloadbalancing:*:*:listener/net/*/*/*",
          "arn:aws:elasticloadbalancing:*:*:listener/app/*/*/*",
          "arn:aws:elasticloadbalancing:*:*:listener-rule/net/*/*/*",
          "arn:aws:elasticloadbalancing:*:*:listener-rule/app/*/*/*"
        ]
      },
      {
        "Effect" : "Allow",
        "Action" : [
          "elasticloadbalancing:ModifyLoadBalancerAttributes",
          "elasticloadbalancing:SetIpAddressType",
          "elasticloadbalancing:SetSecurityGroups",
          "elasticloadbalancing:SetSubnets",
          "elasticloadbalancing:DeleteLoadBalancer",
          "elasticloadbalancing:ModifyTargetGroup",
          "elasticloadbalancing:ModifyTargetGroupAttributes",
          "elasticloadbalancing:DeleteTargetGroup",
          "elasticloadbalancing:ModifyListenerAttributes",
          "elasticloadbalancing:ModifyCapacityReservation"
        ],
        "Resource" : "*",
        "Condition" : {
          "Null" : {
            "aws:ResourceTag/elbv2.k8s.aws/cluster" : "false"
          }
        }
      },
      {
        "Effect" : "Allow",
        "Action" : [
          "elasticloadbalancing:AddTags"
        ],
        "Resource" : [
          "arn:aws:elasticloadbalancing:*:*:targetgroup/*/*",
          "arn:aws:elasticloadbalancing:*:*:loadbalancer/net/*/*",
          "arn:aws:elasticloadbalancing:*:*:loadbalancer/app/*/*"
        ],
        "Condition" : {
          "StringEquals" : {
            "elasticloadbalancing:CreateAction" : [
              "CreateTargetGroup",
              "CreateLoadBalancer"
            ]
          },
          "Null" : {
            "aws:RequestTag/elbv2.k8s.aws/cluster" : "false"
          }
        }
      },
      {
        "Effect" : "Allow",
        "Action" : [
          "elasticloadbalancing:RegisterTargets",
          "elasticloadbalancing:DeregisterTargets"
        ],
        "Resource" : "arn:aws:elasticloadbalancing:*:*:targetgroup/*/*"
      },
      {
        "Effect" : "Allow",
        "Action" : [
          "elasticloadbalancing:SetWebAcl",
          "elasticloadbalancing:ModifyListener",
          "elasticloadbalancing:AddListenerCertificates",
          "elasticloadbalancing:RemoveListenerCertificates",
          "elasticloadbalancing:ModifyRule"
        ],
        "Resource" : "*"
      }
    ]
  })
}


--- [FILE]: ./ap_south_1/eks/eks_output.tf ---
output "eks" {
  value = {
    cluster_name     = module.eks.cluster_name
    cluster_version  = module.eks.cluster_version
  }
}


--- [FILE]: ./ap_south_1/endpoint/endpoint_main.tf ---
module "endpoints" {
  source = "terraform-aws-modules/vpc/aws//modules/vpc-endpoints"
  vpc_id = var.vpc_id

  endpoints = {
    s3 = {
      service         = "s3"
      endpoints       = "gateway"
      route_table_ids = ["${var.route_table.id}"]
      tags            = { Name = "${var.project_name}-youngjin-s3-endpoint" }
    },
  }
}


--- [FILE]: ./ap_south_1/endpoint/endpoint_variables.tf ---
variable "vpc_id" {
  description = "VPC ID"
  type        = string
}

variable "project_name" {
  description = "Project Name"
  type        = string
}

variable "route_table" {
  description = "Route table list"
  type        = any
}


--- [FILE]: ./ap_south_1/rds/rds_variabels.tf ---
variable "vpc_id" {
  description = "VPC ID"
  type        = string
}
variable "project_name" {
  description = "Project Name"
  type        = string
}

variable "subnets" {
  description = "생성된 서브넷 목록"
  type        = any
}

variable "db_subnet" {
  description = "DB 서브넷 그룹"
  type        = any
}


--- [FILE]: ./ap_south_1/rds/rds_aurora_mysql.tf_ ---
module "cluster" {
  source = "terraform-aws-modules/rds-aurora/aws"

  name           = "${var.project_name}-aurora-mysql"
  engine         = "aurora-mysql"
  engine_version = "8.0.mysql_aurora.3.05.2"
  instance_class = "db.r7g.large"
  instances = {
    01 = {}
    02 = {}
  }
  autoscaling_enabled      = true
  autoscaling_min_capacity = 2
  autoscaling_max_capacity = 5
  skip_final_snapshot      = true
  database_name            = {put-your-db-name-here}
  master_username          = {put-your-db-username-here}
  master_password          = {put-your-db-password-here}

  vpc_id               = var.vpc_id
  db_subnet_group_name = var.db_subnet.db_subnet.name
  security_group_rules = {
    ex1_ingress = {
      cidr_blocks = ["150.0.40.0/24"]
    }
    ex2_ingress = {
      cidr_blocks = ["150.0.30.0/24"]
    }
  }

  storage_encrypted   = true
  apply_immediately   = true
  monitoring_interval = 10


  tags = {
    Environment = "prd"
    Terraform   = "true"
  }
}


--- [FILE]: ./ap_south_1/subnets/private_subnets.tf ---
resource "aws_subnet" "private_pri_subnet_a" {
  vpc_id            = var.vpc_id
  availability_zone = "${var.region_code}a"
  cidr_block        = "150.0.30.0/24"

  tags = {
    Name                              = "${var.project_name}-pri-subnet-${var.region_code}a"
    type                              = "private"
    "kubernetes.io/role/internal-elb" = "1"
    "karpenter.sh/discovery"          = "${var.project_name}-eks-cluster-01"
  }
}

resource "aws_subnet" "private_pri_subnet_c" {
  vpc_id            = var.vpc_id
  availability_zone = "${var.region_code}c"
  cidr_block        = "150.0.40.0/24"

  tags = {
    Name                              = "${var.project_name}-pri-subnet-${var.region_code}c"
    type                              = "private"
    "kubernetes.io/role/internal-elb" = "1"
    "karpenter.sh/discovery"          = "${var.project_name}-eks-cluster-01"
  }
}


--- [FILE]: ./ap_south_1/subnets/subnets_variables.tf ---
variable "project_name" {
  description = "Project Name"
  type        = string
}

variable "vpc_id" {
  description = "라우팅 테이블에 연결할 VPC ID"
  type        = string
}

variable "region_code" {
  description = "Region code"
  type        = string
}

variable "subnets" {
  description = "subnet_list"
  type        = any
}

variable "eks_output"{
  description = "eks_list"
  type        = any
}


--- [FILE]: ./ap_south_1/subnets/subnets_outputs.tf ---
output "subnet_list" {
  value = {
    public_pub_subnets    = [aws_subnet.public_subnet_a, aws_subnet.public_subnet_c]
    private_pri_subnets   = [aws_subnet.private_pri_subnet_a, aws_subnet.private_pri_subnet_c]
    private_pri_a_subnets = aws_subnet.private_pri_subnet_a
    private_pri_c_subnets = aws_subnet.private_pri_subnet_c
    private_nat_subnets   = [aws_subnet.public_subnet_a]
    db_subnets            = [aws_subnet.db_subnet_a, aws_subnet.db_subnet_c]
    eks_subnets           = [aws_subnet.public_subnet_a, aws_subnet.public_subnet_c, aws_subnet.private_pri_subnet_a, aws_subnet.private_pri_subnet_c]
  }
}

output "db_subnet" {
  value = {
    db_subnet    = aws_db_subnet_group.db_subnet
    redis_subnet = aws_elasticache_subnet_group.cache_subnet
  }
}


--- [FILE]: ./ap_south_1/subnets/public_subnets.tf ---
resource "aws_subnet" "public_subnet_a" {
  vpc_id                  = var.vpc_id
  availability_zone       = "${var.region_code}a"
  cidr_block              = "150.0.10.0/24"
  map_public_ip_on_launch = true
  # depends_on              = [ module.igw ]

  tags = {
    Name                     = "${var.project_name}-pub-subnet-${var.region_code}a"
    nat                      = "true"
    type                     = "public"
    "kubernetes.io/role/elb" = "1"
    "karpenter.sh/discovery" = "${var.project_name}-eks-cluster-01"
  }
}

resource "aws_subnet" "public_subnet_c" {
  vpc_id                  = var.vpc_id
  availability_zone       = "${var.region_code}c"
  cidr_block              = "150.0.20.0/24"
  map_public_ip_on_launch = true
  # depends_on              = [ module.igw ]

  tags = {
    Name                     = "${var.project_name}-pub-subnet-${var.region_code}c"
    nat                      = "false"
    type                     = "public"
    "kubernetes.io/role/elb" = "1"
    "karpenter.sh/discovery" = "${var.project_name}-eks-cluster-01"
  }
}


--- [FILE]: ./ap_south_1/subnets/db_subnets.tf ---
resource "aws_subnet" "db_subnet_a" {
  vpc_id                  = var.vpc_id
  availability_zone       = "${var.region_code}a"
  cidr_block              = "150.0.50.0/24"
  map_public_ip_on_launch = true
  //depends_on              = [ module.igw ]

  tags = {
    Name = "${var.project_name}-db-subnet-${var.region_code}a"
    nat  = "true"
    type = "private"
  }
}

resource "aws_subnet" "db_subnet_c" {
  vpc_id                  = var.vpc_id
  availability_zone       = "${var.region_code}c"
  cidr_block              = "150.0.60.0/24"
  map_public_ip_on_launch = true
  //depends_on              = [ module.igw ]

  tags = {
    Name = "${var.project_name}-db-subnet-${var.region_code}c"
    nat  = "false"
    type = "private"
  }
}

resource "aws_db_subnet_group" "db_subnet" {
  name       = "${var.project_name}-db-subnet"
  subnet_ids = [for subnet in var.subnets.db_subnets : subnet.id]
}


--- [FILE]: ./ap_south_1/subnets/redis_subnets.tf ---
resource "aws_elasticache_subnet_group" "cache_subnet" {
  name       = "${var.project_name}-cache-subnet"
  subnet_ids = [for subnet in var.subnets.db_subnets : subnet.id]
}


--- [FILE]: ./ap_south_1/eip/eip_variables.tf ---
variable "vpc_id" {
  description = "인터넷 게이트웨이에 연결할 VPC ID"
  type        = string
}

variable "nat_subnets" {
  description = "NAT가 적용될 서브넷 목록"
  type        = any
}
variable "project_name" {
  description = "Project Name"
  type        = string
}


--- [FILE]: ./ap_south_1/eip/eip_main.tf ---
resource "aws_eip" "nat_eips" {
  count  = length(var.nat_subnets)
  domain = "vpc"

  tags = {
    Name = "${var.project_name}-nat-${substr(element(var.nat_subnets.*.availability_zone, count.index), 14, 1)}-eip"
    nat  = "true"
  }
}


--- [FILE]: ./ap_south_1/eip/eip_outputs.tf ---
output "nat_eip" {
  value = aws_eip.nat_eips
}


--- [FILE]: ./ap_south_1/efs/efs_main.tf ---
module "efs" {
  source = "terraform-aws-modules/efs/aws"

  name      = "${var.project_name}-efs-01"
  encrypted = true

  performance_mode = "generalPurpose"
  throughput_mode  = "bursting"

  lifecycle_policy = {
    transition_to_ia = "AFTER_30_DAYS"
  }

  attach_policy                      = true
  bypass_policy_lockout_safety_check = false

  mount_targets = {
    "${var.region_code}a" = {
      subnet_id = var.subnets.private_pri_a_subnets.id
    }
    "${var.region_code}c" = {
      subnet_id = var.subnets.private_pri_c_subnets.id
    }
  }
  security_group_description = "Youngjin EFS SG"
  security_group_vpc_id      = var.vpc_id
  security_group_rules = {
    vpc = {
      description = "EFS Inbound SG Rule"
      cidr_blocks = ["150.0.30.0/24", "150.0.40.0/24"]
    }
  }

  access_points = {
    root_example = {
      root_directory = {
        path = "/example"
        creation_info = {
          owner_gid   = 1001
          owner_uid   = 1001
          permissions = "755"
        }
      }
    }
  }

  enable_backup_policy = true

  tags = {
    Terraform = "true"
  }
}


--- [FILE]: ./ap_south_1/efs/efs_variables.tf ---
variable "vpc_id" {
  description = "VPC ID"
  type        = string
}
variable "project_name" {
  description = "Project Name"
  type        = string
}

variable "subnets" {
  description = "생성된 서브넷 목록"
  type        = any
}

variable "region_code" {
  description = "Region Code"
  type        = string
}

variable "security_groups" {
  type = any
}  


--- [FILE]: ./ap_south_1/ec2/ec2_bastion.tf ---
# 1. Amazon Linux 2023 AMI를 조회하는 Data Source
data "aws_ami" "amazon_linux_2023" {
  most_recent = true
  owners      = ["amazon"] # AWS 공식 AMI
  filter {
    name   = "name"
    values = ["al2023-ami-2023.*"] # Amazon Linux 2023 이미지 이름 패턴
  }
  filter {
    name   = "state"
    values = ["available"]
  }
  filter {
    name   = "architecture"
    values = ["x86_64"]
  }
}

# 2. EC2 인스턴스 생성 모듈
module "ec2_instance" {
  source = "terraform-aws-modules/ec2-instance/aws"

  name = "${var.project_name}-ec2-bastion"

  instance_type          = "t3a.nano"
  monitoring             = true
  vpc_security_group_ids = [var.security_groups.bastion_sg.id]
  subnet_id              = var.subnets.private_pri_a_subnets.id
  iam_instance_profile   = var.iam_ssm_profile.name

  # 여기서 ami 파라미터에 Data Source의 id를 전달
  ami = data.aws_ami.amazon_linux_2023.id

  # IMDSv2만 사용하도록 metadata_options 설정 (IMDSv1 비활성화)
  metadata_options = {
    http_tokens                 = "required" # 토큰 없이 접근 불가 → IMDSv2 강제
    http_put_response_hop_limit = 2          # 응답 hop 제한 (필요에 따라 조정)
    http_endpoint               = "enabled"  # 메타데이터 엔드포인트 활성화
  }

  # 사용자 데이터 (user_data) 스크립트: kubectl 설치 및 kubeconfig 업데이트  
  user_data = <<-EOF
  #!/bin/bash
  # set -e 제거, 대신 -u와 -o pipefail만 사용
  set -uo pipefail

  LOG_FILE="/root/user_data.log"
  success_list=()
  fail_list=()

  # 명령어를 실행하면서 로그를 남기고 성공/실패를 배열에 저장
  run_command() {
    local cmd="$*"
    echo "---------------------------------------------------" | tee -a $LOG_FILE
    echo "[INFO] Running: $cmd" | tee -a $LOG_FILE
    eval "$cmd" >> $LOG_FILE 2>&1
    if [ $? -eq 0 ]; then
      echo "[SUCCESS] $cmd" | tee -a $LOG_FILE
      success_list+=("$cmd")
    else
      echo "[FAILURE] $cmd" | tee -a $LOG_FILE
      fail_list+=("$cmd")
    fi
  }

  # 예시: yum 업데이트
  run_command "yum update -y"
  run_command "yum install -y awscli vim git"

  # kubectl 설치
  run_command "curl -O https://s3.us-west-2.amazonaws.com/amazon-eks/1.31.3/2024-12-12/bin/linux/amd64/kubectl"
  run_command "chmod +x ./kubectl"
  run_command "mkdir -p /root/bin && cp ./kubectl /root/bin/kubectl && export PATH=/root/bin:\$PATH"
  run_command "echo 'export PATH=/root/bin:\$PATH' >> /root/.bashrc"

  # kubeconfig 업데이트
  run_command "aws eks update-kubeconfig --region ${var.region_code} --name ${var.eks_cluster.cluster_name}"

  # k9s 설치
  run_command "curl -LO https://github.com/derailed/k9s/releases/download/v0.40.7/k9s_Linux_amd64.tar.gz"
  run_command "tar -xvf k9s_Linux_amd64.tar.gz"
  run_command "mkdir -p /root/.local/bin && mv ./k9s /root/.local/bin && chmod +x /root/.local/bin/k9s"
  run_command "rm -f k9s_Linux_amd64.tar.gz"
  run_command "echo 'export PATH=\$PATH:/root/.local/bin' >> /root/.bashrc"

  # helm 설치
  run_command "curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3"
  run_command "chmod 700 get_helm.sh"
  run_command "./get_helm.sh"

  # eksctl 설치
  run_command "ARCH=amd64 && PLATFORM=\$(uname -s)_\$ARCH"
  run_command "curl -sLO https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_\$PLATFORM.tar.gz"
  run_command "curl -sL https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_checksums.txt | grep \$PLATFORM | sha256sum --check"
  run_command "tar -xzf eksctl_\$PLATFORM.tar.gz -C /tmp && rm -f eksctl_\$PLATFORM.tar.gz"
  run_command "mv /tmp/eksctl /usr/local/bin"

  # git clone
  run_command "mkdir /root/git && cd /root/git && git clone https://github.com/PYJ-zero/youngjin_oss.git"

  # velero 설치
  run_command "curl -L https://github.com/vmware-tanzu/velero/releases/download/v1.15.2/velero-v1.15.2-linux-amd64.tar.gz -o /tmp/velero.tar.gz"
  run_command "tar -xzvf /tmp/velero.tar.gz -C /tmp && rm -f /tmp/velero.tar.gz"
  run_command "mv /tmp/velero-v1.15.2-linux-amd64/velero /usr/local/bin/"
  run_command "chmod +x /usr/local/bin/velero"

  # credentials-velero 파일 생성
  run_command "cat <<CRED > /root/credentials-velero
  [default]
  aws_access_key_id=${var.iam_users.velero_user.id}
  aws_secret_access_key=${var.iam_users.velero_user.secret}
  CRED
  "

  # velero instal
  run_command "velero install \
    --provider aws \
    --plugins velero/velero-plugin-for-aws:v1.11.1 \
    --bucket ${var.s3_list.velero_bucket.id} \
    --backup-location-config region=${var.region_code} \
    --secret-file /root/credentials-velero \
    --snapshot-location-config region=${var.region_code}"

  # 스크립트 마지막에 요약 로그 출력
  echo "---------------------------------------------------" | tee -a $LOG_FILE
  echo "[INFO] Summary of commands" | tee -a $LOG_FILE

  if [ $${#success_list[@]} -gt 0 ]; then
    echo "[INFO] Succeeded: $${success_list[*]}" | tee -a $LOG_FILE
  fi

  if [ $${#fail_list[@]} -gt 0 ]; then
    echo "[INFO] Failed: $${fail_list[*]}" | tee -a $LOG_FILE
  fi
  source /root/.bashrc
  EOF

  tags = {
    Terraform   = "true"
    Environment = "dev"
  }
}


--- [FILE]: ./ap_south_1/ec2/ec2_variables.tf ---
variable "vpc_id" {
  description = "VPC ID"
  type        = string
}
variable "project_name" {
  description = "Project Name"
  type        = string
}

variable "subnets" {
  description = "생성된 서브넷 목록"
  type        = any
}

variable "region_code" {
  description = "Region Code"
  type        = string
}

variable "security_groups" {
  type = any
}

variable "iam_ssm_profile" {
  type = any
}

variable "eks_cluster" {
  type = any
}

variable "iam_users" {
  type = any
}

variable "s3_list" {
  type = any
  
}
